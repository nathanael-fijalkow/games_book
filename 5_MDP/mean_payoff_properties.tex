We will use the $\liminf$ variant of mean-payoff:

\[
\MeanPayoffInf(\play) = \liminf_n \frac{1}{n} \sum_{i = 0}^{n-1} c(\play_i)
\]

In particular, the play-based expected mean-payoff achieved by $ \sigma $ from $ v $ is $ \playPay(v, \sigma) = \expv_v^\sigma[\MeanPayoffInf] $, while for the step-based counterpart we have $ \stepPay(v, \sigma) = \liminf_{n \rightarrow \infty} \frac{1}{n} \sum_{i=0}^{n-1} \expv_v^\sigma[\colouring(\play_i)]$.

The use of $ \liminf $ is natural in the formal verification setting: taking the $\liminf$ rather than $ \limsup $ emphasizes the worst-case behaviour along a play. However, all the results in this section hold also for $\limsup$-based mean-payoff, though some proofs are more complex. See the bibliographic remarks for more details.
%
%In the probabilistic setting, there are two ways of defining the expected mean-payoff under a given strategy $ \sigma $. The first way follows the recipe laid out in the preliminaries to this chapter, i.e. viewing $ \MeanPayoffInf $ as a random variable in the probability space induced by $ \sigma $. In such a case, the mean-payoff achieved by $ \sigma $ from vertex $ v $ is $ \expv_v^\sigma[\MeanPayoff]$, i.e. we compute the expectation of mean-payoff over all runs. The second way does just the reverse, i.e. for each time step $i$ we compute the expected reward accumulated in that step and then compute the limit average of these one-step expectations. Formally, the mean-payoff of $ \sigma $ from $ v $ is, according to this semantics, defined as $ \liminf_{ n\rightarrow \infty }  \frac{1}{ n } \sum_{i=0}^{n-1} \expv_v^\sigma [c(\play_i)] $.

In general, $\stepPay(v,\sigma)$ can be different from $ \playPay(v, \sigma) $. However, we have the following simple consequence of the dominated convergence theorem:
\begin{lemma}
	\label{5-lem:limit-defined}
Let $U$ be the set of all plays $\play$  for which $\lim_{n\rightarrow \infty} \frac{1}{n} \sum_{i=0}^{n-1}[\colouring(\play_i)]$ is undefined. If $\vinit,\sigma$ are such that $\probm^\sigma_{\vinit}(U) = 0$, then $\stepPay(\vinit,\sigma) = \playPay(\vinit, \sigma) $.
\end{lemma}


In particular, for any finite-memory strategy $ \sigma $, the two values coincide, since applying such a strategy turns an MDP into a finite Markov chain where the existence of the limit can be inferred using decomposition into strongly connected components and applying the Ergodic theorem.
We will show that in our case of finite-state MDPs, the two approaches coincide not only at the levels of finite-memory strategies, but also as optimality criteria: that is, no matter which of the two semantics we use, the optimal values are the same and a strategy that is optimal w.r.t. one of the semantics is also optimal for the other one. We caution the reader that such a result does not automatically transfer to more complex settings, such as infinite-state MDPs or multi-objective optimization. 

%We are interested in optimizing the expected value of these functions. Under a concrete strategy, these two functions might have a different expectation. However, in finite MDPs they coincide as an optimization criterion: we will see that a strategy optimizing the expectation of one is also optimal w.r.t. the expectation of the other one. The function $\MeanPayoffInf,$ which corresponds to optimizing the worst-case performance of the modelled system, is somewhat more standard in stochastic optimization, and hence we provide full formal proofs for this one. At an appropriate place, we sketch a proof of a crucial and technically non-trivial lemma which allows our results to be lifted also to $\MeanPayoffSup$.

%The main aim of the rest of this chapter is to show that: 1) similarly to mean-payoff games, memoryless deterministic strategies are sufficient for optimality in mean-payoff MDPs; but 2) unlike for games, in the MDP case the optimal strategy as well as the optimal value can be found in polynomial time. For clarity of presentation, we focus on the case when the MDP is strongly connected. This case captures the fundamental issues in analysis of mean-payoff MDPs.

%This section is structured as follows: first, we show an important linear programming connection to MDPs. Then, we exploit this connection to show how to find memoryless optimal strategies in \emph{strongly connected MDPs.} We then use this as a stepping stone for solving general MDPs in the remainder of the chapter.

%\begin{definition}
%An MDP is \emph{strongly connected} if for each pair of vertices $u,v$ there exists a finite play from $u$ to $v$. 
%\end{definition}

%\subsection*{Linear Programming for Mean-Payoff MDPs}

To simplify the subsequent notation, we define the \emph{expected one-step} reward of a vertex-action pair $(v,a)$ to be the number $\sum_{w\in \vertices} \probTranFunc(w\mid v,a)\cdot \colouring(v,w)$. Overloading the notation, we denote this quantity by $\colouring(v,a)$.

In mean-payoff  MDPs, a crucial role is played by the linear program $\lpmp$ pictured in~\cref{5-fig:mp-lin}.

%\vspace{-1em}

HOW DO WE DEAL WITH EQUATION LABELS IN FIGURES?

\begin{figure}[h]
	\begin{alignat}{2}
	\text{{maximize}} \quad &\sum_{v\in \vertices, a \in \actions} x_{(v,a)} \cdot \colouring(v,a),\quad \textrm{ 
			subject to }\nonumber \\
	  \sum_{\substack{u\in\vertices\\ a \in \actions}} x_{(u,a)}\cdot \probTranFunc(v\mid u,a) &= \sum_{a \in \actions} x_{v,a}
		&\quad&\text{for all $v\in \vertices$}\label{5-eq:mdp-flow} \\
	 \sum_{v\in\vertices,a\in \actions} x_{v,a}&=1 &&\label{5-eq:mdp-freq-1} \\
	 x_{v,a} &\geq 0  &&\text{for all $v\in \vertices,a\in \actions$} \label{5-eq:mdp-freq-nonnegp}
	\end{alignat}
	\caption{The linear program $\lpmp$ with variables $x_{(v,a)}$ for  $v\in \vertices,a\in \actions$.}
	\label{5-fig:mp-lin}
\end{figure}

%\begin{figure}[h]
%	\begin{alignat}{2}
%	\text{{maximize}} \quad &\sum_{v\in \vertices, a \in \actions} x_{(v,a)} \cdot \colouring(v,a),\quad \textrm{ 
%			subject to }\nonumber \\
%	  \sum_{\substack{u\in\vertices\\ a \in \actions}} x_{(u,a)}\cdot \probTranFunc(v\mid u,a) &= \sum_{a \in \actions} x_{v,a}
%		&\quad&\text{for all $v\in \vertices$}\label{5-eq:mdp-flow} \\
%	 \sum_{v\in\vertices,a\in \actions} x_{v,a}&=1 &&\label{5-eq:mdp-freq-1} \\
%	 x_{v,a} &\geq 0  &&\text{for all $v\in \vertices,a\in \actions$} \label{5-eq:mdp-freq-nonnegp}
%	\end{alignat}
%	\caption{The linear program $\lpmp$ with variables $x_{(v,a)}$ for  $v\in \vertices,a\in \actions$.}
%	\label{5-fig:mp-lin}
%\end{figure}

There is a correspondence between feasible solutions of $\lpmp$ and the strategies in the corresponding MDP. Intuitively, in a solution corresponding to a strategy $\sigma$, the value of a variable $x_{v,a}$ describes the expected frequency with which $\sigma$ visits $v$ and plays $a$ upon such a visit. These frequencies must obey the \emph{flow constraints}~\cref{5-eq:mdp-flow} and must represent a probability distribution, which is ensured by~\cref{5-eq:mdp-freq-1} and~\cref{5-eq:mdp-freq-nonnegp}. The objective function then represents the expected mean payoff. Of high importance is also the linear program which is \emph{dual} to $\lpmp$. This dual program, denoted $\lpmpdual$, is pictured in~\cref{5-fig:mp-dual}. (For a refresher on linear programming and duality, see, e.g.~\cite{Matousek:2007}).

\begin{figure}[h]
	\begin{alignat}{2}
	&\text{{minimize} }\quad g \quad \textrm{ 
		subject to }\nonumber \\
	&g -\colouring(v,a) + \sum_{u\in\vertices}\probTranFunc(u\mid v,a)\cdot y_u \geq y_v 
	\quad\text{for all $(v,a)\in \vertices\times \actions$}
	\end{alignat}
	\caption{The linear program $\lpmpdual$ with variables $g$ and $y_v$ for each  $v\in \vertices$.}
	\label{5-fig:mp-dual}
\end{figure}

\begin{remark}[Nomenclature]
A feasible solution of $ \lpmp $ is a vector $\vec{x} \in \R^{\vertices\times \actions} $ s.t. setting $ x_{(v,a)}=\vec{x}_{(v,a)} $ for all $ (v,a) $ satisfies the constraints in $ \lpmp $. A feasible solution of $ \lpmpdual  $ is a tuple $ (g,\vec{y}) $, where $ g\in\R $ (using the same notation for the number and the variable should not cause  confusion here) and $ \vec{y}\in \R^{\vertices}$ s.t. setting the corresponding variables to numbers prescribed by $ g $ and $ \vec{y} $ satisfies the constraints.
\end{remark}

The variable $g$ in $\lpmpdual$ is often called \emph{gain} while the vector of $y$-variables is called \emph{bias.} This is because it provides information on how much does the payoff (under some strategy) accumulated up to a certain step deviate from the estimate provided by the mean-payoff value of that strategy. This is illustrated in the following lemma, which forms the first step of our analysis. 

\begin{lemma}
\label{5-lem:dual-bound-step}
Let $({g}, \vec{y})$ be a feasible solution of $\lpmpdual$ and let $Y^{(i)}$, where $i\geq 0$, be a random variable such that $Y^{(i)}(\play)= \vec{y}_{\ing(\pi_i)}.$ Then for each strategy $\sigma$, each vertex $\vinit$, and each $n\geq 0$ it holds $\expv^\sigma_{\vinit}[\sum_{i=0}^{n-1}\colouring(\play_i)]\leq n\cdot {g}- \vec{y}_{\vinit} +\expv^\sigma_{\vinit} [Y^{(n)}]$.	
\end{lemma}
\begin{proof}
By induction on $n$. For $n=0$, both sides are equal to 0. Now assume that the inequality holds for some $n\geq 0$. By the induction hypothesis
\begin{align}
\expv^\sigma_{\vinit}[\sum_{i=0}^{n}\colouring(\play_i)] &= \expv^\sigma_{\vinit}[\sum_{i=0}^{n-1}\colouring(\play_i)] + \expv^\sigma_{\vinit}[\colouring(\play_n)] \leq n{g}- \vec{y}_{\vinit} +\expv^\sigma_{\vinit} [Y^{(n)}] + \expv^\sigma_{\vinit}[\colouring(\play_n)] \label{5-eq:mpdual-1}
\end{align}

\begingroup
\allowdisplaybreaks
\noindent
We now obtain a bound for the third term on the RHS of~\cref{5-eq:mpdual-1}. In the following, we denote by $\Pi_n$ the set of all plays of length $n$. Then we have
\begin{align*}
\expv^\sigma_v [Y^{(n)}]&= \sum_{v\in\vertices}  \vec{y}_v\cdot\probm^\sigma_{\vinit}(\ing(\pi_n)=v)
=\sum_{v\in\vertices}  \vec{y}_v\cdot \bigg(\sum_{\substack{\play'\in \Pi_n\\ \last(\play')=v}}\probm^\sigma_{\vinit}(\play') \bigg) \\
&=\sum_{v\in\vertices}  \vec{y}_v\cdot \bigg(\sum_{\substack{\play'\in \Pi_n\\ \last(\play')=v}}\probm^\sigma_{\vinit}(\play')\cdot\big(\underbrace{\sum_{a\in\actions}\sigma(a\mid \play')}_{=1} \big)\bigg) \\
&= \sum_{\substack{v\in\vertices\\ a \in \actions}}  \vec{y}_v\cdot \bigg(\sum_{\substack{\play'\in \Pi_n\\ \last(\play')=v}}\probm^\sigma_{\vinit}(\play')\cdot\sigma(a\mid \play')\bigg)\\
\text{(by~\cref{5-fig:mp-dual})}\hspace{3mm}
&{\leq}\sum_{\substack{v\in\vertices\\ a\in \actions}} \bigg( {g} -\colouring(v,a) + \sum_{u\in\vertices}\probTranFunc(u\mid v,a)\cdot  \vec{y}_u\bigg)\cdot\bigg(\sum_{\substack{\play'\in \Pi_n\\ \last(\play')=v}}\probm^\sigma_{\vinit}(\play')\cdot\sigma(a\mid \play')\bigg)\\
&= {g}\cdot\underbrace{\sum_{\substack{v\in\vertices\\ a\in \actions}}\sum_{\substack{\play'\in \Pi_n\\ \last(\play')=v}}\probm^\sigma_{\vinit}(\play')\cdot\sigma(a\mid \play')}_{=1}\\&\quad-\underbrace{\sum_{\substack{v\in\vertices\\ a\in \actions}}\sum_{\substack{\play'\in \Pi_n\\ \last(\play')=v}}\probm^\sigma_{\vinit}(\play')\cdot\sigma(a\mid \play')\cdot\colouring(v,a)}_{=\expv^\sigma_{\vinit}[\colouring(\play_n)]}\\
&\quad +\underbrace{\sum_{\substack{v,u\in \vertices\\ a \in \actions}}\sum_{\substack{\play'\in \Pi_n\\ \last(\play')=v}} \probm^\sigma_{\vinit}(\play')\cdot\sigma(a\mid \play')\cdot \probTranFunc(u\mid v,a)\cdot  \vec{y}_u}_{=\expv^\sigma_{\vinit}[Y^{(n+1)}]}.
\end{align*}
\endgroup

Plugging this into~\cref{5-eq:mpdual-1} yields the desired $\expv^\sigma_{\vinit}[\sum_{i=0}^{n}\colouring(\play_i)] \leq (n+1) {g}- \vec{y}_{\vinit} + \expv^\sigma_{\vinit}[Y^{(n+1)}]$. 
%We remark that the equalities on the last three lines of the above computations follow directly from the definition of $\probm^\sigma_{\vinit}$ and we leave the formal proof to the reader as an exercise.
\end{proof}

\begin{corollary}
\label{5-cor:mp-value-bound}
Let $g$ be the objective value of some feasible solution of $\lpmpdual$. Then for every strategy $\sigma$ and every vertex $\vinit$ it holds $\playPay(\vinit,\sigma) \leq \stepPay(\vinit,\sigma) \leq g$.
\end{corollary}
\begin{proof}
Let $ ({g}, \vec{y})$ be any feasible solution of $\lpmpdual$.
By \cref{5-lem:dual-bound-step} we have, for every $n\geq 0$, that $\expv^\sigma_{\vinit}[\frac{1}{n}\sum_{i=0}^{n-1}\colouring(\play_i)]\leq g - \frac{ \vec{y}_{\vinit}}{n}+ \frac{1}{n}\expv^\sigma_{\vinit} [Y^{(n)}]$. Since $ \vec{y}_{\vinit}$ is a constant and $|\expv^\sigma_{\vinit} [Y^{(n)}]|$ is bounded by the constant $ \max_{v\in \vertices}|\vec{y}_v|$ that is independent of $n$, the last two terms on the RHS vanish as $n$ goes to $\infty$. Hence, also $\stepPay(\vinit,\sigma) = \liminf_{n\rightarrow \infty} \expv^\sigma_{\vinit}[\frac{1}{n}\sum_{i=0}^{n-1}\colouring(\play_i)] \leq g$. It remains to show that we have $\playPay(\vinit,\sigma) \leq \stepPay(\vinit,\sigma)$, but this immediately follows from the Fatou's lemma~\cite[Theorem 1.6.8]{Ash&Doleans-Dade:2000}.
\end{proof}

\begin{corollary}
\label{5-cor:lpmp-optimal-exists}
Both the linear programs $\lpmp$ and $\lpmpdual$ have a feasible solution. Hence, both have an optimal solution and the optimal values of the objective functions in these programs are equal.
\end{corollary}
\begin{proof}
One can easily construct a feasible solution for $\lpmpdual$ by setting all the $y$-variables to $0$ and $g$ to $\maxc$. By the duality theorem for linear programming, to show that also $\lpmp$ has feasible solution it suffices to show that the objective function of $\lpmpdual$ is bounded from below. But this follows from \cref{5-cor:mp-value-bound}, since there is at least one strategy $\sigma$ giving us the lower bound (in particular, the objective function is bounded from below by $-\maxc$). The second part follows immediately by linear programming duality.
\end{proof}


%\textbf{Petr: Add sketch for $\MeanPayoffSup$ (via martingale bound)}

%The following lemma lifts~\Cref{5-cor:mp-value-bound} to $\MeanPayoffSup$.
