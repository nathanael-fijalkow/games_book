% !TEX root = ../8_standalone.tex
\newcommand{\probimp}[3]{\mathbb{P}^{#1}_{#2}\left({#3}\right)}
\newcommand{\rand}{{\tt rand}}
\newcommand{\Isafe}{{\tt ISafe}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\KK}{\mathcal{K}}
\newcommand{\LLE}{\LL_{\text{Eve},=1}}
\newcommand{\LLA}{\LL_{\text{Adam},>0}}
\newcommand{\can}{\textsf{max}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\targets}{TT}
\newcommand{\bh}{\setminus}
\newcommand{\signauxdeux}{T}
\newcommand{\actionsun}{A}


\paragraph{Imperfect information.}
This chapter presents a few results about zero-sum games with imperfect information.
Those games are a generalization of concurrent games in order to take into account the possibility that players might be imperfectly informed about the current state of the game
and the actions taken by their opponent, or even their own action. We will also discuss situations where players may forget what they used to know.

Before providing formal definitions of games with imperfect information,
we give three examples.

\newcommand{\ini}{\delta_0}

\paragraph{Simple poker.}
%On the Borel and von Neumann Poker Models
%Chris Ferguson, Bright Trading, Westwood, California Thomas S. Ferguson, University of California, Los Angeles

Our first example is a finite duration game which is a simplified version of poker,
inspired by Borel and von Neumann simplified poker~\cite{ferguson}.
This game is played with  $4$ cards $\{\spadesuit,\heartsuit,\clubsuit,\diamondsuit\}$.

\begin{itemize}
\item The goal of Eve and Adam is to win
the content of a pot in which, initially, they both put $1$ euro.
\item Eve receives a private random card, unknown by Adam.
\item Eve decides whether to "check" or "raise".
If she "checks" then she wins the pot iff her card is $\spadesuit$.
\item If Eve raises then Adam has two options: "fold"
or "call". If Adam folds then Eve receives the pot.
If Adam raises then both player add two euros in the pot
and Eve wins the pot iff her card is $\spadesuit$.
\end{itemize}

A natural strategy for Eve is to raise when she has a spade and otherwise
check. Playing so, she reveals her card to Adam,
and we will see that the optimal behaviour for her
consists in "bluffing" from time to time,
i.e. raise although her card is not a spade.

\paragraph{The distracted logician.}

Our second example is another finite duration game.
A logician is driving home. For that he should go through two crossings,
and turn left at the first one and right at the second one.
This logician is very much absorbed in his thoughts,
trying to prove that $P\neq NP$,
and is thus pretty distracted: upon taking a decision, he cannot  tell
whether he already saw a crossing or not.

This simple example is useful to discuss the observability of actions
and make a distinction between
mixed strategies and behavioral strategy.
 
 \paragraph{Network controller.}
The following example is inspired from collision regulation
in ethernet protocols: the controller of a network card
has to share an ethernet layer with
another network card, controller by another controller,
possibly malicious.

When sending a data packet,
the controller selects a delay in microseconds between $1$ and $512$
and transmits this delay to the network card.
The other controller does the same.
The network cards try to send their data packet at the chosen dates.
Choosing the same date results in a data collision, and the process is repeated until
there is no collision, at that time the data can be sent.

The chosen delay has to be kept hidden from the opponent.
This way, it can be chosen randomly,
which ensures that the data will eventually be sent with probability $1$,
whatever does the opponent.
 
 
\paragraph{Guess my set.}

Our third example is an infinite duration game,
parametrized by some integer $n$.
The play is
divided into three phases.
\begin{itemize}
\item
In the first phase, Eve secretly chooses a subset
$X \subsetneq \{1, \ldots,2n\}$ of size $n$
among the $\binom{2n}{n}$ possibilities.
\item
In the second phase, Eve discloses to Adam
$\frac{1}{2}\binom{2n}{n}$ pairwise distinct sets of size
$n$ which are all different from $X$. 
\item
In the third phase, Adam aims at guessing $X$ by trying up to
$\frac{1}{2} \binom{2n}{n}$ sets of size $n$. 
If Adam succeeds in guessing $X$,
the game restarts from the beginning. Otherwise, 
Eve wins.
\end{itemize}

Clearly Adam has a strategy to prevent forever
Eve to win: try up one by one all those sets
that were not disclosed by Eve.
This strategy uses a lot of memory:
Adam has to remember the whole sequence of $\frac{1}{2} \binom{2n}{n}$
 sets disclosed by Eve.
We will see that a variant of this game can be represented 
in a compact way, using a number of states polynomial in $n$.
As a consequence,
playing optimally a game with imperfect-information and infinite duration
might require a memory of size doubly-exponential in the size of the game.


\section{\label{subsec:formalimp}Playing stochastic games with signals.}
We consider \emph{stochastic games with
  signals}, that are a standard tool in game theory to model {imperfect information in stochastic games}~\cite{sorinafirst,dinahnicolas1,renault2}.
When playing a stochastic game with signals, players cannot observe
the actual state of the game, nor the actions played by themselves or
their opponent: the only source of information of a player are private
signals they receive throughout the play.  Stochastic games with
signals subsume standard stochastic games~\cite{shapley}, repeated
games with incomplete information~\cite{aumann}, games with imperfect
monitoring~\cite{dinahnicolas1}, concurrent games~\cite{dAH00} and
deterministic games with imperfect information on one
side~\cite{reif,chdr07}.

%\newcommand{\SE}{S_{\mEve}}
%\newcommand{\SA}{S_{\mAdam}}
Like in previous chapters, $V$, $C$ and $A$  
denote respectively the sets
	of vertices, colors and actions.
\begin{definition}
An imperfect information arena $\arena$ is a tuple $(S,\Delta)$ where 
\begin{itemize}
	\item $S$ is the set of \emph{signals}
	\item $\Delta : V \times A \times A \to \dist(V \times S \times S \times C)$
	 maps the current vertex and a pair of actions to a probability distribution
	 over vertices, pairs of signals and colors.
\end{itemize}
\end{definition}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}


Initially, the game is in a state $v_0 \in V$ chosen according to a probability distribution
$\ini\in\dist(V)$ known by both players; the initial state is
$v_0$ with probability $\ini(v_0)$.  At each step $n\in\NN$, both players
simultaneously choose some actions $a,b \in A$
 They respectively receive signals
$s,t \in S$ ,
 and the game moves to a
new state $v_{n+1}$.  This happens with probability
$\Delta(v_{n},a,b)(v_{n+1},c,d)$.
{This fixed probability is known by both players,
as well as the whole description of the game.}

A \emph{play} is a sequence $(v_0,a_0,b_0,s_0,t_0,c_0),(v_1,a_1,b_1,s_1,t_1,c_1),(v_2\ldots$
such that for every $n$, the probability $\Delta(v_{n},a_n,b_n)(v_{n+1},s_n,t_n,c_n)$
is positive.

A sequence of signals for a player
is \emph{realisable} for Eve if it appears in a play,
we denote $R_E \subseteq S^*$ the set of these sequence.
Similarly for Adam.


\paragraph{An example.}
The simplified poker can be 
modelled as a stochastic game with signals.
Actions of players are \emph{public signals}
sent to both players.
Also their the payoff of Eve is publicly announced,
when non-zero. 
Upon choosing whether to call or fold,
Adam cannot distinguish between states
$\spadesuit${\tt Raised} and $\blacksquare${\tt Raised},
in both cases he received the sequence of signals $\circ,{\tt raise}$.
A graphical representation is provided on
Figure~\ref{9-fig:poker}.

% In case a transition sends the dummy signal
%$\circ$ to both players, the signals are not represented.
\newcommand{\pay}{ {\tt pay}}


\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=0.8]

\node (root) at (0,0) {{\tt Start}};
\node[below left =of root] (spade) {$\spadesuit${\tt Play}};
\node[below right =of root] (nospade) {$\blacksquare${\tt Play}};
\node[below left =of spade] (spaderaise) {$\spadesuit${\tt Raised}};
\node[below right =of nospade] (nospaderaise) {$\blacksquare${\tt Raised}};
\node(end) at (0,-5) {{\tt End}};

\path[->](root) edge node[near start,left,align=center]  
{$(\cdot,\cdot)\frac{1}{4}$
\\Eve receives $\spadesuit$
\\Adam receives $\circ$
} (spade);
\path[->](root) edge node[near start,right,align=center]  {$(\cdot,\cdot)\frac{3}{4}$\\Eve receives $ \blacksquare$\\Adam receives $\circ$} (nospade);
\path[->](spade) edge node[near start,left,align=center]  {$({\tt raise},\cdot)
%\stackrel{1}{\to}(\circ,{\tt EveRaised})
$
%\\Adam receives {\tt EveRaised}
} (spaderaise);
\path[->](nospade) edge node[near start,right,align=center]  {$({\tt raise},\cdot)
%\stackrel{1}{\to}(\circ,{\tt EveRaised})
$
%\\Adam receives {\tt EveRaised}
} (nospaderaise);
\path[->](spade) edge 
node[very near start,right] {$({\tt check},\cdot)$}
node[near end, right] {{\bf +1}} (end);
\path[->](nospade) edge 
node[very near start, left]  {$({\tt check},\cdot)
%\stackrel{1}{\to}(\circ,\circ)
$} 
node[near end, left] {{\bf -1}} 
(end);
\path[->,bend left=0](spaderaise) edge 
node[very near start, right]  {$(\cdot,{\tt call})
%\stackrel{1}{\to}(\circ,\circ)
$} 
node[near end, right] {{\bf +3}} 
(end);
\path[->,bend left=-80](spaderaise) edge 
node[near start, right]  {$(\cdot,{\tt fold},)
%\stackrel{1}{\to}(\circ,\circ)
$} 
node[right,near end] {{\bf +1}} 
(end);

\path[->,bend left=0](nospaderaise) edge 
node[very near start, left]  {$(\cdot,{\tt call})
%\stackrel{1}{\to}(\circ,\circ)
$} 
node[near end, left] {{\bf -3}} 
(end);
\path[->,bend left=80](nospaderaise) edge 
node[near start, left]  {$(\cdot,{\tt fold},)
%\stackrel{1}{\to}(\circ,\circ)
$} 
node[left,near end] {{\bf +1}} 
(end);

% (ab) edge [loop] (ab)
% (notb) edge  node [above] {} (notab)
% (notab) edge   (ab)
% (ab) edge[out=195,in=-15] (notab)
% (ab) edge[out=205,in=-25] (notb)
;

    \end{tikzpicture}
    \caption{The simplified poker game.}
    \label{9-fig:poker}
  \end{figure}


The game is played with  $4$ cards $\{\spadesuit,\heartsuit,\clubsuit,\diamondsuit\}$.
We exploit the symmetry of payoffs with respect to $\{\heartsuit,\clubsuit,\diamondsuit\}$ and identify these three colours 
as a single one, denoted $\blacksquare$, received initially by Eve with probability $\frac{3}{4}$.
The set of vertices
is an initial vertex ${\tt Start}$,
a terminal vertex ${\tt End}$
plus the four states
\[
\{\spadesuit,\blacksquare\} \times 
 \{{\tt Play,Raised}\}\enspace.
 \]
The set of colors are possible payoffs $C=\{0,-1,+1,-3,+3\}$.

The set of actions $A$ 
is the union of 
actions of Eve 
$A_E=\{{\tt \cdot, check,raise}\}$
and actions of Adam
$A_A=\{{\tt \cdot, call, fold}\}$.

The set of signals is $\{\circ , \spadesuit, \blacksquare\}$ plus
$\{{\tt check},{\tt raise},{\tt call},{\tt fold}\}\times \{0,-1,+1,-3,+3\}$.

The rules of the game,
are defined by the set of \emph{legal} transitions.
Let $c \in \{\spadesuit,\blacksquare\}$.
The following transitions are legal.
\begin{align*}
&~\Delta({\tt Start},{\tt \cdot},{\tt \cdot})((c,{\tt Play}),c,\circ,0)=
\begin{cases}
\frac{1}{4}& \text{ if } c= \spadesuit\\
\frac{3}{4}& \text{ if } c= \blacksquare\enspace.
\end{cases}\\
&~\Delta((c,{\tt Play}),{\tt check},{\tt \cdot})({\tt End},{\tt check}_x,{\tt check}_x,x)=1
\text{ where } x=
\begin{cases}
+1 & \text{ if } c=\spadesuit\\
-1& \text{ if } c=\blacksquare.
\end{cases}
\\
&~\Delta((c,{\tt Play}),{\tt raise},{\tt \cdot})((c,{\tt Raised}),{\tt raise_0},{\tt raise_0},0)=1\\
&~\Delta((c,{\tt Raised}),{\tt \cdot},{\tt call})({\tt End},{\tt call}_x,{\tt call}_x,x)=1 
\text{ where } x=
\begin{cases}
+3 & \text{ if } c=\spadesuit\\
-3 & \text{ if } c=\blacksquare.
\end{cases}
\\
&~\Delta((c,{\tt Raised}),{\tt \cdot},{\tt fold})({\tt End},{\tt fold_1},{\tt fold_1},+1)=1\\
&~\text{state ${\tt End}$ is absorbing with payoff $0$.}
\end{align*}




To simplify the notations,
we assumed in the general case
that players share the same set of actions and signals.
As a consequence, other transitions than the legal ones
are possible. 
One can use a threat to guarantee that Eve plays
${\tt check}$ and ${\tt raise}$ after receiving her card,
by setting a heavy loss of $-10$ if she plays another action instead.
Same thing to enforce that Adam plays ${\tt call}$ or ${\tt fold}$
after receiving the signal ${\tt raise}$.
When targetting applications,
legal moves should be explicitely
specified, typically using an automaton
to compute the set of legal actions
depending on the sequence of signals.



\paragraph{Strategies: behavioral, mixed and general.}

Intuitively, players make their decisions based upon the sequence of
signals they receive, which is formalised with strategies. 
 There are several natural classes of strategies to play  games
 with signals,
as discussed in~\cite{horn_remember} and~\cite[Section 4]{BGGjacm}. 
.

A behavioural strategy of Eve associates
with every realisable sequence of signals a probability distribution
over actions:  
\[
\sigma: R_E \to \dist(A)\enspace.
\]
When Eve plays $\sigma$, after having received a sequence of signals
$s_0,\ldots,s_n$ she chooses action $a$ with probability
$\sigma(s_0,\ldots,s_n)(a)$. 
Strategies of Adam are the same, except they are defined on $R_A$.


Remark that in general a player may not observe which actions he actually played,
for example $S$ might be a singleton 
in which case the players only knows the number of steps so far.
\newcommand{\Act}{\text{Act}}
A game has \emph{observable actions} if there exists a mapping
 $\Act:S \to A$ 
 such that
\[
\Delta(v,a,b)(w,s,t)>0 
\implies
(a=\Act(s) \land b=\Act(t))\enspace. 
\]

In~\cite[Lemma 4.6 and 4.7]{BGGjacm} it was shown that without loss of generality,
one can consider games where actions are observable and players 
play behavioural strategies. The discussion is technical and beyond the scope of this book.


%\begin{abstract}
%We consider two-person zero-sum stochastic games with signals, a standard model of stochastic games with imperfect information. The only source of information for the players consists of the signals they receive; they cannot directly observe the state of the game, nor the actions played by their opponent, nor their own actions.
%
%We are interested in the existence of almost-surely winning or positively winning strategies, under reachability, safety, B{\"u}chi, or co-B{\"u}chi winning objectives, and the computation of these strategies when the game has finitely many states and actions. We prove two qualitative determinacy results. First, in a reachability game, either player 1 can achieve almost surely the reachability objective, or player 2 can achieve surely the dual safety objective, or both players have positively winning strategies. Second, in a B{\"u}chi game, if player 1 cannot achieve almost surely the B{\"u}chi objective, then player 2 can ensure positively the dual co-B{\"u}chi objective. We prove that players only need strategies with finite memory. The number of memory states needed to win with finite-memory strategies ranges from one (corresponding to memoryless strategies) to doubly exponential, with matching upper and lower bounds. Together with the qualitative determinacy results, we also provide fix-point algorithms for deciding which player has an almost-surely winning or a positively winning strategy and for computing an associated finite-memory strategy. Complexity ranges from EXPTIME to 2EXPTIME, with matching lower bounds. Our fix-point algorithms also enjoy a better complexity in the cases where one of the players is better informed than their opponent.
%
%Our results hold even when players do not necessarily observe their own actions. The adequate class of strategies, in this case, is mixed or general strategies (they are equivalent). Behavioral strategies are too restrictive to guarantee determinacy: it may happen that one of the players has a winning general strategy but none of them has a winning behavioral strategy. On the other hand, if a player can observe their actions, then general, mixed, and behavioral strategies are equivalent. Finite-memory strategies are sufficient for determinacy to hold, provided that randomized memory updates are allowed.
%\end{abstract}

\section{Finite duration.}

We start with some results on the very interesting class of game
with finite duration.

A game has \emph{finite duration}
if there is a set of absorbing vertices $L$, called \emph{leaves},
such that every play eventually reaches $L$.
In other words, the directed graph $(V,E)$ induced by all pairs $(v,w)$
such that 
$\exists a,b\in A, s,t \in S, \Delta(v,a,b)(w,s,t) > 0$
is acyclic, except for self loops on leaves.

Moreover, $C$ is the set of real numbers,
colours are called \emph{payoffs}.
At the moment the play $\pi$ reaches a leaf $\ell\in L$
for the first time,
the game is essentially over:
Eve receives the sum of payoffs seen to far,
denoted ${\tt pay}(\pi)$ and all future payoffs are $0$.
Such plays are called \emph{terminal plays}.

Once a terminal play occurs, the game is over.
For this reason, in this section we restrict realisable sequences of signals
to the ones occurring in terminal plays and their prefixes.
This guarantees finiteness of $R_E$ and $R_A$ since
\[
R_E \cup R_A \subseteq S^{\leq n}\enspace.
\]


An initial distribution $\ini$ and two strategies $\sigma$ and $\tau$ of Eve and Adam naturally induce a probability distribution $\mathbb{P}_{\ini}^{\sigma,\tau}$
on the set of terminal plays starting in one of the vertices $v_0, \ini(v_0)>0$.
Players have opposite interests:
Eve seeks to maximize her expected payoff
\[
\mathbb{E}_{\ini}^{\sigma,\tau}= \sum_{\text{ terminal plays }\pi} 
\mathbb{P}_{\ini}^{\sigma,\tau}(\pi) \cdot {\pay}(\pi)\enspace,
\]
while Adam wants to minimize it.


\subsection{Existence and computability of the value}

Next theorem gathers several folklore results.

\begin{theorem}\label{thm:finiteimperfecthaveval}
A game with finite duration and imperfect information has a value:
for every initial distribution $\ini$,
\[
\sup_\sigma \inf_\tau \mathbb{E}_{\ini}^{\sigma,\tau}
~=~
 \inf_\tau \sup_\sigma \mathbb{E}_{\ini}^{\sigma,\tau}\enspace.
\]
This value is denoted $\val(\ini)$
and is computable~\footnote{provided payoffs are presented in a way
compatible with linear solvers, typically 
rational values.}.
Both players have optimal strategies.
\end{theorem}

\paragraph{Reduction to normal form.}
The main ingredient for proving this theorem is a transformation
of the game into a matrix game called its \emph{normal form}.

The intuition is that a player,
instead of choosing progressively her actions
as she receives new signals,
may choose once for all at the beginning of the game
how to react to every possible sequence of signals
she might receive in the future.

Fix an initial distribution $\ini$.
In the normal form version the game,
Eve  picks 
a \emph{deterministic} strategy
$\sigma : R_E \to A$
while simultaneously
Adam picks
$\tau : R_A \to A$.
Then the game is over
and Eve receives payoff
$\mathbb{E}_{\ini}^{\sigma,\tau}$.
There are finitely many such deterministic strategies,
thus the normal form game is a \emph{matrix game}.
See Section~\ref{sec:matrixgames} for more details
about matrix games.


\paragraph{An example.}
In the simplified poker example,
the reduction is as follows.

We rely on the formal description of the game 
at the end of Section~\ref{subsec:formalimp}
and perform two simplifications.
First, we only consider strategies playing moves according to the rules,
other strategies are strategically useless.


Deterministic strategies of Eve are
mappings $\sigma : \{\spadesuit,\blacksquare\}
\to\{ {\tt check},{\tt raise}\}$.
Adam has only two deterministic strategies:
after the sequence $\circ {\tt Raised}$,
he should choose between
actions ${\tt call}$ and ${\tt fold}$.

The normal form is
\[
\begin{array}{c|c|c}
&  {\tt call} &{\tt fold}\\
\hline
\spadesuit\to {\tt check},  \blacksquare\to {\tt check}
& -0.5 & -0.5\\
\hline
\spadesuit\to {\tt raise},  \blacksquare\to {\tt check}
& 0 & -0.5\\
\hline
\spadesuit\to {\tt raise},  \blacksquare\to {\tt raise}
& -1.5 & +1\\
\hline
\spadesuit\to {\tt check},  \blacksquare\to {\tt raise}
& -2 & +1\\
\end{array}
\]
The first line corresponds to Eve never raising,
thus her odds are +1 euro at $25\%$ 
and -1 at $75\%$ thus an expected payoff of
$-0.5$.
The third line corresponds to Eve always raising.
If Adam calls then her odds are +3 at $25\%$
and -3 at $75\%$, on average $-1.5$.
If Adam folds, she gets payoff +1.

Remark that the rows where Eve checks with $\spadesuit$
are dominated by the corresponding row where Eve does not.
Thus checking with $\spadesuit$ (slow playing) has no strategic interest,
and  by elimination of weakly dominated strategies,
the normal form game is equivalent to:
\[
\begin{array}{c|c|c}
&  {\tt call} &{\tt fold}\\
\hline
\spadesuit\to {\tt raise},  \blacksquare\to {\tt check}
& 0 & -0.5\\
\hline
\spadesuit\to {\tt raise},  \blacksquare\to {\tt raise}
& -1.5 & 1\\
\end{array}
\]
The value of this game is $-\frac{1}{4}$.
Eve has a unique optimal strategy which consists in playing the top row with probability
$\frac{5}{6}$.
In other words, she should bluff with probability $\frac{1}{6}$ when she receives $\blacksquare$.
Adam has a unique optimal strategy which consists in calling or folding
with equal probability $\frac{1}{2}$\enspace.
 
 
\paragraph{Proof of Theorem~\ref{thm:finiteimperfecthaveval}.}
The example illustrates
the correspondance between behavioural strategies in the finite-duration game on one side
and mixed strategies in the normal form game on the other.
In the general case, the correspondance can be stated as follows.

\newcommand{\Strat}{\text{Strat}}
\begin{lemma}\label{lem:impinffinite}
Denote $\Strat$ the set of behavioural strategies,
$\Strat_d$ the subset of deterministic strategies
and $\dist(\Strat_d)$ the set of strategies in the normal form game.
\begin{enumerate}
\item[i)]
There is a mapping 
$
\Phi : \Strat \to \dist(\Strat_d)
$ 
%from strategies in the imperfect information game to strategies
%in the normal form game 
which preserves payoffs:
\[
\forall \sigma,\tau \in \Strat,
\mathbb{E}_{\ini}^{\sigma,\tau}
=
\sum_{\sigma',\tau' \in \Strat_d}\Phi(\sigma)(\sigma')\cdot\Phi(\tau)(\tau') 
\cdot\mathbb{E}_{\ini}^{\sigma',\tau'}\enspace.
\]
\item[ii)]
Since actions are observable,
there is a mapping 
$
\Phi' : \dist(\Strat_d) \to \Strat 
$ 
%from strategies in the normal form game to strategies 
%in the  imperfect information game  
which preserves payoffs:
\[
\forall \Sigma,T \in \dist(\Strat),
\sum_{\sigma',\tau' \in \Strat_d}\Sigma(\sigma') T(\tau')
\mathbb{E}_{\ini}^{\sigma',\tau'}
=
\mathbb{E}_{\ini}^{\Phi'(\sigma),\Phi'(\tau)}
\enspace.
\]
\item[iii)]
$\Phi'\circ \Phi$ is the identity.
\end{enumerate}
\end{lemma}

We assumed earlier that each player can observe
its own actions. This hypothesis is necessary for ii) and iii)
to hold in general.

\begin{proof}
We start with i).
Intuitively,
all random choices of actions performed by
a behavioural strategy $\sigma$ of Eve can be done at the beginning of the play.
Playing $\sigma$ 
is equivalent to playing each deterministic strategy $\sigma'$ 
with probability
\[
\Phi(\sigma)(\sigma') = 
\Pi_{u \in R_E} \sigma(u)(\sigma'(u))\enspace.
\]

We prove ii).
Let $\Sigma\in\dist(\Strat)$. The definition of the behavioural strategy
$\sigma=\Phi'(\Sigma)$ is as follows.
Let $s_0\ldots s_k$ be a finite sequence of signals.
Since actions are observable, this defines unambigously
the sequence of corresponding actions $a_0\ldots a_k$
where $a_i = \Act(s_i)$.
%Let $Z$ be the subset of deterministic strategies
%consistent with $s_0\ldots s_k$ and $a_0\ldots a_k$ :
%\[
%Z =\{ \sigma' \in \Strat_d \mid \forall 0\leq i \leq k,
% \sigma'(s_0\ldots s_{i-1})=\Act(s_i)
% \} \enspace.
%\]
We set $\sigma(s_0\ldots s_k)(a)$ to be the probability that a 
deterministic strategy
chosen with $\Sigma$ chooses action $a$ after signals
$s_0\ldots s_k$, conditioned on the fact that it has already
chosen action $a_0\ldots a_k$:
\[
\sigma(s_0\ldots s_k)(a) 
=
\Sigma\left(\sigma'(s_0\ldots s_k)=a \mid \forall 0\leq i \leq k,
 \sigma'(s_0\ldots s_{i-1})=\Act(s_i)\right)\enspace,
\]
where the vertical pipe denotes a conditional probability.
%The proof of  i), ii) and iii) is straightforward.
\end{proof}

We proceed with the proof of  Theorem~\ref{thm:finiteimperfecthaveval}.
%\begin{proof}[of]
According to Theorem~\ref{lem:mat},
the normal form has a value and optimal strategies
for each player. 
Denote $\val_N$ the value
and $\Sigma^\sharp$ and $T^\sharp$ the optimal strategies.
Let $\sigma^\sharp=\Phi'(\Sigma^\sharp)$.
Then $\sigma^\sharp$ ensures a payoff
of at least $\val_N$ in the imperfect information game,
because for every strategy $\tau$,
\[
\mathbb{E}_{\ini}^{\sigma^\sharp,\tau}
=
\mathbb{E}_{\ini}^{\Phi'(\Sigma^\sharp),\Phi'(\Phi(\tau))}
=
\sum_{\sigma',\tau' \in \Strat_d}\Sigma^\sharp(\sigma') \Phi(\tau)(\tau')
\mathbb{E}_{\ini}^{\sigma',\tau'}
\geq \val_N\enspace,
\]
where the first equalities are applications of 
Lemma~\ref{lem:impinffinite}
and the inequality is by optimality of $\Sigma^\sharp$.
Symmetrically, 
$\tau^\sharp=\Phi'(T^\sharp)$ guarantees 
$\forall \sigma,\mathbb{E}_{\ini}^{\sigma,\tau^\sharp}\leq\val_N$. 
Thus the value of the game with finite duration
is $\val_N$ and $\sigma^\sharp$
and $\tau^\sharp$ are optimal.\qed
%\end{proof}

\subsection{ The Koller-Meggido-von Stengel reduction to linear programming}




The reduction of a finite-duration game with imperfect information
to its normal form proves that the value exists and
is computable.
However the corresponding algorithm is computationally
very expensive, it requires solving
a linear program of size roughly doubly-exponential in the size 
of the original game, since the normal form is a matrix
index by $A^{R_E} \times A^{R_A}$ and the set of signal sequences
might contain all sequences of $S$ of length $\leq n$.

Koller, Meggido and von Stengel did provide a
more efficient direct reduction to linear programming.
Strategies of Eve in the normal form game live
in $\RR^{A^{R_E}}$
while her strategies in the game with imperfect information
live in a space
with exponentially fewer dimensions, namely
$\RR^{R_E\times A}$.
The direct reduction avoids this dimensional blowup.


\begin{theorem}
The value of a game with imperfect information
can be computed by a linear program with
$|R_E| + |R_A|$ variables.
\end{theorem}

As a consequence, in the particular case where the game graph is a tree
then $|R_E|\leq n$ and $|R_A|\leq n$
and the value can be computed in polynomial time,
like stated in~\cite{stengel}.

\begin{proof}
The construction of the linear program relies on three key ideas.

First, representing a behavioral strategy $\sigma:R_E \to \dist(A)$
 of Eve as a \emph{plan} $\pi:R_E  \to [0,1]$
 recursively defined by $\pi(\epsilon) = 1$
 and for every $s_0\cdots s_n \in R_E, s\in S$,
 \begin{align*}
& \pi(s_0\cdots s_n\cdot s) = \pi(s_0\cdots s_n) \cdot
 \sigma(s_0\cdots s_n)(\Act(s))\enspace.
 \end{align*}
Remind that actions are observable and $\Act(s)$
denotes the action that Eve has just played
before receiving signal $s$.
In the linear program, plans are represented by variables 
$\left(p_r\right)_{r \in R_E}$. 
Valuations corresponding to plans can be characterized by 
the following equalities.
First, $p_\epsilon = 1$.
Second, for every 
$s_0\ldots s_{n-1}s,s_0\ldots s_{n-1}s' \in R_E$,
\begin{align*}
(\Act(s)=\Act(s')) \implies \left(p_{s_0\ldots s_{n-1}s}= p_{s_0\ldots s_{n-1}s'}\right)\enspace.
\end{align*}
We denote $p_{s_0\ldots s_{n-1}a}$ the common value of
all $p_{s_0\ldots s_{n-1}s}$ with $a=\Act(s)$.
The third equality is 
$p_{s_0\ldots s_{n-1}}=\sum_{a\in A} p_{s_0\ldots s_{n-1}a}$ \enspace.

 The second key idea is to introduce variables evaluating the contribution of a 
 (realisable) sequence
 of signals of Adam to the total expected payoff Eve.
 These contributions are represented by variables $(v_r)_{r \in R_A}$.
% and the linear program minimizing these contributions.
 
 The third key idea is to aggregate the product of transition
 probabilities along a play.
For every play $(v_0,a_0,b_0,s_0,t_0,c_0),\ldots,(v_k,a_k,b_k,s_k,t_k,c_k)$
 we denote $\mathbb{E}(\pi)$ the product of all transition
probabilities of $\pi$ and $r_{E}(\pi)$ the sequence of signals of Eve in this play:
\begin{align*}
&\mathbb{E}(\pi) = \ini(v_0)\cdot \Delta(v_0,a_0,b_0,s_0,t_0,c_0)
  \cdots \Delta(v_k,a_k,b_k,s_k,t_k,c_k)\\
 & r_{E}(\pi) = s_0,s_1,\ldots,s_k\enspace.
\end{align*}
 
  
We show that the following linear program with variables
  $(p_r)_{r \in R_E}$, $(v_r)_{r\in R_A}$
  has an optimal solution which equals to $\val(\ini)$.
 For every sequences of signals $r \in R_A$
 we denote $T_A(r)$ the (possibly empty)
 set of terminal plays whose sequence of signals for Adam is $r$.

%\begin{tcolorbox}  
\begin{align}
&\text{Maximise $v_{\epsilon}$ subject to}
\notag\\
\notag\\
\notag&\text{$\left(p_r\right)_{r \in R_E}$ is a plan of Eve}
\notag\\
\notag\\
\notag\forall r \in R_A,
\forall a \in A,&
\\
%\forall u=(o_0,\ldots, o_n) \in \viewsI 
&
\label{eq:implp2}
v_{r} \leq \sum\limits_{\substack{rs \in R_A\\s \in S, \Act(s)=a}}
v_{rs}~+~\sum\limits_{\pi \in T(r)} \mathbb{E}(\pi) \cdot \pay(\pi) \cdot 
p_{r_E(\pi)}
%\enspace.
\end{align}
%\end{tcolorbox}  

For our purpose,
it is enough to establish 
that the optimal solution of the LP
is 
\[
\val(\ini) = \sup_\sigma \min\limits_{\tau\text{ deterministic}} \mathbb{E}_{\ini}^{\sigma,\tau}\enspace.
\]
The reason is that in a matrix game,
for every fixed strategy of Eve,
Adam can minimize the payoff by playing a single action
with probability $1$.
Thus, according to the reduction to normal form seen in the previous chapter,
for every strategy $\sigma$ of Eve,
there is a \emph{deterministic} strategy $\tau$ of Adam
which minimizes $\mathbb{E}_{\ini}^{\sigma,\tau}$.



We show first that for every feasible solution 
$(p_r)_{r \in R_E}$, $(v_r)_{r\in R_A}$ of the linear program,
the strategy $\sigma$ corresponding to the plan $(p_r)_{r \in R_E}$
guarantees that for every \emph{deterministic} strategy $\tau$,
$\mathbb{E}_{\ini}^{\sigma,\tau} \geq v_\epsilon$.
Since $\tau$ is deterministic
then $\mathbb{E}_{\ini}^{\sigma,\tau}$
is the sum of all $\mathbb{E}(\pi) \cdot \pay(\pi) \cdot 
p_{r_E(\pi)}$ over plays $\pi$ played according to $\tau$
thus a trivial induction shows $\mathbb{E}_{\ini}^{\sigma,\tau}\geq v_\epsilon$.

We show now that to every strategy $\sigma$ of Eve,
and to every deterministic optimal answer $\tau$ of Adam, 
corresponds a feasible solution of the program
such that $v_\epsilon = \mathbb{E}_{\ini}^{\sigma,\tau}$.
Let  $(p_r)_{r \in R_E}$ the plan corresponding to $\sigma$.
For every $r\in R_A$ define $v_r$ be the expected payoff of Eve
in an auxiliary game where she plays $\sigma$
and Adam plays $\tau$ and the payoff of Eve is turned to $0$
whenever Adam signals sequence does not start with $r$.
We show that the linear constraint~\eqref{eq:implp2} holds for every $r\in R_A$
 and action $a$.
Since $\tau$ is deterministic then~\eqref{eq:implp2} is an equality
whenever $a=\tau(r)$.
And since $\tau$ is an optimal answer to $\sigma$,
it is locally optimal in the sense where playing an action
different from $\tau(r)$ after $r$ cannot be profitable to Adam,
hence~\eqref{eq:implp2} holds.
Finally, $(p_r)_{r \in R_E}$, $(v_r)_{r\in R_A}$ is a feasible solution.
\end{proof}

\paragraph{An example.}

The following linear program computes the value
of the simplified poker example.
$\text{Maximise $v_{\epsilon}$ subject to}$
\begin{align*}
\forall r \in R_E,~& 0\leq p_r \leq 1\\
&p_{\spadesuit,{\tt check} } +  p_{\spadesuit,{\tt raise} } = 1\\
&p_{\blacksquare,{\tt check} } +  p_{\blacksquare,{\tt raise} } = 1\\
%&p_{\dagger,{\tt raise} } =p_{\dagger,{\tt raise},{\tt fold} }=p_{\dagger,{\tt raise},{\tt call} }\\
&v_\epsilon \leq v_\circ \leq v_{\circ, {\tt check}} + v_{\circ, {\tt raise}}\\
&v_{\circ, {\tt check}}\leq  \frac{1}{4} \cdot p_{\spadesuit,{\tt check} } \cdot (+1) 
+ \frac{3}{4} \cdot p_{\blacksquare,{\tt check} } \cdot (-1)\\
&v_{\circ, {\tt raise}} \leq \frac{1}{4} \cdot p_{\spadesuit,{\tt raise} } \cdot (+1) + \frac{3}{4} \cdot p_{\blacksquare,{\tt raise} } \cdot (+1)\\
&v_{\circ, {\tt raise}} \leq \frac{1}{4} \cdot p_{\spadesuit,{\tt raise} } \cdot (+3) + \frac{3}{4} \cdot p_{\blacksquare,{\tt raise} } \cdot (-3)
\end{align*}
Setting $x=p_{\spadesuit,{\tt check} }$
and $y=  p_{\blacksquare,{\tt check} }$,
the solution is
\begin{align*}
&
\frac{1}{4}\max_{(x,y)\in[0,1]^2}
\left(
{x - 3y}
+
\min\left(
{(1-x) +  3 (1-y)},
{3(1-x) - 9(1-y)}
 \right)\right)\\
 =&
 \frac{1}{4}\max_{(x,y)\in[0,1]^2}
\min\left(
4 - 6y,
-6 -2x + 6y   
\right)
=
 \frac{1}{4}\max_{y\in[0,1]}
\min\left(
4 - 6y,
-6 + 6y   
\right)
 \enspace,
\end{align*}
which is maximal when $y=\frac{5}{6}$
and the solution is $-\frac{1}{4}$.

\paragraph{Nose scratch variant.}
Assume now that Eve does not have the perfect poker face:
whenever she has $\spadesuit$ she scratches
her nose with probability $\frac{1}{2}$ whereas
in general it happens only with probability $\frac{1}{6}$.
Only Adam is aware of this sign,
which he receives
as a private signal $s$ (scratch) or $n$ (no scratch).

Compared to the perfect poker face situation,
 the situation is slightly better for Adam:
 the value drops from $-\frac{1}{4}$
to $(-\frac{1}{4} -\frac{1}{10})$.
The optimal bluff frequency of Eve decreases
 from $\frac{1}{6}$ to $\frac{1}{10}$.
Computation details follow.

\begin{align*}
&\text{Maximise $v_{\epsilon}$ subject to}
\\
\forall u \in R_E,~& 0\leq p_u \leq 1\\
&p_{\spadesuit,{\tt c} } +  p_{\spadesuit,{\tt r} } = 1~~~~~p_{\blacksquare,{\tt c} } +  p_{\blacksquare,{\tt r} } = 1\\
%&p_{\dagger,{\tt raise} } =p_{\dagger,{\tt raise},{\tt fold} }=p_{\dagger,{\tt raise},{\tt call} }\\
&v_\epsilon \leq v_{\tt s} + v_{\tt n}~~~~~v_{\tt s} \leq v_{{\tt sc}} + v_{{\tt sr}}~~~~~
 v_{\tt n} \leq v_{{\tt nc}} + v_{{\tt nr}}\\
&v_{{\tt sc} } \leq 
 \frac{1}{4}\cdot\frac{1}{2} \cdot p_{\spadesuit,{\tt c} } \cdot (+1) 
+ \frac{3}{4}\cdot \frac{1}{6} \cdot p_{\blacksquare,{\tt c} } \cdot (-1)\\
&v_{{\tt nc} } \leq 
 \frac{1}{4}\cdot\frac{1}{2} \cdot p_{\spadesuit,{\tt c} } \cdot (+1) 
+ \frac{3}{4} \cdot\frac{5}{6} \cdot p_{\blacksquare,{\tt c} } \cdot (-1)\\
&v_{{\tt sr}} \leq \frac{1}{4}\cdot \frac{1}{2}\cdot p_{\spadesuit,{\tt r} } \cdot (+1) 
+ \frac{3}{4} \cdot\frac{1}{6}\cdot p_{\blacksquare,{\tt r} } \cdot (+1)\\
&v_{{\tt sr}} \leq \frac{1}{4}\cdot \frac{1}{2}\cdot p_{\spadesuit,{\tt r} } \cdot (+3) 
+ \frac{3}{4} \cdot\frac{1}{6}\cdot p_{\blacksquare,{\tt r} } \cdot (-3)\\
&v_{{\tt nr}} \leq \frac{1}{4}\cdot \frac{1}{2}\cdot p_{\spadesuit,{\tt r} } \cdot (+1) 
+ \frac{3}{4} \cdot\frac{5}{6}\cdot p_{\blacksquare,{\tt r} } \cdot (+1)\\
&v_{{\tt nr}} \leq \frac{1}{4}\cdot \frac{1}{2}\cdot p_{\spadesuit,{\tt r} } \cdot (+3) 
+ \frac{3}{4} \cdot\frac{5}{6}\cdot p_{\blacksquare,{\tt r} } \cdot (-3)
\end{align*}
Set $y=p_{\blacksquare,{\tt c} }$.
Some elementary simplifications lead to the equivalent program:
\begin{align*}
\max_{0\leq y \leq 1} \frac{1}{8} \left(\min\left( 8 -12y,-10 +8y
,  6  - 8y,-12  +12y \right) \right) 
\end{align*}
The optimum is reached when $8y-10=8-12y$
i.e. when $p_{\blacksquare,{\tt c} }=\frac{9}{10}$
and is equal to $-\frac{7}{20}=-\frac{1}{4}-\frac{1}{10}$ .


 
\section{Infinite duration.}


Games with infinite duration and imperfect information
are a natural model for applications such as synthesis 
of controllers of embedded systems.
This is illustrated by the example of the network controller.
Whereas in the previous section games of finite-duration
were equipped with real-valued payoffs, 
here we focus on B{\"u}chi conditions.


\subsection{Playing games with infinite duration and imperfect information }

Notations used for games of finite duration are kept.
On top of that we need to define how probabilities are measured
and the winning conditions.

\paragraph{Measuring probabilities.}
The choice of an initial distribution
$\ini\in\dist(V)$ 
and two strategies
$\sigma:  R_E \to \dist(A)$
and $\tau:  R_A \to \dist(A)$
for Eve and Adam
defines a Markov chain on the set of all finite plays.
This in turn defines a probability measure
$\mathbb{P}_{\ini}^{\sigma,\tau}$ on the Borel-measurable
subsets of $\Delta^\omega$.
The random variables $V_n,A_n,B_n,S_{n}$ and $T_{n}$ denote
respectively the $n$-th state, action of Eve, action of Adam, 
signal received by Eve and Adam,
and we denote $\pi_n$ the finite play 
$\pi_n = V_0,A_0,B_0,S_0,T_0,V_1,\ldots,S_{n},T_{n},V_{n+1}$.

%The following random variables indexed by $n\in \NN$ are useful.
%\begin{itemize}
%\item
%$V_n$ the $n$-th vertex visited by the play,
%\item
%$A_n,B_n$ the $n$-th actions chosen by Eve and Adam respectively,
%\item
%$S_n,T_n$ the $n$-signals received by Eve and Adam respectively,
%\item
%$C_n$ the $n$-th color seen during the play.
%\end{itemize}
The probability measure $\mathbb{P}_{\ini}^{\sigma,\tau}$ is the only
probability measure over $\Delta^\omega$ such that
for every $v\in V$, 
$\mathbb{P}^{\sigma,\tau}_{\ini}(V_0 = v) = \ini(v)$
and for every $n\in\NN$,
\begin{multline*}
%\label{eq:defproba}
\mathbb{P}^{\sigma,\tau}_{\ini}(V_{n+1}, S_{n}, T_{n} \mid \pi_n) \\
= \sigma(S_0\cdots S_{n-1})(A_{n}) \cdot \tau(T_0\cdots T_{n-1})(B_n) \cdot \Delta(V_n,A_n,B_n)(V_{n+1},S_{n},T_{n})\enspace,
\end{multline*}
where we use standard notations for conditional probability measures.

\newcommand{\win}{{\tt Win}}
\newcommand{\winreach}{{\tt Reach}}
\newcommand{\winsafe}{{\tt Safety}}
\newcommand{\winbuchi}{{\tt Buchi}}
\newcommand{\wincobuchi}{{\tt CoBuchi}}
\paragraph{Winning conditions.}

The set of colours is $C=\{0,1\}$.
The reachability, safety, B{\"u}chi and coB{\"u}chi condition
 condition are defined as follows:
 \begin{align*}
 &\winreach=\{\exists n\in\NN, C_n  = 1\}\\
&\winsafe=\{\forall n\in\NN, C_n = 0\}\\
&\winbuchi=\{\forall m \in \NN, \exists n \geq m, C_n=1\}\\
&\wincobuchi = \{\exists m \in \NN, \forall n \geq m, C_n = 0\}\enspace.
\end{align*}

When the winning condition is $\win$,
Eve and Adam use strategies
$\sigma$ and $\tau$ and the initial distribution is $\ini$,
then Eve wins the game with probability:
\[
\mathbb{P}^{\sigma,\tau}_{\ini}(\win)\enspace.
\]
Eve wants to maximise this probability, while Adam wants
to minimise it.  
%An enjoyable situation for Eve is when she has an
%almost-surely winning strategy.



\subsection{The value problem.}

The value problem is computationally intractable
for games with infinite duration and imperfect information.
This holds even for the very simple case
of blind one-player games with reachability conditions.
Those are games where the set of
signals is a singleton and actions of Adam have no influence
on the transition probabilities. These games can be seen
as probabilistic automata, hence the undecidability result of Paz applies.

\begin{theorem}\cite{Paz}
Whether Eve has a strategy to win with probability $\geq \frac{1}{2}$
is undecidable, even in blind one-player games.
\end{theorem}

Actually, the value might not even exist.
\begin{proposition}~\cite{repgames}
There is a game with infinite duration imperfect information and B{\"u}chi condition
in which 
\[
\sup_\sigma \inf_\tau \mathbb{P}^{\sigma,\tau}_{\ini}(\winbuchi)
=
\frac{1}{2}
<
1
=
\inf_\tau \sup_\sigma  \mathbb{P}^{\sigma,\tau}_{\ini}(\winbuchi)\enspace.
\]
\end{proposition}

The value however exists for games with reachability condition~\cite{repgames}.

Although the value problem is not decidable,
there are
some other interesting  decision problems to consider.

\subsection{Winning with probability $1$ or $>0$}


\paragraph{Winning almost-surely or positively.}
  A strategy $\sigma$ for Eve is \emph{almost-surely winning}
  from an initial distribution $\ini$ if
\begin{equation*}\label{eq:as}
  \forall \tau,
  \mathbb{P}^{\sigma,\tau}_{\ini}(\win)=1\enspace.
\end{equation*}
When such an almost-surely strategy $\sigma$ exists, the initial distribution $\ini$
is said to be almost-surely winning (for Eve).

A less enjoyable situation for Eve is when she only has a
positively winning strategy.
  A strategy $\sigma$ for Eve is \emph{positively winning} from
  an initial distribution $\ini$ if
\begin{equation*}%\label{eq:pos}
  \forall \tau,
  \mathbb{P}^{\sigma,\tau}_{\ini}(\win)>0\enspace.
\end{equation*}
When such a strategy $\sigma$ exists, the initial distribution $\delta$
is said to be positively winning (for Eve).
Symmetrically, a
strategy $\tau$ for Adam is positively winning if it guarantees
$\forall \sigma, \mathbb{P}^{\sigma,\tau}_{\ini}(\win)<1$.

The worst situation for Eve is when her opponent has an
almost-surely winning strategy $\tau$, which thus ensures $\mathbb{P}^{\sigma,\tau}_{\ini}(\win)=0$
whatever strategy $\sigma$ is chosen by Eve. 


\paragraph{Qualitative determinacy.}

\begin{theorem}
\label{theo:qdet}
Stochastic games with signals and reachability, safety and B{\"u}chi
winning conditions are qualitatively determined:
either Eve wins almost-surely winning
or Adam wins positively.
Formally, in those games,
\[
\left(\forall \tau, \exists \sigma,\mathbb{P}^{\sigma,\tau}_{\ini}(\win)=1\right)
\implies
\left(\exists \sigma,\forall \tau ,\mathbb{P}^{\sigma,\tau}_{\ini}(\win)=1\right)\enspace.
\]
\end{theorem}

The proof of this result is given in the next section.

Since reachability and safety games are dual, a consequence of
Theorem~\ref{theo:qdet}, is that in a reachability game, every initial
distribution is either almost-surely winning for Eve,
almost-surely winning for Adam, or positively
winning for both players.
When a safety condition is satisfied almost-surely for a fixed profile of strategies,
it trivially implies that the safety condition is
satisfied by all consistent plays,
thus for safety games winning \emph{surely} is the same than winning almost-surely.



By contrast, co-B{\"u}chi games are \emph{not} qualitatively determined:
\begin{lemma}
There is a co-B{\"u}chi game in which neither Eve has an almost-surely winning strategy
nor Adam has a positively winning strategy.
\end{lemma}
\begin{proof}
In this game, Eve observes
everything, Adam is blind (he only observes his own actions),
and Eve's objective is to visit only finitely many times the ${\large \frownie}$-state. The initial state is $\large{\frownie}$. The set of actions is $\{a,b,c,d\}$.
All transitions are deterministic.%$t$.

\begin{figure}[h]
\begin{center}
%\includegraphics[scale=.5]{cobuc_nondet.png}
\end{center}
\caption{Co-B{\"u}chi games are not qualitatively determined.}
\label{chap9fig4}
\end{figure} 


On one hand, no strategy $\Sigma$
is almost-surely winning for Eve
for her co-B{\"u}chi objective.
{
Since both players can observe their actions,
it is enough to prove that no behavioral
strategy
$\sigma\in C^*\to \Delta(I)$ of Eve is almost-surely winning.
Fix strategy $\sigma$ and assume towards contradiction that $\sigma$ is almost-surely winning. 
We define a strategy $\tau$
such that
$\probimp{\sigma,\tau}{\frownie}{ \winbuchi} > 0$.
Strategy $\tau$ starts by playing only $c$.
The probability to be in state $\frownie$ at step $n$ is
$x^{0}_n = \probimp{\sigma,c^\omega}{\frownie}{V_n=\frownie}$ and since $\sigma$ is almost-surely winning then $x^{0}_n \to_n 0$ thus there exists  $n_0$ such that 
$x^{0}_{n_0}\leq \frac{1}{2}$.
Then $\tau$ plays $d$ at step $n_0$.
Assuming the state was $2$ when $d$ was played, 
the probability to be in state $\frownie$ at step $n\geq n_0$ is
$x^{1}_n = \probimp{\sigma,c^{n_0}dc^\omega}{\frownie}{V_{n}=\frownie\mid V_{n_0}=\frownie}$
and since $\sigma$ is almost-surely winning there exists $n_1$ such that
$x^{1}_{n_1}\leq  \frac{1}{4}$.
Then $\tau$ plays $d$ at step $n_1$.
By induction we keep defining $\tau$ this way so that
$\tau=c^{n_0-1}d c^{n_1 - n_0 - 1}dc^{n_2 - n_1 - 1}d \cdots $.
and for every $k\in \NN$,
$\probimp{\sigma,\tau}{\frownie}{
V_{n_{k+1}}=\frownie
\text{ and }
V_{n_{k+1}-1}=2
\mid 
V_{n_{k}}=\frownie
} \geq 1 - \frac{1}{2^{k+1}}$.
Thus finally
$\probimp{\sigma,\tau}{\frownie}{\winbuchi} \geq
\Pi_{k} (1 - \frac{1}{2^{k+1}})>0$ which contradicts the hypothesis.

}

On the other hand, Adam does not have a positively winning
strategy either.
{
Intuitively, Adam cannot win positively because as time passes, either the play reaches state $1$ or the chances that Adam plays action $d$ drop to $0$. When these chances are small, 
Eve can play action $c$ and she bets no more $d$ will be played and the play will stay safe in state $2$. If Eve loses her bet
then again she waits until the chances to see another $d$ are small and then plays action $c$. Eve may lose a couple of bets but almost-surely she eventually is right and the $\wincobuchi$ condition is almost-surely fulfilled.
%Formally, according to Theorem~\ref{theo:actionobservable},
%since both players can observe their actions,
%it is enough to prove that no behavioral
%strategy
%$\tau\in D^*\to \Delta(J)$ of Adam is positively winning.
%The strategy $\tau$ being fixed,
%we define a strategy $\sigma$ for 
%Eve such that 
%$\probimp{\sigma,\tau}{\frownie}{\winbuchi}=1$.
%The only state where Eve's action matters
%is  $\frownie$.
%After a play $p=V_0i_0j_0\cdots V_n$ ending up in state $\frownie$
%(Eve can observe the state), the strategy $\sigma$ plays action $a$ except if the trigger condition
%\[
%\probimp{i_0\cdots i_na^\omega,\tau}{\frownie}{\forall m\geq n, J_m\neq d \mid P_{ n} = p}
%\geq \frac{1}{2}\enspace,
%\]
%is satisfied in this case action $b$ is played.
%Let $E_0$ the event that finitely many $d$ are played i.e. $E_0=\{\exists n, \forall m\geq n, J_m\neq d\}$.
%According to L\'evy law, 
%$\probimp{\sigma,\tau}{\frownie}{ E_0\mid P_{ n}}$
%converges $\mathbb{P}^{\sigma,\tau}_{\frownie}$-almost-surely to the indicator function $1_{E_0}$ of the event $E_0$. 
%If $E_0$ holds then finitely many $d$ are played, and the play cannot stay forever in state $\frownie$ after the last $d$ because $\probimp{\sigma,\tau}{\frownie}{ E_0\mid P_{ n}}$ converges to $1$ thus the trigger condition is eventually satisfied. Thus when $E_0$ holds the play eventually stays in state $1$ or $2$ and the $\wincobuchi$ condition is satisfied.
%If $E_0$ does not hold then $\probimp{\sigma,\tau}{\frownie}{ E_0\mid P_{ n}}$ converges to $0$ thus eventually the trigger condition is not satisfied anymore hence Eve eventually plays no more $b$'s, only  $a$'s. But $E_0$ 
%does not hold thus infinitely many $d$ are played,
%thus the play reaches state $1$.
%In both cases $\wincobuchi$ holds $\mathbb{P}^{\sigma,\tau}_{\frownie}$-almost-surely, thus $\tau$ is not positively winning.

Finally neither Eve wins almost-surely nor Adam wins positively.
}
\end{proof}


\paragraph{Decidability}
\newcommand{\EXPTIME}{{\sc EXPTIME}}
\newcommand{\PTIME}{{\sc PTIME}}

\begin{theorem}
\label{th:main}
Deciding whether the initial distribution of a B{\"u}chi games,
is almost-surely winning for Eve is
2\EXPTIME-complete.
For safety games, the same problem is \EXPTIME-complete.
\end{theorem}

Concerning winning positively a {\em safety or co-B{\"u}chi game}, one
can use Theorem~\ref{theo:qdet} and the determinacy property: Adam
has a positively winning strategy in the above game if and only if
Eve has no almost-surely winning strategy. Therefore, deciding
when Adam has a positively winning strategy can also be done, with
the same complexity.


\begin{theorem}
\label{th:main2}
For reachability and B{\"u}chi games where either Eve is perfectly informed about the state
or Adam is
better informed than Eve, deciding whether the initial distribution is
almost-surely winning for Eve is
\EXPTIME-complete.
In safety games
Eve is perfectly
informed {about the state}, the decision problem is in \PTIME.
\end{theorem}

%Similar examples can be used to prove that
%stochastic B{\"u}chi games with signals do not have a
%value~\cite{repgames}.  In this game, Eve observes
%everything, Adam is blind (he only observes his own actions),
%and Eve's objective is to visit only finitely many times the ${\large \frownie}$-state. The initial state is $\large{\frownie}$.%$t$.
%
%\begin{figure}[h]
%\begin{center}
%\setlength{\unitlength}{2mm}
%\begin{gpicture}(30,13)(0,5)
%
%\gasset{Nframe=y,Nfill=n,Nw=3,Nh=3,ExtNL=n,Nmr=2}
%
%\node(L)(0,10){$1$}
%%\node[Nmarks=r](1)(15,10){$t$}
%\node[Nframe=n,Nw=3,Nh=3](1)(15,10){$\Huge{\frownie}$}
%\node(2)(30,10){$2$}
%\drawedge[curvedepth=2](2,1){$* d$}
%\drawedge[curvedepth=2](1,2){$b c$}
%\drawedge(1,L){$* d$}
%\drawloop[loopdiam=3](1){$a c$}
%\drawloop[loopdiam=3](2){$* c$}
%\drawloop[loopdiam=3](L){$* *$}
%\end{gpicture}
%\end{center}
%\caption{Co-B{\"u}chi games are not qualitatively determined.}
%\label{fig4}
%\end{figure} 
%
%
%On one hand, no strategy $\Sigma$
%is almost-surely winning for Eve
%for the co-B{\"u}chi objective.
%{
%According to Theorem~\ref{theo:actionobservable},
%since both players can observe their actions,
%it is enough to prove that no behavioral
%strategy
%$\sigma\in C^*\to \Delta(I)$ of Eve is almost-surely winning.
%Fix strategy $\sigma$ and assume towards contradiction that $\sigma$ is almost-surely winning. 
%We define a strategy $\tau$
%such that
%$\probimp{\sigma,\tau}{\frownie}{ \winbuchi} > 0$.
%Strategy $\tau$ starts by playing only $c$.
%The probability to be in state $\frownie$ at step $n$ is
%$x^{0}_n = \probimp{\sigma,c^\omega}{\frownie}{V_n=\frownie}$ and since $\sigma$ is almost-surely winning then $x^{0}_n \to_n 0$ thus there exists  $n_0$ such that 
%$x^{0}_{n_0}\leq \frac{1}{2}$.
%Then $\tau$ plays $d$ at step $n_0$.
%Assuming the state was $2$ when $d$ was played, 
%the probability to be in state $\frownie$ at step $n\geq n_0$ is
%$x^{1}_n = \probimp{\sigma,c^{n_0}dc^\omega}{\frownie}{V_{n}=\frownie\mid V_{n_0}=\frownie}$
%and since $\sigma$ is almost-surely winning there exists $n_1$ such that
%$x^{1}_{n_1}\leq  \frac{1}{4}$.
%Then $\tau$ plays $d$ at step $n_1$.
%By induction we keep defining $\tau$ this way so that
%$\tau=c^{n_0-1}d c^{n_1 - n_0 - 1}dc^{n_2 - n_1 - 1}d \cdots $.
%and for every $k\in \NN$,
%$\probimp{\sigma,\tau}{\frownie}{
%V_{n_{k+1}}=\frownie
%\text{ and }
%V_{n_{k+1}-1}=2
%\mid 
%V_{n_{k}}=\frownie
%} \geq 1 - \frac{1}{2^{k+1}}$.
%Thus finally
%$\probimp{\sigma,\tau}{\frownie}{\winbuchi} \geq
%\Pi_{k} (1 - \frac{1}{2^{k+1}})>0$ which contradicts the hypothesis.
%
%}
%
%On the other hand, Adam does not have a positively winning
%strategy either.
%{
%Intuitively, Adam cannot win positively because as time passes, either the play reaches state $1$ or the chances that Adam plays action $d$ drop to $0$. When these chances are small, 
%Eve can play action $c$ and she bets no more $d$ will be played and the play will stay safe in state $2$. If Eve loses her bet
%then again she waits until the chances to see another $d$ are small and then plays action $c$. Eve may lose a couple of bets but almost-surely she eventually is right and the $\wincobuchi$ condition is fulfilled.
%Formally, according to Theorem~\ref{theo:actionobservable},
%since both players can observe their actions,
%it is enough to prove that no behavioral
%strategy
%$\tau\in D^*\to \Delta(J)$ of Adam is positively winning.
%The strategy $\tau$ being fixed,
%we define a strategy $\sigma$ for 
%Eve such that 
%$\probimp{\sigma,\tau}{\frownie}{\winbuchi}=1$.
%The only state where Eve's action matters
%is  $\frownie$.
%After a play $p=V_0i_0j_0\cdots V_n$ ending up in state $\frownie$
%(Eve can observe the state), the strategy $\sigma$ plays action $a$ except if the trigger condition
%\[
%\probimp{i_0\cdots i_na^\omega,\tau}{\frownie}{\forall m\geq n, J_m\neq d \mid P_{ n} = p}
%\geq \frac{1}{2}\enspace,
%\]
%is satisfied in this case action $b$ is played.
%Let $E_0$ the event that finitely many $d$ are played i.e. $E_0=\{\exists n, \forall m\geq n, J_m\neq d\}$.
%According to L\'evy law, 
%$\probimp{\sigma,\tau}{\frownie}{ E_0\mid P_{ n}}$
%converges $\mathbb{P}^{\sigma,\tau}_{\frownie}$-almost-surely to the indicator function $1_{E_0}$ of the event $E_0$. 
%If $E_0$ holds then finitely many $d$ are played, and the play cannot stay forever in state $\frownie$ after the last $d$ because $\probimp{\sigma,\tau}{\frownie}{ E_0\mid P_{ n}}$ converges to $1$ thus the trigger condition is eventually satisfied. Thus when $E_0$ holds the play eventually stays in state $1$ or $2$ and the $\wincobuchi$ condition is satisfied.
%If $E_0$ does not hold then $\probimp{\sigma,\tau}{\frownie}{ E_0\mid P_{ n}}$ converges to $0$ thus eventually the trigger condition is not satisfied anymore hence Eve eventually plays no more $b$'s, only  $a$'s. But $E_0$ 
%does not hold thus infinitely many $d$ are played,
%thus the play reaches state $1$.
%In both cases $\wincobuchi$ holds $\mathbb{P}^{\sigma,\tau}_{\frownie}$-almost-surely, thus $\tau$ is not positively winning.
%
%Finally neither Eve wins almost-surely nor Adam wins positively.

\subsection{Qualitative determinacy: proof of Theorem~\ref{theo:qdet}}

\paragraph{Beliefs.}
The
\emph{belief} of a player
is the set of possible states of the game, according
to the signals received by the player. 

\newcommand{\states}{V}
\newcommand{\ar}{\arena}
\newcommand{\action}{a}
\newcommand{\belun}{\mathcal{B}_{\text{Eve}}}
\newcommand{\beldeux}{\mathcal{B}_{\text{Adam}}}
\newcommand{\deuxbelun}{\mathcal{B}^{(2)}_{Eve}}
\newcommand{\tp}{\Delta}
\newcommand{\parties}[1]{\ensuremath{\mathcal{P}(#1)}}

\begin{definition}[Belief]
{Let $\arena$ be an arena with observable actions.}
  From an initial set of states $L\subseteq\states$, the belief of
  Eve after having received signal $s$ is:
  \begin{multline*}
\belun(L,s) =
 \{ v\in\states \mid \exists l\in L, t\in S \text{ such that } \tp(l,s,t)(v,\Act(s),\Act(t))>0\}\enspace.  
  \end{multline*}

Remark that in this definition we use the fact that actions of Eve are observable,
thus when he receives a signal $s\in C$ Eve can deduce he played action
$\action_1(c)\in I$.
The belief of Eve after having received a sequence of signals $s_1,\ldots,s_n$ is defined inductively by:
\[
\belun(L,s_1,s_2,\ldots,s_n)
 = \belun(\belun(L,s_1,\ldots,s_{n-1}),s_n).\enspace
\]
Beliefs of Adam are defined similarly.
{
Given an initial distribution $\delta$,
we denote
$\belun^n$ the random variable defined by}
\begin{align*}
&{\belun^{0} = \supp(\delta)}\\
&{\belun^{n+1} =\belun(\supp(\delta),C_1,\ldots,C_{n+1})
= \belun(\belun^n,C_{n+1})
\enspace.}
\end{align*}
\end{definition}



We will also rely on the notion of \emph{belief of belief}, called
here \emph{2-belief}, which, roughly speaking, represents for one
player the set of possible beliefs for his (or her) adversary,
as well as the possible current state.
\begin{definition}[2-Belief]
{Let $\ar$ be an arena with observable actions.}
  From an initial set ${\mathcal{L}} \subseteq \states
  \times \parties{\states}$ of pairs composed of a state and a belief
  for Adam, the 2-belief of Eve after having received signal $c$ is the subset of 
$\states
  \times \parties{\states}$ defined by:
%\begin{multline*}
\[
  \deuxbelun({\mathcal{L}},s) = \{ (v,\beldeux(L,t)) \mid
\exists  (\ell,L) \in {\mathcal{L}},  t\in S,
  \tp(v,s,t)(\ell,\Act(s),\Act(t))
  >0\} \enspace.
  \]
%\end{multline*}

From an initial set ${\mathcal{L}} \subseteq \states
\times \parties{\states}$ of pairs composed of a state and a belief
for Adam, the 2-belief of Eve after having  received a sequence of
signals $s_1,\ldots,s_n$ is defined inductively by:
\[
\deuxbelun({\mathcal{L}},s_1,s_2,\ldots,s_n) =
\deuxbelun\left(\deuxbelun\left({\mathcal{L}},s_1,\ldots,s_{n-1}\right),s_n\right)\enspace.
\]
\end{definition}

There are natural definitions of $3$-beliefs (beliefs on beliefs on beliefs)
and even $k$-beliefs however for our purpose, $2$-beliefs are enough,
in the following sense:
in B{\"u}chi games the positively winning sets of Adam
can be characterised by fix-point equations on sets of $2$-beliefs,
and some positively winning strategies of Adam with finite-memory
can be implemented using $2$-beliefs.


\paragraph{Supports positively winning supports.}

Note that whether an initial distribution $\ini$ is almost-surely or
positively winning depends only on its support, because
$\mathbb{P}^{\sigma,\tau}_{\ini}(\win)
=\sum_{v\in V}\ini(v)\cdot\mathbb{P}^{\sigma,\tau}_{\ini}(\win \mid V_0= v)$.
As a consequence, we will say that a support
$L\subseteq V$ is almost-surely or positively winning for a
player if there exists a distribution with support $L$ which has the
same property.

In the sequel, we will denote $\LLE$ the set of supports almost-surely winning for Eve
and  $\LLA$ those positively winning for Adam.

Then the qualitative determinacy theorem is a corollary of the following lemma.
\begin{lemma}
In every B{\"u}chi game, every non-empty support
which does not belong to $\LLA$ belongs to $\LLE$.
\end{lemma}

The proof of this lemma relies on the definition of a strategy
called the maximal strategy.
We prove that this strategy is almost-surely winning from any initial
distribution which is not positively winning for Adam. 

\begin{definition}[Maximal strategy]\label{def:maximalstrategy}
For every non-empty support $L\subseteq V$ we define
 the set  of {$L$-safe} actions for Eve as
\[
\Isafe(L) = \left\{ a \in A \mid  \forall s \in S, (\Act(s)=a) \implies (\belun(L,s)\not\in\LLA
  )\right\}\enspace,
\]
in other words these are the actions which Eve can play without taking the risk
that her belief is positively winning for Adam.

The \emph{maximal strategy} is the strategy of Eve
which plays the uniform distribution
on $\Isafe(\belun)$
when it is not empty and plays the uniform distribution on $A$ otherwise.
It is denoted $\sigma_{\can}$.

\end{definition}

To play her maximal strategy at step $n$,
Eve only needs to keep track of her belief $\belun^n$,
thus $\sigma_{\can}$ can be implemented by Eve 
using a finite-memory device which keeps track of the current belief.
Such a strategy is said to be \emph{belief-based}.
We will use several times  the following technical lemma about belief-based strategies.

\begin{lemma}\label{lem:borelcantelli}
{Fix a B{\"u}chi game.}
Let $\LL \subseteq \parties{V}$ 
and $\sigma$ a strategy for player $1$.
Assume that
$\sigma$ is a belief
strategy,
$\LL$ is downward-closed
(i.e. $L\in\LL \land L' \subseteq L \implies L'\in \LL$)
and for every $L\in\LL\setminus \{\emptyset\}$ and every strategy $\tau$,
\begin{align}
\label{eq:pos}
& \probimp{\sigma,\tau}{\delta_L}{\Reach} > 0\enspace,\\
 \label{eq:belstab}
&\probimp{\sigma,\tau}{\delta_L}{\forall n\in\NN,\belun^n\in \LL} = 1\enspace.
\end{align}
Then $\sigma$ is almost-surely winning for the B{\"u}chi game from any support 
$L\in \LL\setminus \{\emptyset\}$. 
\end{lemma}
\begin{proof}
{
Since $\LL$ is downward-closed then $\forall L\in \LL,\forall l\in L, \{l\}\in\LL$
thus~\eqref{eq:pos} implies 
\be
\forall L\in \LL,\forall l\in L,
 \probimp{\sigma,\tau}{\delta_L}{\Reach\mid V_0=l} > 0\enspace.
 \label{eq:pos2}
\ee 
}
{
Once $\sigma$ is fixed then the game is a one-player game with state space $V\times 2^V$ and imperfect information and~\eqref{eq:pos2} implies that in this one-player game,
\be
\label{eq:sigmamproperty}
\forall L\in \LL,\forall l\in L, \forall \tau,
 \probimp{\tau}{\delta_L}{\Reach \mid V_0=l} > \varepsilon\enspace,
 \ee
where $N=|K|\cdot 2^{|K|}$
and $\varepsilon = p_{\min}^{|K|\cdot 2^{|K|}}$
and $p_{\min}$ is the minimal non-zero transition probability.
Moreover~\eqref{eq:belstab} implies that
in this one-player game the second component of the state space is always in $\LL$, whatever strategy $\tau$ is played by player $2$.
Remind the definition
 \begin{align*}
 \winreach=\{\exists n\in\NN, C_n  = 1\}\enspace.
\end{align*}
As a consequence, in this one-player game
for every $m\in\NN$,
and every behavioral strategy $\tau$ and every %sequence of signals $c_1\cdots c_m\in C^*$
%and $d_1\cdots d_m\in D^*$ and 
$l\in V$,
\be
\label{eq:sigmamproperty2}
\probimp{\tau}{\delta_L}
{ \exists m \leq n \leq m+ N, C_n = 1 \mid
K_m = l
}
\geq \varepsilon,
\ee
whenever $\probimp{\tau}{\delta_L}{V_m=l} > 0$.
}
We use the Borel-Cantelli Lemma to conclude the proof.
According to~\eqref{eq:sigmamproperty2},
for every $\tau$, $L\in\overline{\LL}$, 
$m\in \NN$,
\be
\probimp{\tau}{\delta_L}
{ \exists n, mN \leq n < (m+ 1)N, C_n=1
\mid V_{mN}
}
\geq \varepsilon,
\ee
which implies for every behavioral strategy $\tau$ and $k,m\in\NN$,
\[
\probimp{\tau}{\delta_L}
{\forall n,  \left((m\cdot N) \leq n < ((m+k) \cdot N) \implies  C_n \neq 1 \right)}\leq  \left(1 - \varepsilon\right)^k\enspace.
\]
Since $\sum_k \left(1 - \varepsilon\right)^k$ is finite,
we can apply Borel-Cantelli Lemma for the events 
$(\{\forall n, m\cdot N \leq n < (m+k) \cdot N \implies  C_n\neq 1\})_k$
and we get
$\nonumber
\probimp{\tau}{\delta_L}{\forall n, m\cdot N \leq n  \implies  C_n\neq 1}=0
$
thus
\be\label{eq:assss}\nonumber
\probimp{\tau}{\delta_L}{\winbuchi}=1\enspace.
\ee

As a consequence $\sigma$ is almost-surely winning for the 
B{\"u}chi game.
\end{proof}


An important feature of the maximal strategy is the following.
\begin{lemma}
In a B{\"u}chi game
with observable actions,
let $\delta\in\Delta(K)$ be an initial distribution which is not positively 
winning for Adam,
i.e. $\supp(\delta)\not\in\LLA$.
Then for every strategy $\tau$ of Adam
\begin{equation}
\label{eq:LLstable}
\mathbb{P}^{\sigma_\can,\tau}_{\delta}(\forall n \in \NN, \belun^n \not\in\LLA )=1\enspace.
\end{equation}
\end{lemma}
\begin{proof}
We only provide a sketch of proof.
The proof is an induction based on the fact that for every non-empty subset $L\subseteq V$,
\[
(L\not \in \LLA) \implies (\Isafe(L)\neq \emptyset)\enspace.
\]
Assume a contrario that $\Isafe(L) = \emptyset$ for some $L\not \in \LLA$.
Then for every action $a\in A$ there exists a signal $s_a\in S$
such that $\belun(L,s_a)\neq \emptyset$ and $\belun(L,s_a) \in \LLA$.
Since $\belun(L,s_a)\neq \emptyset$, the definition of the belief operator implies:
\[
\exists v_a\in L, w_a\in V,  t_a \in T, \text{ such that }  \tp(w_a,s_a,t_a)(v_a,\Act(s),\Act(t_a)) > 0\enspace.
\]

But then Adam can win positively from $L$ with the following strategy.
At the first round, Adam plays randomly any action in $A$.
At the next round, Adam picks up randomly a belief in  $\LLA$ and 
plays forever the corresponding positively winning strategy.
Remark that this strategy of Adam is not described as a behavioural strategy
but rather as a finite-memory strategy. Since actions are observable,
such a finite-memory strategy can be turned into a behavioural one,
see~\cite[Lemma 4.6 and 4.7]{BGGjacm}.

Why is Adam strategy positively winning from $L$?
Whatever action $a\in A$ is played by Eve,
with positive probability she will receive signal $s_a$,
because Adam might play the action $\Act(t_a)$.
Since $\belun(L,s_a) \in \LLA$ then there Adam might with positive probability
play a strategy positively winning when the initial belief
of Eve is $\belun(L,s_a)$. Thus whatever action Eve chooses,
she might lose with positive probability.

\end{proof}

\medskip 

The notion of maximal strategy being defined,
we can complete the proof of Theorem~\ref{theo:qdet}.
For that, we show that
$\sigma_{\can}$
is almost-surely
winning from every support not in $\LLA$.



Reachability and safety conditions can be easily encoded as B{\"u}chi conditions,
thus it is enough to consider  B{\"u}chi games.



The first step is to prove that for every $L\in\LLE$,
for every strategy $\tau$ of Adam,
\be\label{eq:LLpasM}
%\forall k_0\in L, 
\probimp{\sigma_{\can},\tau}{\delta_L}{\winsafe} < 1 \enspace.
\ee
We prove~\eqref{eq:LLpasM} by contradiction.
Assume~\eqref{eq:LLpasM} does not hold for some $L\in \LLE$
%, state $k_0\in L$
and strategy $\tau$:
\be\label{eq:winsafe}
\probimp{\sigma_{\can},\tau}{\delta_L}{\winsafe } = 1\enspace.
\ee
Under this assumption we use $\tau$ to build a strategy positively winning from $L$,
which will contradict the hypothesis 
$L\in\LLA$.
Although $\tau$ is surely winning from $L$ against the particular strategy $\sigma_{\can}$,
there is no reason for $\tau$ to be positively winning from $L$
against all other strategies of player $1$.
{
However we can rely on $\tau$
in order to define another strategy 
$\tau'$ for Adam positively winning from $L$.
The strategy $\tau'$ is a strategy
which gives positive probability to play $\tau$
all along the play,
as well as any strategy in the family
of strategies
$(\tau_{n,B})_{n\in\NN,B\in \LLA}$ defined as follows.
For every $B\in\LLA$ we choose a strategy $\tau_B$ positively winning from $B$.
Then
$\tau_{n,B}$ is the strategy which plays 
the uniform distribution on $A$ for the first $n$ steps then forgets past signals and switches definitively to $\tau_B$. 

A possible way to implement the  strategy $\tau'$
is as follows.
At the beginning of the play
player $2$ tosses a fair coin. If the result is head then he plays $\tau$. Otherwise he keeps 
tossing coins and as long as the coin toss is head, player $2$ plays randomly an action in $J$ .
The day the coin toss is tail, he picks up randomly some $B\in\LLA$ and starts playing $\tau_B$.
}
Remark that this strategy of Adam is not described as a behavioural strategy
but, since actions are observable,
such a finite-memory strategy can be turned into a behavioural one,
see~\cite[Lemma 4.6 and 4.7]{BGGjacm}.

%The strategy $\tau'$ is 
%the strategy
%that tosses a fair coin at the
%beginning of the play and:
%\begin{itemize}
%  \item plays like $\tau$ forever if the result of this initial coin toss is head,
%\item[\textbullet] otherwise at each step $\tau'$ tosses a fair coin and as long as the result is tail
%$\tau'$ plays the uniform distribution over $J$. If the result of one of the coin tosses is head,
%$\tau'$ switches to a new behaviour:  $\tau'$ selects randomly a support $B\in \LL$
%and a strategy $\tau_B$ positively winning from $B$, 
%\end{itemize}


Now that $\tau'$ is defined, we prove it is positively winning from $L$.
Let $E$ be the event 
{"player $1$ plays only actions that are safe with respect to her belief", i.e.}
\[
E = \{ \forall n\in \NN, A_n \in \Isafe_\LL(\belun^n)\}\enspace.
\]
Then for every behavioral strategy $\sigma$:
\begin{itemize}
  \item[\textbullet]
Either $\probimp{\sigma,\tau'}{\delta_L}{E}=1$. In this case 
\[
\probimp{\sigma,\tau'}{\delta_L}{\winsafe}> 0\enspace,
\]
because for every {finite play $\play=v_0a_0b_0s_1t_1v_1\cdots v_n$,}
\[
\left(\probimp{\sigma,\tau'}{\delta_L}{\play} > 0\right)
\implies
\left(\probimp{\sigma_{\can},\tau'}{\delta_L}{\play} > 0\right)
\implies
\winsafe\enspace,
\]
where the first implication holds because, by definition of $\sigma_{\can}$ and $E$,
for every $s_1\cdots s_n\in CS^*, \supp(\sigma(s_1\cdots s_n))\subseteq \supp(\sigma_{\can}(s_1\cdots s_n))$
%all actions played by $\sigma$ with positive probability can be played by $\sigma_{\can}$ as well,
while the second implication is from~\eqref{eq:winsafe}.
Thus $\probimp{\sigma,\tau}{\delta_L}{\winsafe}= 1$ and we get
$\probimp{\sigma,\tau'}{\delta_L}{\winsafe} > 0$ by definition of
$\tau'$.
\item[\textbullet]
Or $\probimp{\sigma,\tau'}{\delta_L}{E}<1$.
Then by definition of $E$ there exists $n\in\NN$
% and a support $I'\subseteq I$ 
such that 
\[
\probimp{\sigma,\tau'}{\delta_L}{A_n  \not\in
\Isafe_\LL(\belun^n)}>0\enspace.
\]
{
By definition of $\Isafe_\LL$ it implies
$\probimp{\sigma,\tau'}{\delta_L}{\belun^{n+1}  \in \LL}>0$,
thus there exists $B\in \LL$ such that
$\probimp{\sigma,\tau'}{\delta_L}{\belun^{n+1} =B}>0$.
By definition of $\tau'$ we get
$\probimp{\sigma,\tau_{n+1,B}}{\delta_L}{\belun^{n+1} =B}>0$,
because whatever finite play $v_0,\ldots, v_{n+1}$ leads with positive probability to
the event $\{\belun^{n+1} =B\}$,
the same finite play can occur with 
$\tau_{n+1,B}$ since $\tau_{n+1,B}$ plays every possible action for the $n+1$ first steps.
Since $\tau_{n+1,B}$ coincides with $\tau_\rand$ for the first $n+1$ steps then
by definition of beliefs,
$\probimp{\sigma,\tau_{n+1,B}}{\delta_L}{\belun^{n+1} =B}>0$
and $B\subseteq \{ k\in K\mid \probimp{\sigma,\tau_{n+1,B}}{\delta_L}{K_{n+1}=k\mid \belun^{n+1} =B}>0\}$.
%By definition of $\tau_B$, for every distribution $\delta'$ whose support contains $B$,
%$\probimp{\sigma,\tau_{n+1,B}}{\delta_L}{\belun^{n+1} =B\mid K_0 = k_0}>0$
%
Using the definition of $\tau_B$ we get 
$\probimp{\sigma,\tau_{n+1,B}}{\delta_L}{\wincobuchi}>0$.
}
As a consequence by definition of $\tau'$
we get  $ \probimp{\sigma,\tau'}{\delta_L}{\wincobuchi}
>0 $.
\end{itemize}  
In both cases, for every $\sigma$,
$\probimp{\sigma,\tau'}{\delta_L}{\wincobuchi } >0 $
thus $\tau'$ is positively winning from $L$.
This contradicts the
hypothesis $L\in \LLE$. As a consequence we get~\eqref{eq:LLpasM} by contradiction.

\medskip


Using~\eqref{eq:LLpasM}, 
we apply Lemma~\ref{lem:borelcantelli} to the collection 
$\overline{\LLA}$ and the strategy $\sigma_{\can}$.
The collection $\overline{\LLA}$ is downward-closed because $\LLA$ is upward-closed: if a support is positively winning for Adam then any greater support is positively winning as well, using the same positively winning strategy.

Thus $\sigma_{\can}$ is almost-surely winning for the B{\"u}chi game from every support in $\overline{\LLA}$ i.e. every support which is not positively winning for Adam, hence the game is qualitatively determined.

\subsection{Decidability: proof of Theorem~\ref{th:main} and ~\ref{th:main2}}

\subsubsection{A na\"ive algorithm}
As a corollary of the proof of qualitative determinacy
(Theorem~\ref{theo:qdet}), we get a maximal strategy $\sigma_\can$
for player $1$ (see Definition~\ref{def:maximalstrategy}) to win
almost-surely B{\"u}chi games.
\begin{corollary}\label{cor:asmem}
  If player $1$ has an almost-surely winning strategy in a B{\"u}chi
  game {with observable actions} then the maximal strategy $\sigma_{\can}$ is almost-surely
  winning.
\end{corollary}

{
   A simple algorithm to decide for which player a
  game is winning can be derived from Corollary~\ref{cor:asmem}:
this simple algorithm enumerates all possible belief strategies
% for $X \subseteq\mathcal{P}(K)$, 
  and test each one of them to see if it is almost-surely
  winning. The test reduces to checking positive winning in one-player co-B{\"u}chi games
and can be done in exponential time.
}
As there is a doubly exponential number of {belief} strategies, 
  %$X \subseteq \mathcal{P}(K)$,
   this can be done in time doubly exponential. 
This algorithm also appears in \cite{GS-icalp09}.
 This settles the upper bound for Theorem~\ref{th:main}. The lower bounds are established in
  Theorem~\ref{theo-hard}, proving that this enumeration algorithm is
  optimal for worst case complexity.  While optimal in the worst case,
  this algorithm is {likely to be unefficient in practice}.  For instance, if player
  $1$ has no almost-surely winning strategy, then this algorithm will
  enumerate every single of the doubly exponential many {possible belief}
  strategies.  Instead, we provide fix-point algorithms which do not
  enumerate every possible strategy in Theorem~\ref{theo:qdec1} for
  reachability games and Theorem~\ref{theo:qdec2} for B{\"u}chi games.
  Although they should perform better on games with particular
  structures, these fix-point algorithms still have a worst-case
  2-\EXPTIME\ complexity.


\subsubsection{A fix-point algorithm for reachability games}

We turn now to the {\color{black} (fix-points)} algorithms which compute the set of supports that
are almost-surely or positively winning for various objectives.
\newcounter{theo:qdec1}
\setcounter{theo:qdec1}{\value{theorem}}
\begin{theorem}[Deciding positive winning in reachability games]
  \label{theo:qdec1} In a reachability game each initial distribution
  $\delta$ is either positively winning for player $1$ or surely
  winning for player $2$, and this depends only on
  $\supp(\delta)\subseteq \states$.
%and we denote $(\LL^1_{>0},\LL^2_{=1})$ the corresponding partition
%of $\parties{\states}$.
%In the first case, player $1$ has a memoryless strategy while in the second case player $2$
%has a strategy with finite memory $\parties{\states}$.
  The corresponding partition of $\parties{\states}$ is computable in
  time $\mathcal{O}\left(|G| \cdot 2^{|\states|}\right)$, where $|G| $ denotes
  the size of the description of the game,
  as the largest fix-point of a monotonic operator
$\Phi:\parties{\parties{V}}\to \parties{\parties{V}}$
computable in time linear in $|G| $.
  %The algorithm computes at
  %the same time the finite-memory strategies described in
  %Theorem~\ref{theo:memory}.
\end{theorem}

We denote $\targets$ the set of vertices whose colour is $1$.

\begin{proof}


Let $\LL_\infty\subseteq \parties{\states\bh\targets}$
be the greatest fix-point of the monotonic operator
$\Phi:\parties{\parties{\states\bh\targets}}\to \parties{\parties{\states\bh\targets}}$ defined by:
\be
\label{eq:defphi}
\Phi(\LL)=\{L\in \LL \mid
\exists j_L\in J, \forall d\in\signauxdeux, (\action_2(d)=j_L)\implies (\beldeux(L,d)\in \LL \cup \{\emptyset)\}\}\enspace,
\ee
in other words $\Phi(\LL)$ is the set of supports
such that player $2$ has an action which
ensure his next belief will be in $\LL$,
whatever signal $d$ he might receive.
%\end{multline*}
Let $\sigma_{\rand}$ be the strategy for player $1$ that plays randomly any action.

We are going to prove that:
\begin{enumerate}
\item[(A)] every support in $\LL_\infty$ is surely winning for player $2$,
\item[(B)] and $\sigma_{\rand}$ is positively winning from any support $L\subseteq\states$ which is not in $\LL_\infty$.
\end{enumerate}

We start with proving (A).
To win surely from any support $L\in\LL_\infty$, player $2$ uses the following
belief strategy $\tau_B$: when the current belief of player $2$ is $L\in\LL_\infty$ then player $2$
plays an action $j_L$ defined as in~\eqref{eq:defphi}.
By definition of $\Phi$ and since $\LL_\infty$ is a fix-point of $\Phi$,
there always exists such an action.
%and this defines a finite-memory strategy with memory $\LL_\infty$
%and update operator $\beldeux$.
When playing with the belief strategy $\tau_B$,
starting from a support in $\LL_\infty$,
the beliefs of player $2$ stay in $\LL_\infty$
and never intersect $\targets$ because $\LL_\infty\subseteq \parties{\states\bh\targets}$.
{According to property~\eqref{eq:beln_lemma} of beliefs (Lemma~\ref{lem:beliefs})},
this guarantees the play never visits $\targets$,
whatever strategy is used by player $1$.

We now prove (B).
%Remember that $\sigma_{\rand}$ is the memoryless strategy for player $1$
%that plays randomly any action.
Let
$\LL_0=\parties{\states\bh\targets}\supseteq
\LL_1=\Phi(\LL_0)\supseteq \LL_2=\Phi(\LL_1)\ldots$ and $\LL_\infty$
be the limit of this sequence, the greatest fix-point of $\Phi$.
  We
prove that for any support $L\in\parties{\states}$, if
$L\not\in\LL_\infty$ then: \be\label{eq:postoprove} \text{$\sigma_{\rand}$ is
  positively winning for player $1$ from $L$}\enspace.  \ee If $L\cap\targets
\not=\emptyset$,~\eqref{eq:postoprove} is obvious.  To deal with
the case where {$L\cap \targets =\emptyset$}, we define for every
$n\in\NN$, $\KK_n = \parties{\states\bh\targets} \bh \LL_n$, and we
prove by induction on $n\in\NN$ that for every $L\in\KK_n$, for every
initial distribution $\delta_L$ with support $L$, for every {behavioral} strategy
$\tau$, \be\label{eq:topo} \probimp{\sigma_{\rand},\tau}{\delta_L}{\exists m, 2\leq
  m\leq n+1, V_m\in\targets }>0 \enspace.  \ee For
$n=0$,~\eqref{eq:topo} is obvious because $\KK_0=\emptyset$.  Suppose
that for some $n\in\NN$, \eqref{eq:topo} holds for every $L'\in\KK_n$,
and let $L\in\KK_{n+1}\bh \KK_n$.
Then by definition of $\KK_{n+1}$, \be\label{eq:LLLn}
L\in\LL_{n}\bh\Phi(\LL_n)\enspace.  \ee Let $\delta_L$ be an initial
distribution with support $L$ and $\tau$ any behavioral strategy for player $2$.
Let $J_0\subseteq J$ be the support of $\tau(\delta_L)$ and $j_L\in J_0$.  According
to~\eqref{eq:LLLn}, by definition of $\Phi$, there exists a signal
$d\in D$ such that $\action_2(d)=j_L$ and
 $\beldeux(L,d)\not \in \LL_n$ and $\beldeux(L,d)\neq \emptyset$.
{According to  property~\eqref{eq:belief_compute} of beliefs (Lemma~\ref{lem:beliefs}),} 
 $\forall k \in \beldeux(L,d),\probimp{\sigma_{\rand},\tau}{\delta_L}{V_2 =k\land D_1=d}  > 0$.
   If
$\beldeux(L,d)\cap\targets \not= \emptyset$ then according to
the definition of beliefs,
$\probimp{\sigma_{\rand},\tau}{\delta_L}{V_2\in\targets}>0$.  Otherwise
$\beldeux(L,d)\in\parties{\states\bh\targets}\bh\LL_n=\KK_n$ hence
distribution $\delta_{d}:k\to \probimp{\sigma_{\rand},\tau}{\delta_L}{V_2 =k\mid D_1=d}$
has its support in $\KK_n$. By inductive hypothesis, for every
behavioral strategy $\tau'$,
\[\probimp{\sigma_{\rand},\tau'}{\delta_{d}}{\exists m\in\NN, 2\leq
  m\leq n+1, V_m\in\targets}>0\]
hence using the shifting lemma and the
definition of $\delta_{d}$,
\[
\probimp{\sigma_{\rand},\tau}{\delta}{\exists m\in\NN,
  3\leq m\leq n+2, V_m\in\targets}>0\enspace,\]
which completes the proof of the inductive
step.
{
Hence~\eqref{eq:topo} holds for every behavioral strategy 
$\tau$. Thus, according to Lemma~\ref{actioneq3},~\eqref{eq:topo}
holds as well for every general strategy $\tau$.
}

To compute
%The computation of 
the partition of supports between those positively winning for player $1$
and those surely winning for player $2$,
% and a surely winning strategy for player
it is enough to compute
%$2$ amounts to the computation of 
the largest fix-point of $\Phi$.
Since $\Phi$ is monotonic, and each application of the operator
can be computed in time linear in the size of the game ($|G|$)
and the number of supports ($2^{|\states|}$)
the overall computation can be achieved in time $|G| \cdot 2^{|\states|}$.
To compute the strategy $\tau_B$, it is enough to compute
for each $L\in\LL_\infty$ one action $j_L$ such that
$(\action_2(d)=j_L)\implies (\beldeux(L,d)\in\LL_\infty)$.
\end{proof}

As a byproduct of the proof one obtains the following bounds on time
and probabilities before reaching a target state, when player $1$ uses
the uniform memoryless strategy $\sigma_{\rand}$.  From an initial
distribution positively winning for the reachability objective, for
every strategy $\tau$, \be\label{eq:bounds}
\probimp{\sigma_{\rand},\tau}{\delta}{\exists n\leq 2^{\mid \states
    \mid}, C_n = 1}\geq \left(
  \frac{1}{p_{\min}\mid\actionsun\mid}\right)^{2^{\lvert\states\lvert}}\enspace,
\ee where $p_{\min}$ is the smallest non-zero transition probability.


\subsubsection{A fix-point algorithm for B{\"u}chi games}

To decide whether player $1$ wins almost-surely a B{\"u}chi game,
we provide an algorithm which runs in doubly-exponential time.
It uses the algorithm for reachability games as a sub-procedure.


\begin{theorem}[Deciding almost-sure winning in B{\"u}chi games]
  \label{theo:qdec2} In a B{\"u}chi game each initial distribution
  $\delta$ is either almost-surely winning for player $1$ or
  positively winning for player $2$, and this depends only on
  $\supp(\delta)\subseteq \states$.
%and we denote $(\LL^1_{>0},\LL^2_{=1})$ the corresponding partition
%of $\parties{\states}$.
%In the first case, player $1$ has a memoryless strategy while in the second case player $2$
%has a strategy with finite memory $\parties{\states}$.
The corresponding partition of $\parties{\states}$ is computable in
time $\mathcal{O}\left(2^{2^{|G|}}\right)$, where $|G|$ denotes the size of the description of the game,
as a projection of the greatest
fix-point $\LL_\infty$
of a monotonic operator
\[\Psi:
\parties{\parties{\states}\times\states}
\to
\parties{\parties{\states}\times\states}
\enspace.
\]
The operator $\Psi$ is computable using as a nested fix-point the operator $\Phi$ of Theorem~\ref{theo:qdec1}.
 The almost-surely winning belief strategy of player $1$ and the positively winning $2$-belief strategy of player $2$  can be extracted 
from $\LL_\infty$.
\end{theorem}

%In her positively winning strategy player $2$ uses the finite memory
%$\parties{\parties{\states}\times\states}$ to remember the possible
%pairs of current state and pessimistic belief of player $1$.
% The finite memory $\parties{\parties{\states}\times\states}$ of the
% positively winning strategy of player $2$ is used by player $2$ to
% remember what are the possible pairs of current state and pessimistic
% belief of player $1$.

\smallskip
%{\small SKETCH OF PROOF.}
%\begin{proof}[Sketch of proof]
We sketch the main ideas of the proof of Theorem~\ref{theo:qdec2}.

%We use the following notion in the sketch of proof.
%\begin{definition}[Uniform positive reachability]
%A support $L\subseteq K$ is positively winning for player $1$ for the uniform reachability game if player $1$ has a strategy $\sigma$ such that for every strategy
%$\tau$ of player $2$,
%\[
%\forall l\in L, \probimp{\sigma,\tau}{\delta_L}{\exists n,K_n\in\targets\mid K_0=l}>0\enspace.
%\]
%\end{definition}
%Intuitively, this means that
%player $1$ should win positively the reachability game whatever initial state has been selected by the initial distribution.

First, suppose that from \emph{every} initial support, player $1$ can
win positively the  reachability game.
{Then she can do so using a belief strategy and according to Lemma~\ref{lem:borelcantelli},}
this strategy guarantees
almost-surely the B{\"u}chi condition.
%This is no more true if the condition is relaxed to reachability instead of uniform reachability.

In general though player $1$ is not in such an easy situation and
there exists a support $L$ which is \emph{not} positively winning
for her for the reachability objective.
Then by qualitative determinacy, player $2$ has a strategy to achieve surely her safety objective
from $L$, which is \emph{a fortiori}
surely winning for her co-B{\"u}chi objective as well.


We prove that in case player $2$ can \emph{force with positive
  probability the belief of player $1$} to be $L$ eventually from another
support $L'$, then player $2$
{ has a general strategy to win positively from $L'$}.
This is not completely obvious because in general player $2$ cannot
know exactly \emph{when} the belief of player $1$ is $L$ (he can only
compute the 2-Belief, letting him know all the possible beliefs player
1 can have).  However player $2$ can make blind guesses,
and be right with $>0$ probability.
For winning positively from $L'$, player $2$ plays
totally randomly until he guesses randomly that the belief of player
$1$ is $L$, at that moment he switches to a strategy surely winning
from $L$.  Such a strategy is far from being optimal, because player
$2$ plays randomly and in most cases he makes a wrong guess about the
belief of player $1$.  However 
there is a non zero probability for his guess to be right.
%that he guesses correctly at the
%right moment the belief of player $1$.

Hence, player $1$ should surely avoid her belief to be $L$
or $L'$ if she wants to win almost-surely.
However, doing so player $1$ may prevent the play from
reaching target states, which may create another positively winning
support for player $2$, and so on. This is the basis of our fix-point algorithm.

Using these ideas, we prove that the set
$\LL_\infty\subseteq \parties{\states}$ of supports almost-surely
winning for player $1$ for the B{\"u}chi objective is the largest set of
initial supports from which:
\begin{multline}
\label{eq-dag}
\tag{$\dag$}
\textrm{player $1$ has a strategy
  which win positively the reachability game}\\
\textrm{and also ensures at the same time
  her belief to stay in } \LL_\infty .
\end{multline}

Property \eqref{eq-dag} can be reformulated as a reachability
condition in a new game whose states are states of the original game
augmented with beliefs of player $1$, kept hidden to player $2$.

The fix-point characterisation suggests the following algorithm for
computing the set of supports positively winning for player $2$:
$\parties{\states}\bh\LL_\infty$ is the limit of the sequence
$\emptyset=\LL_0'\subsetneq \LL_0'\cup \LL_1''\subsetneq\LL_0'\cup
\LL_1'\subsetneq \LL_0'\cup \LL_1'\cup \LL_2''\subsetneq\ldots
\subsetneq \LL_0'\cup \cdots \cup \LL'_m
=\parties{\states}\bh\LL_\infty$, where
\begin{itemize}
\item[(a)]
from supports in $\LL''_{i+1}$ player $2$ can surely guarantee the safety objective,
under the hypothesis that player $1$ 
{guarantees for sure} her beliefs to stay outside $\LL'_i$,
\item[(b)]
from supports in $\LL'_{i+1}$ player $2$ can ensure with positive probability the belief of player
$1$ to be in $\LL''_{i+1}$ eventually,
under the same hypothesis.
\end{itemize}

The overall strategy of player $2$ positively winning for the co-B{\"u}chi objective
% to hold positively
consists in playing randomly for some time until he decides to pick
up randomly a belief $L$ of player $1$ in some $\LL''_i$,
bets that the current belief of player $1$ is $L$ and that player $1$
guarantees for sure
her future beliefs 
will stay outside $\LL'_i$.
He forgets
the signals he has received up to that moment and switches
definitively to a strategy which guarantees (a).  With positive
probability, player $2$ %is lucky enough to
guesses correctly the belief of player $1$ at the right moment, and
future beliefs of player $1$ will stay in $\LL'_i$, in which case the
co-B{\"u}chi condition holds and player $2$ wins.

{In order to ensure (a), player $2$ makes use of the hypothesis
about player $1$ beliefs staying outside $\LL_i'$. For that player $2$ needs to keep track of all the possible beliefs of player $1$, hence the doubly-exponential memory.
The reason is player $2$ can infer
from this data structure some information about the possible actions played by player $1$: in case
for every possible belief of player $1$ an action $i\in I$ creates a risk to reach $\LL'_i$
then player $2$ knows for sure this action is not played by player $1$.
This in turn helps player $2$ to know which are the possible states of the game.
Finally, when player $2$ estimates the state of the game using his $2$-beliefs,
this gives a potentially more accurate estimation of the possible states than simply computing his $1$-beliefs.}

{The positively winning $2$-belief strategy of player $2$ has a particular structure.
All memory updates are deterministic except for one: from
the initial memory state $\emptyset$,
whatever signal is received there is non-zero chance that the memory state stays $\emptyset$ but it may as well 
be updated to several other memory states.}
%\qed
%\end{proof}
\smallskip

%Property~\eqref{eq-dag} can be formulated by mean of a fix-point
%according to Theorem~\ref{theo:qdec1}, hence the set of supports
%positively winning for player $2$ can be expressed using two nested
%fix-points.  This should be useful for actually implementing the
%algorithm and for computing symbolic representations of winning sets.
%


