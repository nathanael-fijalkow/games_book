Value iteration algorithms manipulate value functions and never construct any strategy, at least explicitly.
This is a key difference with strategy improvement algorithms (also called policy iteration algorithms) whose fundamental idea is to maintain and improve a strategy.
We assume that the games we consider in this section are positionally determined, therefore all strategies are assumed to be positional.

\vskip1em
Let us consider a game $\game$ and set as a goal to construct an optimal strategy for Eve.
As for value iteration algorithms we work with a value function: 
the key idea behind strategy improvement is to use $\val^{\sigma}$ to improve the strategy $\sigma$ 
by \emph{switching} an edge, which is an operation that creates a new strategy.
This involves defining the notion of \emph{switchable edge}:
the edge $(v,u)$ is switchable if 
\[
\delta(\val^{\sigma}(u),\col(v)) > \delta(\val^{\sigma}(v'),\col(v)) \text{ where } \sigma(v) = (v,v').
\]
Intuitively: according to $\val^{\sigma}$, playing $(v,u)$ is better than playing $\sigma(v)$.

Given a strategy $\sigma$ and an edge $e = (v,u)$ we use $\sigma[v \to e]$ to denote the strategy playing $e$ from $v$ and all other vertices follow $\sigma$.
Let us write $\sigma \le \sigma'$ if for all vertices $v$ we have $\val^{\sigma}(v) \le \val^{\sigma'}(v)$,
and $\sigma < \sigma'$ if additionally $\neg (\sigma' \le \sigma)$.

The difficulty is that $e = (v,u)$ being switchable does not mean that it is a better move than $\sigma(v)$ in any context,
but only according to the value function $\val^{\sigma}$, so it is not clear that $\sigma[v \to e]$ is better than $\sigma$.
Strategy improvement algorithms depend on the following two principles.

\begin{property}[Progress]
\label{1-property:progress}
Let $\sigma$ be a strategy and let $e = (v,u)$ be a switchable edge. 
Then $\sigma < \sigma[v \to e]$.
\end{property}

\begin{property}[Optimality]
\label{1-property:optimality}
Let $\sigma$ be a strategy that has no switchable edges, then $\sigma$ is optimal.
\end{property}

The algorithm is the following: start at an initial strategy $\sigma_0$. 
In each round $i$ compute $\val^{\sigma_i}$ and look for a switchable edge.
If there exists a switchable edge $e_i = (v_i,v'_i)$, let $\sigma_{i+1} = \sigma_i[v_i \to e_i]$ and iterate to the next round.
Otherwise, return the optimal strategy $\sigma_i$.

The algorithm computes a sequence of strategies 
$\sigma_0 < \sigma_1 < \sigma_2 < \dots$.
Note that any such sequence must be finite, since at each step we strictly increase in the ordering and there are only finitely many (positional) strategies. 

\vskip1em
If both progress and optimality principles hold as stated this yields a strategy improvement algorithm computing the optimal strategy.
Unfortunately such ideal properties rarely hold and it is often necessary to state and prove weaker properties,
we refer to \cref{3-chap:parity,4-chap:payoffs} for examples.
%Let us illustrate this by looking at \cref{1-fig:counter_example_strategy_improvement} representing a CoB{\"u}chi game.
%Assume that the initial strategy is $\sigma_0$ defined by 
%$\sigma_0(v_0) = (v_0,v_1),
%\sigma_0(v_1) = (v_1,v_1)$,
%and $\sigma_0(v_2) = (v_2,v_1)$.
%This strategy is losing from all vertices since it eventually ends up looping around $v_1$.
%This implies that there are no switchable edges and in particular the two winning self loops around $v_0$ and $v_1$ are not considered.
%However $\sigma_0$ is clearly not optimal, contradicting the progress principle.
%
%For this reason the initial strategy $\sigma_0$ must be carefully chosen.
%A solution is to add for each vertex of Eve a new edge for stopping the game, 
%and to define $\sigma_0$ to be the strategy choosing this option from every vertex.
%The benefit of this approach is to avoid declaring some vertices losing only because they are losing with the (badly chosen) initial strategy.
%
%\begin{figure}
%\centering
%  \begin{tikzpicture}[scale=1.3]
%    \node[s-eve] (v0) at (0,0) {$\begin{array}{c} v_0 \\ 2 \end{array}$};
%    \node[s-eve] (v1) at (2,0) {$\begin{array}{c} v_1 \\ 3 \end{array}$};
%    \node[s-eve] (v2) at (4,0) {$\begin{array}{c} v_2 \\ 2 \end{array}$};
%    % create edges
%    \path[arrow]
%      (v1) edge[bend left] (v0)
%      (v0) edge[bend left] (v1)
%      (v1) edge[bend left] (v2)
%      (v1) edge[selfloop=90] (v1)
%      (v2) edge[bend left] (v1)
%      (v2) edge[selfloop=0] (v2)
%      (v0) edge[selfloop=180] (v0);
%  \end{tikzpicture}
%\caption{The optimality principle rarely holds, here illustrated on a CoB{\"u}chi game.}
%\label{1-fig:counter_example_strategy_improvement}
%\end{figure}

\begin{remark}
In the description above we did not specify which switchable edge to choose.
Actually strategy improvement algorithms often switch more than one edge at a time, making this question worse: 
which subset of the switchable edges should be chosen? 
Many possible rules for choosing this set have been studied, as for instance the \emph{greedy all-switches} rule. 
\end{remark}
