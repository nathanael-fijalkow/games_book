
% \fbox{parler de la quantitative determinacy des Blackwell games
%   [Mar98] ?}
Pure memoryless determinacy for non-stochastic games with parity
objectives was established in~\cref{2-chap:regular} (see~\cref{2-thm:parity}). 
In this section we extend this result to stochastic games. 
We focus on reachability objectives, since we will see in~\cref{6-sec:relations} 
that other natural objectives reduce to reachability.

%\fbox{check whether pure memoryless determinacy is preserved by
%  these reductions}

\begin{theorem}[Pure positional determinacy for stochastic reachability games]
\label{6-thm:determinacy}
Stochastic reachability games are pure positionally determined.
\end{theorem}

\begin{proof}
  Let $G = (\vertices,E,\delta,\Win)$ be a stochastic reachability
  game.  We define an operator $\mathfrak{F}$ expressing Bellman-like
  equations for the game $G$:
  \[
  \mathfrak{F}(\nu)(v) = \left\{\begin{array}{l@{~~}l}
      1 & \text{if}\ v = \Win \\
      \max_{(v,v') \in E} \nu(v') & \text{if}\ v \in \VE \\
      \min_{(v,v') \in E} \nu(v') & \text{if}\ v \in \VA \\
      \sum_{v' \in \vertices} \delta(v)(v') \cdot \nu(v') & \text{if} \in \Randomvertices
    \end{array}\right.
  \]
  This operator, defined on the complete lattice $[0,1]^{\vertices}$
  (equipped with the pointwise standard inequality) is
  monotonic. Hence it admits a least fixpoint, which we denote
  $\lfp(\mathfrak{F})$.  We show that:
  \begin{lemma}
    \label{6-lem:lfpgeval}
    For every $v$:
    \[
    \lfp(\mathfrak{F}) (v) \le \sup_\sigma \inf_\tau
    \probm_{\sigma,\tau}^v(\Reach(\Win)) \le \inf_\tau \sup_\sigma
    \probm_{\sigma,\tau}^v(\Reach(\Win)) \enspace.
    \]
  \end{lemma}
  \begin{proof}
    We first argue for the right inequality:
    \begin{eqnarray*}
      \forall \sigma' \forall \tau:\
      \probm_{\sigma',\tau}^v(\Reach(\Win)) & \le & \sup_\sigma
      \probm_{\sigma,\tau}^v(\Reach(\Win)) \\
      \forall \sigma'\forall \tau :\  \inf_{\tau'}
      \probm_{\sigma',\tau'}^v(\Reach(\Win)) & \le & \sup_\sigma
      \probm_{\sigma,\tau}^v(\Reach(\Win)) \\
      \forall \tau:\ \sup_{\sigma'} \inf_{\tau'}
      \probm_{\sigma',\tau'}^v(\Reach(\Win)) & \le & \sup_\sigma
      \probm_{\sigma,\tau}^v(\Reach(\Win)) \\
      \sup_{\sigma'} \inf_{\tau'}
      \probm_{\sigma',\tau}'^v(\Reach(\Win)) & \le & \inf_{\tau} \sup_\sigma
      \probm_{\sigma,\tau}^v(\Reach(\Win)) 
    \end{eqnarray*}

    For proving the left inequality, it is sufficient to show that
    $v \mapsto \sup_\sigma \inf_\tau
    \probm_{\sigma,\tau}^v(\Reach(\Win))$ is a fixpoint of
    $\mathfrak{F}$.  Pick $v \in \VE$. 
    % First, when $\sigma$ and $\tau$
    % are both fixed, then
    % \[
    %   \probm_{\sigma,\tau}^v(\Reach(\Win)) = \sum_{v' \in
    %     \text{supp}(\sigma(v))} \sigma(v)(v') \cdot
    %   \probm_{\sigma'_{v'},\tau'}^{v'}(\Reach(\Win))
    % \]
    % where, given $v' \in \text{supp}(\sigma(v))$, $\sigma'_{v'}$ is
    % the strategy coinciding with $\sigma$ after the first move to $v'$
    % (formally, $\sigma'_{v'}(v' h) = \sigma(v v' h)$ for every finite
    % sequence of vertices $h$).
 \begin{eqnarray*}
      \sup_\sigma \inf_\tau \probm_{\sigma,\tau}^v(\Reach(\Win)) & = &
      \sup_\sigma \inf_\tau \sum_{v'} \sigma(v)(v')  \cdot
      \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
      & & \text{where}\ \sigma'\ \text{and}\ \tau'\ \text{are residual
        strategies after}\ (v,v') \\
      & = & \sup_\sigma \sum_{v'} \sigma(v)(v')  \cdot \inf_\tau \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
      & & \text{since}\ \tau\ \text{is not concerned by this choice}
      \\
      & = & \max_{(v,v') \in E} \sup_{\sigma\ \text{s.t.}\
        \sigma(v) = v'} \inf_\tau
      \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
      & & \text{since deterministic choices are the best} \\
      & = & \max_{(v,v') \in E} \sup_{\sigma'} \inf_{\tau'}
      \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
      & & \text{since the first choice is now fixed} \\
      & = & \mathfrak{F}(\probm_{\sigma,\tau}^{\bullet}(\Reach(\Win)))(v)
    \end{eqnarray*}
    where $\probm_{\sigma,\tau}^{\bullet}(\Reach(\Win)))$ is the
    function with associates with vertex $v$ the value
    $\probm_{\sigma,\tau}^{v}(\Reach(\Win)))$.
    % \begin{eqnarray*}
    %   \sup_\sigma \inf_\tau \probm_{\sigma,\tau}^v(\Reach(\Win)) & = &
    %   \sup_\sigma \inf_\tau \sum_{v'} \sigma(v)(V'd)
    %   \cdot \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
    %   & & \text{where}\ \sigma'\ \text{and}\ \tau'\ \text{are residual
    %     strategies after}\ (v,\adist,v') \\
    %   & = & \sup_\sigma \sum_{(\adist,v')} \sigma(v)(\adist,v')
    %   \cdot \inf_\tau \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
    %   & & \text{since}\ \tau\ \text{is not concerned by this choice}
    %   \\
    %   & = & \max_{(v,\adist) \in E} \sup_{\sigma\ \text{s.t.}\
    %     \sigma(v) = \adist} \inf_\tau
    %   \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
    %   & & \text{since deterministic choices are the best} \\
    %   & = & \max_{(v,\adist) \in E} \sup_{\sigma'} \inf_{\tau'}
    %   \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
    %   & & \text{since the first choice is now fixed} \\
    %   & = & \mathfrak{F}(\probm_{\sigma,\tau}^{\bullet}(\Reach(\Win)))(v)
    % \end{eqnarray*}

    Assume now that $v \in \VA$. 
    % \begin{eqnarray*}
    %   \sup_\sigma \inf_\tau \probm_{\sigma,\tau}^v(\Reach(\Win)) & = &
    %   \sup_\sigma \inf_\tau \sum_{(\adist,v')} \tau(v)(\adist,v')
    %   \cdot \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
    %   & & \text{where}\ \sigma'\ \text{and}\ \tau'\ \text{are residual
    %     strategies after}\ (v,\adist,v') \\
    %   & = & \sup_\sigma \min_{(v,\adist) \in E} \inf_{\tau\
    %     \text{s.t.}\ \tau(v) = \adist} \sum_{v'} \tau(v)(\adist,v')
    %   \cdot \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
    %   & & \text{since deterministic choices are the best} \\
    %   & = & \min_{(v,\adist) \in E}  \sup_{\sigma'} \inf_{\tau\
    %     \text{s.t.}\ \tau(v) = \adist} \sum_{v'} \tau(v)(\adist,v')
    %   \cdot \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
    %   & & \text{since}\ \sigma\ \text{is not concerned with the first
    %     choice} \\
    %   & = & \mathfrak{F}(\probm_{\sigma,\tau}^{\bullet}(\Reach(\Win)))(v)
    % \end{eqnarray*}
    \begin{eqnarray*}
      \sup_\sigma \inf_\tau \probm_{\sigma,\tau}^v(\Reach(\Win)) & = &
      \sup_\sigma \inf_\tau \sum_{v'} \tau(v)(v')
      \cdot \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
      & & \text{where}\ \sigma'\ \text{and}\ \tau'\ \text{are residual
        strategies after}\ (v,v') \\
      & = & \sup_\sigma \min_{(v,v') \in E} \inf_{\tau\
        \text{s.t.}\ \tau(v) = v'} 
       \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
      & & \text{since deterministic choices are the best} \\
      & = & \min_{(v,v') \in E}  \sup_{\sigma'} \inf_{\tau\
        \text{s.t.}\ \tau(v) = v'} \probm_{\sigma',\tau'}^{v'}(\Reach(\Win)) \\
      & & \text{since}\ \sigma\ \text{is not concerned with the first
        choice} \\
      & = & \mathfrak{F}(\probm_{\sigma,\tau}^{\bullet}(\Reach(\Win)))(v)
    \end{eqnarray*}

    Finally, let  $v \in \Randomvertices$. 
    \begin{eqnarray*}
      \sup_\sigma \inf_\tau \probm_{\sigma,\tau}^v(\Reach(\Win)) & = & \sum_{v'} \delta(v)(v') \cdot \sup_{\sigma'} \inf_{\tau'} \probm_{\sigma',\tau'}^{v'}(\Reach(\Win))\\
            & & \text{where}\ \sigma'\ \text{and}\ \tau'\ \text{are residual
                strategies after}\ (v,v') \\
      & = & \mathfrak{F}(\probm_{\sigma,\tau}^{\bullet}(\Reach(\Win)))(v)
    \end{eqnarray*}
    This concludes the proof.
%
    
  \end{proof}

  \begin{lemma}
    \label{6-lem:lfpleval}
    For every $v \in \vertices$:
    \[
    \lfp(\mathfrak{F})(v) \ge \inf_\tau \sup_\sigma
    \probm_{\sigma,\tau}^v(\Reach(\Win)) \enspace.
    \]
  \end{lemma}

  \begin{proof}
    Let $v \in \VA$. Since $\lfp(\mathfrak{F})$ is a fixpoint of
    $\mathfrak{F}$, and because the arena is finitely branching, there exists $v'\in \vertices$ such
    that $(v,v') \in E$ and:
    \[
      \lfp(\mathfrak{F})(v) = \lfp(\mathfrak{F})(v') \enspace.
    \]
    We define $\tau^*$ the MD strategy, that selects for every
    $v\in \VA$ such a vertex $v'$, and we show now that for every
    $v \in \vertices$:
    \[
    \lfp(\mathfrak{F})(v) = \sup_\sigma
    \probm_{\sigma,\tau^*}^v(\Reach(\Win)) \enspace.
    \]
    When $\tau^*$ is fixed, the game becomes a finite Markov decision
    process $\game_{\tau^*}$ (in that MDP, the random vertices are
    isolated from the non-deterministic ones, see
    Remark~\ref{5-rk:??}). Note that by standard results on MDP,
    see~\ref{5-thm:quantireach} there exists $\sigma^*$ MD such that
    for every $v \in V$:
    \[
    \probm_{\sigma^*,\tau^*}^v(\Reach(\Win)) = \sup_\sigma
    \probm_{\sigma,\tau^*}^v(\Reach(\Win)) \enspace.
    \]
    Furthermore, $\probm_{\sigma^*,\tau^*}^\bullet(\Reach(\Win))$ is
    the least fixpoint of the Bellman equations for the MDP
    $\game_{\tau^*}$ (see~\ref{5-eq:Bellman}).
    
    It remains to observe that $\lfp(\mathfrak{F})$ satisfies the
    Bellman equations of the MDP under $\tau^*$: this is true from
    \Eve vertices since equations at those vertices are the same in
    the game and in the MDP; this is true from \Adam vertices by local
    optimality of $\tau^*$. Hence we deduce that
    \[
      \lfp(\mathfrak{F}) \ge
      \probm_{\sigma^*,\tau^*}^\bullet(\Reach(\Win)) = \sup_\sigma
      \probm_{\sigma,\tau^*}^\bullet(\Reach(\Win)) \ge \inf_\tau
      \sup_\sigma \probm_{\sigma,\tau}^\bullet(\Reach(\Win)) \enspace.
    \]
    This concludes the proof of the lemma. 
  \end{proof}
By Lemma~\ref{6-lem:lfpgeval} and~\ref{6-lem:lfpleval}  we derive that:
 \[
   \lfp(\mathfrak{F}) =
   \sup_\sigma \inf_\tau 
    \probm_{\sigma,\tau}^\bullet(\Reach(\Win)) =  \inf_\tau \sup_\sigma
   \probm_{\sigma,\tau}^\bullet(\Reach(\Win)) = 
   \probm_{\sigma^*,\tau^*}^\bullet(\Reach(\Win)) \enspace.
  \]
  This proves that $\game$ is pure memoryless determined, and that
  $(\sigma^*,\tau^*)$ is an optimal pure positional strategy profile.
  % \begin{itemize}
  % \item $\tau^*$ is optimal (and MD)
  % \item $\sigma^*$ is optimal (and MD)
  % \item we can compute the values by iterating $\mathfrak{F}$ 
  % \end{itemize}
  %
   
\end{proof}

The proof of Theorem~\ref{6-thm:determinacy} yields an algorithm to
compute the value of a stochastic game with reachability
objective. Indeed, because the value in every vertex $v \in \vertices$
agrees with $\lfp(\mathfrak{F})(v)$, it suffices to compute
iteratively approximations of the fixpoint $\lfp(\mathfrak{F})$.  This
provides the first value iteration algorithm for general stochastic
reachability games.

Historically this result was proven in the restricted framework of
stopping games. The stochastic game $\game = (\arena,\Omega)$ is said
stopping whenever the arena $\arena$ has two distinguished sink
vertices $\vwin$ and $\vlose$, and for all strategies $\sigma$ and
$\tau$, for every vertex $v$,
$\probm_{\sigma,\tau}^{\arena,v}(\Reach(\{\vwin,\vlose\})) =
1$. Whenever the game is stopping, $\mathfrak{F}$ has a unique
fixpoint, hence the value iteration algorithm can be applied from any
initial tuple of values.
% \fbox{say more?}  Originally papers assume stopping games...

% \nat{The positional determinacy also holds for stochastic parity games
%   (see CHJ-soda04). Check whether it is a byproduct of reductions.}

% \begin{remark}
%   Let $\arena$ be an arena with two distinguished sink vertices
%   $\vwin$ and $\vlose$. Assume $\Omega$ is the reachability objective
%   $\Reach(\Win)$ where $\Win = \{\vwin\}$. The stochastic game $\game
%   = (\arena,\Omega)$ is said stopping whenever for all strategies
%   $\sigma$ and $\tau$,
%   $\probm_{\sigma,\tau}^{\arena,v}(\Reach(\{\vwin,\vlose\})) = 1$ for
%   every vertex $v$. Whenever the arena/game is stopping,
%   $\mathfrak{F}$ has a unique fixpoint. \fbox{say more?}
% \end{remark}

Notice that the proof of Theorem~\ref{6-thm:determinacy} and the value
iteration algorithm which derives from it do not assume that the game
is stopping, nor that distributions are uniform.
