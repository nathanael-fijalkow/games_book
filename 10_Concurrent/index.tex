\newcommand{\ValueOp}{\text{valOp}}
\newcommand{\rk}{\text{rk}}
\newcommand{\crgLim}{\text{crgLim1}}

%\newcommand{\PSPACE}{\text{PSPACE}}


This chapter considers concurrent games. The concurrent games we consider are extensions of the games considered in Chapter 2 and 4, but 
where the choice of which edge to choose in a round is determined not by the choice of the owner of the vertex (indeed the vertices in concurrent games have no owners), but by the outcome of a matrix game corresponding to the vertex and played in that round. 
A matrix game is in turn a generalization of rock-paper-scissors, where each player picks an action simultaneously and then their pair of actions determines the outcome.

We will consider concurrent discounted, reachability and mean-payoff games and the definitions of the different objectives is as in the introduction. 
The chapter is divided into four sections:
\begin{enumerate}
\item The first section considers matrix games
\item The second section focuses on concurrent discounted games
\item The third section considers concurrent reachability games
\item The fourth section is about concurrent mean-payoff games
\end{enumerate}
As we go through the sections in this chapter, the complexity of the strategies and the computational complexity of solving the games rises: Indeed, since the games are generalizations of rock-paper-scissors, the strategies used requires randomness, but towards the end, no optimal or finite-memory $\epsilon$-optimal strategies exists in general and even the principle of sunken cost does not apply! 
Even with all this, the related questions about values are solvable in polynomial space and thus also in exponential time even in the last section.
The results we will focus on characterizes the complexity of the both the strategies as well as the computational complexity.
In each section we first give some positive result and some number of negative results. Each negative result also applies to the classes of games considered in the latter sections and each positive result applies to the classes considered in earlier sections (however, the positive results of latter sections will have worse complexity than the positive results from earlier sections).
As mentioned, the strategies for this chapter requires randomness and not too surprisingly, this implies that there is little difference between having stochastic or deterministic transition functions.

\section{Definitions}
The definition of arena $\arena$ in this chapter is $\arena=(G,\dest)$, where $G=(V,E)$ is a graph and $\dest:V\times A\times A\rightarrow \Dist(E)$. In particular, we are not using the sets $\VA$ and $\VE$.
The games are played similarly to before and formally as follows: 
There is a token, initially on the initial vertex. 
Whenever the token is on some vertex $v$, 
Eve selects an action $r$ in $A$ and Adam selects an action $c$ in $A$. The edge $e=(v,c,w)$ is then drawn from the distribution $\dest(v,r,c)$ and the token is pushed from $v$ to $w$.
 In general, the game continues like that forever.

We will use the following simplifying assumptions in this chapter:
\begin{enumerate}
\item We will assume that all colors are in $\{0,1\}$, except for the section on Matrix games where we additionally also allow $-1$ (to be able to easily illustrate the game rock-paper-scissors). This simplifies some expressions, but generally, the dependency on the number of colors is not too bad comparatively.
\item To make illustrations easier, we assume that for any pair of edges $e,e'$ in $\dest(v,a,a')$ for any $v,a,a'$, we have that $c(e)=c(e')$, i.e. the color does not depend on which edge is picked from $\dest(v,a,a')$, but only $v,a,a'$. This assumption does not matter for the types of games considered.
\end{enumerate}

We will overload the notation slightly for notational convenience, in that for any $v,a,a'$, we will write $c(v,a,a')$ for $c(e)$ where $e\in \dest(v,a,a')$ (note that the second assumption ensures that this is well-defined, i.e. there is only one such color).


A vertex $v$ is absorbing iff each player has only 1 action in $v$ and $\Delta(v,1,1)=v$.

To describe the complexity of good stationary strategies in concurrent games, we will use the notion of patience. Given a probability distribution $d\in \Dist$ the distribution has patience $p$ if $p=\min_{i\in \supp(d)} d(i)$ (i.e. the patience is the smallest, non-zero probability that an event may happen according to $d$).
In essence, if you have low enough patience you can typically guess the strategy and check whether it is a good strategy (when you fix a strategy, the game becomes a Markov decision process, which are relative easy to work with), the game can solved in $\NP\cup \coNP$. However, some times the patience is huge and writing down a good strategy, in binary, cannot be done in polynomial space (it is quite surprising in some sense that even with this property, finding the values in the games remain in $\PSPACE$).

We will illustrate a stochastic arena $\arena=(G,\dest)$ as follows:
For each non-absorbing vertex $v$, there is matrix.
 Entry $(i,j)$ of the matrix illustrating $v\in V$ describes what happens if, when the token is on $v$, Eve plays $i$ and Adam $j$. The entry contains a color $c$, which is $c(v,i,j)$, and 
there is an arrow from entry $(i,j)$ of $v$ to $w$ if there is an edge   
$e=(v,c,w)$ in $\dest(v,i,j)$. 
 The arrow corresponding to $e$ is denoted with the probability $\dest(v,i,j)(e)$. 
Especially, to simplify the illustrations we will do as follows: If $|\supp(\dest(v,i,j))|=1$, we do not include the probability (which is 1). Also, in that case, let $e=(v,c,w)=\dest(v,i,j)$ 
and 
if $v=w$, we omit the arrow and if $w$ is absorbing we write $c^*$ in entry $i,j$ of $v$, where $c$ is the color $c(w,1,1)$ (in this case, we omit the number $c(e)$ from the illustration, but in none of our illustrations does this number matter for what we try to illustrate). 




\section{Matrix games}
A matrix game is a game defined from a $(R\times C)$-matrix $M$  of numbers for some $R,C$.
The game is played as follows: Eve picks a row $r$ and Adam picks a column $c$ simulations like in rock-paper-scissors. Adam then pays Eve $M[r,c]$, i.e. the content of the entry defined by being in row $r$ and column $c$.
A strategy in such a game for Eve (resp. Adam) consists of a distribution over the rows (resp. columns). 
There is an illustration of rock-paper-scissors as a matrix game in Figure~\ref{fig:rps}.


\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]
\ma{main}[]{3}{3};

\node at (main-1-1.center) {0};
\node at (main-2-2.center) {0};
\node at (main-3-3.center) {0};
\node at (main-1-2.center) {-1};
\node at (main-2-3.center) {-1};
\node at (main-3-1.center) {-1};
\node at (main-1-3.center) {1};
\node at (main-2-1.center) {1};
\node at (main-3-2.center) {1};


\end{tikzpicture}
\caption{Rock-paper-scissors. The color is 1 if Eve wins, 0 if they draw and -1 if Adam wins. Also, the actions are ordered as in the name of the game}\label{fig:rps}
\end{figure}

The following theorem lists some known results for matrix games:
\begin{theorem}\label{lem:mat}
Each $(m\times n)$-matrix game $M$ is determined and there exists optimal strategies for each player. 
\begin{itemize}
\item The value and an optimal strategy for each player can be found in polynomial time and the problem is equivalent to linear programming.

\item Let $c>0$ be some constant. Consider the matrix $cM$ where each entry of $M$ has been multiplied by $c$. Then, the value of $cM$ is $cv$.
\item Let $c$ be some constant. Consider the matrix $M+c$ where each entry of $M$ is $c$ larger (additively). Then, the value of $M+c$ is $v+c$.
\item The value of matrix games are monotone in the entries.
\end{itemize}
\end{theorem}
We will omit the proof of the existence of values, optimal strategies and the first bullet.
The second bullet can be viewed as changing currency and clearly, this does not affect the optimal strategy.
The third bullet can be viewed as getting a reward before playing the game, and again, clearly this does not affect how to play it.
The last bullet can be seen from that each pair of strategies must give a higher reward if the entries of the matrix is higher.
This is especially true if you consider the optimal strategy for Eve in $M$ together with an arbitrary strategy for Adam, which then shows that the value is higher.

%if space: add proof

Given a matrix $M$, we will by $\Value[M]$ denote the value of the matrix game $M$. 

Perhaps interestingly, an illustration of a matrix $M$ can be viewed as a game arena $\arena$ (for concurrent games) with only one non-absorbing vertex. In each type of games considered in this section (apart from concurrent reachability games, where no game can be illustrated as a matrix with non-star entries different from 0), the value of the game with that arena matches $\Value[M]$ and the optimal strategies for each player is to play an optimal strategy from $M$ in each round. One can also consider a game arena $\arena^*$ with an illustration similar to $M$, but where there is a star in each entry (and $c(v,i,j)=0$ for the unique non-absorbing state $v$ and any pair of actions $i,j$).
Again, the value is $\Value[M]$ (except for the case of discounted objectives, where the value is $(1-\delta)\Value[M]$) and the optimal strategies for each player is again to play an optimal strategy from $M$. 

One could easily be lead to believe that in games (called repeated games with absorbing states) that can be illustrated as a single matrix $M$ with some entries stared and others not, the value would be similar to $\Value[M]$ and the optimal strategy would again be to play the optimal strategy from $M$. 
However, this is very much not true and indeed, many of the games in this chapter, illustrating how complex concurrent games can be, are repeated games with absorbing states! In particular repeated games with absorbing states may (1)~have irrational values and probabilities in optimal strategies (with any objective), (2)~have no optimal strategies (for reachability and mean-payoff objectives) and (3)~have no $\epsilon$-optimal finite-memory or $\epsilon$-optimal Markov strategies (for mean-payoff objectives)!



\section{Concurrent discounted games}
In this section we focus on concurrent discounted games. 
The key property of these games is that to a high degree, only the relative early part of the play matters.
We will first argue that the value iteration algorithm works and especially converges to the value of the game and then that there are stationary optimal strategies in concurrent discounted games.
While the value iteration algorithm also works for the games considered in the latter sections, we will not explicitly show it there, since the proofs become much more complex. The argument here however will allow us to show quite a few more statements in essence as corollaries of the theorem that value iteration works.


The value iteration algorithm is based on the concept of finite-horizon (or time limited) games. It is also sometimes referred to as dynamic programming.
Specifically, apart from the usual definition of a game, there is an additional integer $T$, denoting how many rounds are remaining initially, and a vector $v$, assigning a reward to each node if the game ends in that node with 0 rounds remaining. After round $T$ the reward is 0. 
I.e. for $T=0$, the outcome reward from node $x$ is $v_x$ in the first round and 0 in each later round.
Let $\ValueOp^T(v)$ be the vector that assigns to each node its value in the game with time-limit $T$ with vector $v$.

In general this formulation leads to a simple dynamic algorithm that computes $\ValueOp^T(v)$ inductively in $T$. 
We have that $\ValueOp^0(v)=v$ and given $\ValueOp^{T-1}(v)$ it is easy to compute $\ValueOp^T(v)$ because, if Eve selects row $i$ and Adam column $j$ in node $x$ in the first round, the outcome is \[
\sum_{v\in V}\ValueOp^{T-1}(v)\dest(x,i,j)(v)
\]
and thus $(\ValueOp^T(v))_x$ is the value of the matrix $M^{T,x,v}$, where entry $i,j$ is \[
\sum_{v\in V}\ValueOp^{T-1}(v)\dest(x,i,j)(v)
\]

It is common to start with the all-0 vector for $v$ when using the value iteration algorithm.

The following lemma shows many interesting properties of concurrent discounted games.






\begin{lemma}\label{cor:long}
Concurrent discounted games have the following properties:
\begin{itemize}
\item The value iteration algorithm converges for any initial vector $v$
\item The value iteration algorithm has an unique fix-point, independent of the initial vector $v$.
\item There are optimal stationary strategies in concurrent discounted games and the unique fix-point of the value iteration algorithm is the value (thus, the games are determined)
\item The value of a concurrent discounted game can approximated in PPAD
\item There are $\epsilon$-optimal stationary strategies with patience below $\frac{m\log(\epsilon/2)}{\log(1-\gamma)\epsilon}$.
\end{itemize}
\end{lemma}
\begin{proof}
The first item comes from considering the vectors $v$ and $\ValueOp^1(v)$. We thus have that $\ValueOp^{T+1}(v)\in [\ValueOp^{T}(v)-(1-\gamma)^T,\ValueOp^{T}(v)+(1-\gamma)^T]$ for all $T$. The statement then comes from that $\sum_{i=1}^\infty (1-\gamma)^i$ is a converging sum.

The second item comes from considering two fix-points, $u,v$. I.e., $\ValueOp^1(v)=v$ and thus $\ValueOp^T(v)=v$ for all $T$. Similar for $u$.
But, $v=\ValueOp^T(v)\in [\ValueOp^T(u)-(1-\gamma)^T, \ValueOp^T(u)+(1-\gamma)^T]=[u-(1-\gamma)^T, u+(1-\gamma)^T]$. Since it is true for all $T$, we have that $u=v$.

\begin{claim}
 Consider some $T$ and the strategy for Eve that plays the first $T$ steps following an optimal strategy in the finite-horizon game of length $T$ with vector $v$, followed by playing arbitrarily. 
Then, the outcome is above $\ValueOp^T(v)- (1-\gamma)^T\max_{i} v_i$.
\end{claim}
\begin{proof}
For any strategy for Adam, the expected reward for the first $T$ rounds is at least the expected reward in the finite-horizon game. In each remaining round, the reward is at least $0$ in the real game, but $v_i$ in round $T$ for some $i$ followed by 0's in the finite-horizon game.
Since the outcome is $\ValueOp^T(v)$ in the finite-horizon game, the real outcome is then as described.
\end{proof}
One can show a similar statement for Adam.
For any $\epsilon>0$ one can pick a big enough $T$ such that $(1-\gamma)^T\max_{i} v_i\leq \epsilon$.


Let $v^*$ be the unique fix-point of the value iteration algorithm. 
Thus, $v^*=\ValueOp^T(v^*)$ for all $T$. Pick some optimal strategies $\sigma_x,\tau_x$ in $M^{T,x,v^*}$ for each $x$. Let $\sigma^*,\tau^*$ be the strategies that play $\sigma_x,\tau_x$ whenever in node $x$ in each round.
The strategy $\sigma,\tau$ are optimal in $\ValueOp^T(v^*)$ for each $T$, because $v^*$ is a fix-point. 
But, for each $\epsilon>0$,  the strategy $\sigma$ ensures outcome at least $v-\epsilon$ and $\tau$ ensures outcome at most $v+\epsilon$ using the claim. Hence, the third item follows.


The fourth item follows from that the value iteration algorithm is a contraction.

For the fifth item, consider the strategy used in the claim. 
Let $T$ be $\log(\epsilon/2)/\log(1-\gamma)$, i.e. $T$ is such that \[
\gamma \sum_{i=T}^{\infty}(1-\gamma)^i=\epsilon/2
\]
or in words, $T$ is such that the total outcome of each step after the $T$-th step is at most $\epsilon/2$.
Intuitively, if we modify the strategy very little, then the change is unlikely to come up in the first $T$ steps. More precisely, we will modify our strategy so that the probability that change will matter is less than $\epsilon/2$. That implies that the outcome differs by at most $\epsilon$ from the value.
We will use this intuition together with the argument for the third item to give a bound on the patience of $\epsilon$-optimal strategies. Fix some optimal stationary strategy $\sigma$ for Eve and an arbitrary stationary strategy $\tau$ for Adam. Let $\sigma'$ be a stationary strategy obtained from $\sigma$ rounded greedily  so that each probability is a rational number with denominator \[
q=mT/\epsilon=\frac{m\log(\epsilon/2)}{\log(1-\gamma)\epsilon}.
\] We will argue that $\sigma'$ is $\epsilon$-optimal.

The rounding proceeds inductively as follows for each node $x$:
The numbers $p_i$ are the original probability and the numbers $p_i'$ are the new probabilities.
For each $i$, the number $p_i'$ is defined as follows: If $\sum_{j=1}^{i-1}(p_i-p_i')>0$, then round up (i.e. $p_i'$ is the smallest rational with denominator $q$ so that $p_i<p_i'$) and otherwise round down, except the last number $p_\ell'$, which is such that $\sum_{j=1}^{\ell}p_i'=1$.
Note that this ensures that $-1/q<\sum_{j=1}^{i-1}(p_i-p_i')<1/q$. It also ensures that $|p_i-p_i'|<1/q$ for all $i$ (including for $i=\ell$).

For all nodes $x$ and rounds $T'\leq T$ we will define some random variables.
Specifically, the random variables denote what happen in round $T'$ if in node $x$.
The random variable $X_{x,T'}$ (resp. $Y_{x,T'}$) denotes the action picked by Eve if Eve follows $\sigma$ (resp. $\sigma'$).
The random variable $Z_{x,T'}$ denotes the action picked by Adam.
For each action pair $(i,j)$  the random variable $W_{x,i,j,T'}$ denotes the node entered in round $T'+1$, if Eve picks $i$ and Adam $j$. 
(As a side note: Each of the random variables are distributed the same way independent of $T'$).
Each of these random variables are independent of each other, except that (as we will define later) the random variables $X_{x,T'}$ and $Y_{x,T'}$ for each $x,T'$ are very much not independent of each other.

We see that we can view the first $T$ steps of the play when Eve follows $\sigma$ by only considering the outcome of $X_{x,T'}$, $Z_{x,T'}$ and $W_{x,i,j,T'}$ for all $T'$ and $x$ (even stronger: We only need to consider one $x,i,j$ for each $T'$, because the token is on only one node at a time). Similarly, for $\sigma'$, but using $Y_{x,T'}$ instead of $X_{x,T'}$.
For this to work, note that each random variable should be independent, except that the random variables $X_{x,T'}$ and $Y_{x',T''}$ need not be independent of each other for any $x',T''$. This is precisely the property we had for them!
For each $x,T'$,  we will then use a coupling $C_{x,T'}=(X'_{x,T'},Y'_{x,T'})$, a distribution over $[m]^2$, such that $X'_{x,T}$ is distributed as $X_{x,T'}$ and $Y'_{x,T'}$ is distributed as $Y_{x,T'}$. We will use a classic result for distributions, called the Coupling Lemma.

To introduce the Coupling Lemma, first, we need the notion of total variation distance. Given two distributions, $\Delta$ and $\Delta'$ over a set $S$, the total variation distance $t$ between $\Delta$ and $\Delta'$ is \[
t(\Delta,\Delta')=\frac{1}{2}\sum_{x\in S} |\Delta(x)-\Delta'(x)|
\] 

\begin{lemma}
For any distributions $\Delta$ and $\Delta'$ over a set $S$, we have 
\begin{itemize}
\item for all couplings $(X,Y)$ of $\Delta$ and $\Delta'$, that \[
t(\Delta,\Delta)\leq \Pr[X\neq Y]
\]
\item that there is a coupling $(X',Y')$ of $\Delta$ and $\Delta'$ satisfying that \[
t(\Delta,\Delta)= \Pr[X'\neq Y']
\]
\end{itemize}
\end{lemma}

Because of our rounding, we have that $t(X'_{x,T'},Y'_{x,T'})<\frac{m}{2q}$. 
Using that with the coupling lemma (the second part to be precise), lets us find a coupling $C_{x,T'}=(X'_{x,T'},Y'_{x,T'})$ 
such that $\Pr[X'_{x,T'}\neq Y'_{x,T'}]<\frac{m}{2q}$.

Consider the plays $\play_1,\play_2$ for when Eve follows $\sigma$ or $\sigma'$ respectively.
We can view the first $T$ steps of these plays by considering $X'_{x,T'}$ instead of $X_{x,T'}$ and similar when Eve follows $\sigma'$.
We can therefore see that the first $T$ steps two plays are different with probability 
$=p<\frac{mT}{2q}=\epsilon/2$
 using union bounds. 
 

We therefore see that the value for the path $\play_1$ cannot differ from the value of $\play_2$ with more than $p\gamma\sum_{i=1}^T(1-\gamma)^i=p$. I.e. in the worst case, if $\play_1$ and $\play_2$ differs, the reward is 1 in each step for $\play_1$ but 0 in each step for $\play_2$.
Also, the rewards in the steps after step $T$ can also differ by at most $1$ and by our choice of $T$, we have that outcome contributed from these remaining steps are worth less than $\epsilon/2$ as well.
Hence, we see that $\sigma'$ obtains the same as $\sigma$ except for $\epsilon$ against any strategy $\tau$ and is thus $\epsilon$-optimal.













\end{proof}

There is a classic problem in geometry called the sum-of-square-roots problem. The problem is defined as follows:
Let $a,b_1,b_2,\dots,b_n$ be natural numbers. Is $\sum_{i=1}^n\sqrt{b_i}>a$? 

The problem comes up for decision problems about distances in Euclidean space. It is not known to be in P or NP for that matter, but is in the fourth level of the countering hierarchy, slightly inside PSPACE. The issue is in essence that it is not known how good an approximation of $\sqrt{b_i}$ is necessary to decide the strict inequality. 

We will use the sum-of-square-roots problem to give an informal hardness argument, in that finding the exact value of a concurrent game is in general harder than solving the sum-of-square-roots problem. 

Consider the following game $G$:
There are three vertices, $\{0,1,s\}$ where $0$ and $1$ are absorbing, with color 0 and 1 respectively.
The vertex $s$ is such that (1)~$c(s,i,j)=0$, (2)~$\dest(s,i,i)=1$ (for $i\in \{1,2\}$), (3)~$\dest(s,2,1)=0$ and (4)~$\dest(s,1,i)$ is the uniform distribution over $s$ and $0$. The game is illustrated in Figure \ref{fig:sqroot}.

Consider an optimal stationary strategy in $G$ for Eve. Let $p$ be the probability with which she plays the first action. If Adam knows that Eve will follow this strategy, the game devolves into a MDP. We know from that for such there exists optimal positional strategies and thus Adam is either going to play the left or right column always. Clearly, $0<p<1$ because $p=0$ or 1 means that either playing the left or right column with probability 1 would ensure that no positive reward ever happens.


\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]
\ma{s}[$s:$]{2}{2};

\node at (s-1-1.center) {$1^*$};
\node at (s-2-2.center) {$1^*$};
\node at (s-2-1.center) {$0^*$};
\draw (s-1-2.center)  to ($(s-1-2.center)+(0.5cm,0)$)  arc (-90:180:0.5cm) node[pos=.5, right] {$1/2$};
\node at ($(s-1-2.center)-(0.2cm,0)$) {0};

\ma[shift={($(s)+(3 cm,0.5)$)}]{s0}[]{1}{1};
\node at ($(s0-1-1.east)+(0.5cm,0)$) {$:0$};
\node at (s0-1-1.center) {$0^*$};

\draw (s-1-2.center) -- (s0) node[below,pos=0.5] {$1/2$};


\end{tikzpicture}
\caption{Concurrent discounted game with value $v_s=-2+\sqrt{4+2(1-\gamma)}$}\label{fig:sqroot}
\end{figure}





Let $v_0=0,v_1=1,v_s$ be the values of the three vertices. If he plays the left column, the outcome is $p(1-\gamma)$.
If he plays the right column, the outcome is $p/2(1-\gamma)v_s+(1-p)(1-\gamma)$. Observe that the former is increasing in $p$ and the latter is decreasing (since clearly, $0<(1-\gamma)v_s<v_s$). Also, both are continues. Thus, the optimum is for $p(1-\gamma)$ to be equal to $p/2(1-\gamma)v_s+(1-p)(1-\gamma)$ and both equal to $v_s$.
We will first isolate $v_s$ in $v_s=p/2(1-\gamma)v_s+(1-p)(1-\gamma)$.
\begin{align*}
v_s&=p/2(1-\gamma)v_s+(1-p)(1-\gamma)\Rightarrow (1-p/2(1-\gamma))v_s=(1-p)(1-\gamma)\Rightarrow \\
v_s&=\frac{(1-p)(1-\gamma)}{1-p/2(1-\gamma)}\enspace .
\end{align*}

Note that $p,\gamma<1$ thus, $1-p/2(1-\gamma)\neq 0$.
We then have the equality 
\begin{align*}
\frac{(1-p)(1-\gamma)}{1-p/2(1-\gamma)}&=p(1-\gamma)\Rightarrow\\(1-p)(1-\gamma)&=p(1-\gamma)(1-p/2(1-\gamma))\Rightarrow\\
0&=\frac{1-\gamma}{2} p^2+2p-1\Rightarrow \\
p&=\frac{-2\pm\sqrt{4+2(1-\gamma)}}{1-\gamma} \enspace .
\end{align*}

We see that $\frac{-2-\sqrt{4+2(1-\gamma)}}{1-\gamma}<0$. Thus, $p=\frac{-2+\sqrt{4+2(1-\gamma)}}{1-\gamma}$.
Also, \[v_s=-2+\sqrt{4+2(1-\gamma)}\enspace .\] 
It is straight-forward to modify the construction to get any square-root desired for a fixed $\gamma$. 



By making such a construction for each number $\sqrt{b_i}$, we can make another vertex $s^*$ that has the value of $(1-\gamma)\frac{\sum_{i=1}^n\sqrt{b_i}}{n}$ with a single action for each player and that goes to a uniformly random vertex. 
Observe that decreasing each reward by $x$, reduces the value of each vertex by $x$. Reduce each reward by $\frac{an}{1-\gamma}$.
We can then decide the sum-of-square-roots problem by deciding whether the value of $s^*$ is strictly above $0$. 

We get the following lemma.

\begin{lemma}
The (exact) decision problem for the value is sum-of-square-root hard for concurrent discounted games
\end{lemma}

We will use this game  $G$ as an example to illustrate how to make the $\dest$-function deterministic for concurrent games while having the same value and a similar optimal strategy.


\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]
\ma{s}[$s:$]{3}{3};

\node at (s-1-1.center) {$1^*$};
\node at (s-2-1.center) {$1^*$};
\node at (s-3-2.center) {$1^*$};
\node at (s-3-3.center) {$1^*$};
\node at (s-3-1.center) {$0^*$};
\node at (s-1-2.center) {0};
\node at (s-2-3.center) {0};
\node at (s-2-2.center) {$0^*$};
\node at (s-1-3.center) {$0^*$};

\end{tikzpicture}
\caption{Alternate concurrent discounted game with value $v_s=-2+\sqrt{4+2(1-\gamma)}$}\label{fig:sqroot2}
\end{figure}

Consider the following game $G'$:
There are three vertices, $\{0,1,s\}$ where $0$ and $1$ are absorbing, with color 0 and 1 respectively.
The vertex $s$ is such that (1)
$c(s,i,j)=0$ for all $i,j$, (2)~$\dest(s,i,j)=s$ for $i+1=j$ (i.e. for $(i,j)\in \{(1,2),(2,3)\}$), (3)~$\dest(s,i,j)=0$ for $i+j=4$ (i.e. the ``other'' diagonal, $(i,j)\in \{(3,1),(2,2),(1,3)\}$) and (4)~$\dest(s,i,j)=1$ otherwise (i.e. for $(i,j)\in \{(1,1),(2,1),(3,2),(3,3)\}$).
 The game is illustrated in Figure \ref{fig:sqroot2}.
 
We will argue that the value of $G'$ is equal to that of $G$.
 We clearly have that the value of $s$ is in $(0,1)$.
Consider a stationary strategy $\sigma$ for Eve
such that $\sigma(1)\neq \sigma(2)$. Let $p_i=\sigma(i)$ for $i\in \{1,2,3\}$. 
Let $\sigma'$ be such that $\sigma'(3)=p_3$ and otherwise, $\sigma'(i)=\frac{p_1+p_2}{2}$ for $i\in\{1,2\}$. Let $p_i'=\sigma'(i)$ for $i\in \{1,2,3\}$.
\begin{claim}
The strategy $\sigma'$ is at least as good as $\sigma$
\end{claim}
\begin{proof}
If Adam plays 1, then the expected outcome is 
$p_1+p_2=p'_1+p'_2$ no matter if Eve plays $\sigma$ or $\sigma'$. 
If he plays $i$ for $i\in\{2,3\}$, the expected outcome is $
\frac{p_{4-i}}{1-p_{i-1}}$ if Eve plays $\sigma$ and otherwise, if she plays $\sigma'$, the expected outcome is   
$\frac{p'_1}{1-p'_2}=\frac{p'_2}{1-p'_1}$. 
Note that $\frac{p'_1}{1-p'_2}>\min_{i\in\{2,3\}\frac{p_{4-i}}{1-p_{i-1}}}$ and thus, $\sigma'$ is at least as good a strategy as $\sigma$.
\end{proof}

A similar argument shows that for any strategy $\tau$ for Adam the similar strategy $\tau'$ where $\tau'(1)=\tau(1)$ and $\tau'(i)=\frac{\tau(2)+\tau(3)}{2}$ for $i\in\{2,3\}$ is at least as good as $\tau$.
Consider that the players follows such stationary strategies $\sigma'$ and $\tau'$.
Let $\sigma$ be 
\[\sigma(i)=\begin{cases} \sigma'(1)+\sigma'(2)&\text{if }i=1\\
\sigma'(3)&\text{if }i=2\enspace .\end{cases}\]
Similarly, let 
 $\tau$ be 
\[\tau(i)=\begin{cases} \tau'(1)&\text{if }i=1\\
\tau'(2)+\tau'(3)&\text{if }i=2\enspace .\end{cases}\]
But playing $\sigma$ and $\tau$ in $G$ gives the same outcome as playing $\sigma'$ and $\tau'$ in $G'$ as can be seen as follows: In either game, with probability
\[
\sigma'(1)\tau'(2)+\sigma'(2)\tau'(3)=\frac{\sigma(1)\tau(2)}{2}\] we play again with a reward of 0, with probability 
\[\sigma'(1)\tau'(3)+\sigma'(2)\tau'(2)+\sigma'(3)\tau'(1)=
\frac{\sigma(1)\tau(2)}{2}
+\sigma(2)\tau(1)
\]
we get absorbed in 0 after a reward of 0 and with probability \[
(\sigma'(1)+\sigma'(2))\tau'(1)+(\tau'(2)+\tau'(3))\sigma'(3)
\] we get absorbed in 1 after a reward of 0.
But this is in particular the case if the players play optimally and thus, the value is the same in the two games.



Before, in Corollary \ref{cor:long}, we argued that the patience of $\epsilon$-optimal stationary strategies was $q=\frac{m\log(\epsilon/2)}{\log(1-\gamma)\epsilon}$.
Giving a similar exponential bound for the optimal stationary strategies is harder than solving the sum-of-square-roots problem, as we will argue next.
Assume that we had an exponential bound for optimal stationary strategies.

\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]
\ma{s}[$s':$]{2}{2};

\node at (s-1-1.center) {$1^*$};
\node at (s-2-1.center) {$0^*$};
\node at (s-1-2.center) {$0^*$};

\ma[shift={($(s)+(3 cm,-0.5)$)}]{ss}[]{1}{1};
\node at ($(ss-1-1.east)+(0.5cm,0)$) {$:s^*$};
\node at ($(s-2-2.center)-(0.2cm,0)$) {0};


\draw [yscale=-1] (s-2-2.center)  to ($(s-2-2.center)+(0.5cm,0)$)  arc (-90:180:0.5cm);
\draw (s-2-2.center) -- (ss);


\end{tikzpicture}
\caption{Concurrent discounted game that implies that if there is an exponential lower bound on patience, then the sum-of-square-roots problem is in P}\label{fig:exact-hard}
\end{figure}

Consider an arbitrary yes-instance of the sum-of-square-roots problem, giving a vertex $s^*$. Reduce each reward by $a$ and in the new game let $s^*_a$ be the vertex corresponding to $s^*$. 
We will now create a game that uses the previous game as a sub-game.
The game has 1 additional vertex $s'$, which is a 2x2-matrix, such that $c(s',i,j)=0$ and $\dest(s,1,1)=1$ and $\dest(s,2,2)=s^*$ and $\dest(s,i,j)=0$ for $i\neq j$.
There is an illustration in Figure~\ref{fig:exact-hard}, using the vertex $s^*$ as above. 
Using an argument like above, we see that the probability $p$ to play the top action in the vertex $s'$ is such that $p(1-\gamma)=(1-p)(1-\gamma)x$, where $x$ is the value of $s^*$. Thus, $x=\frac{p}{1-p}$. If $p$ only needs to be exponential small, then $x$ is exponentially small as well. This is true for any yes-instance of the sum-of-square-roots problem and thus, we only need polynomially many digits to decide the problem. We can find polynomially many digits of $\sqrt{b_i}$ for each $i$ in polynomial time. We get the following lemma.

\begin{lemma}
Giving an exponential lower-bound on patience for optimal stationary strategies in concurrent discounted games implies that the sum-of-square-roots problem is in P
\end{lemma}





\section{Concurrent reachability games}
In this section we consider concurrent reachability games. 
Intuitively, unlike concurrent discounted games, these games cares only about the final part of the play.
This, while perhaps not clear directly from the definitions, makes the games somewhat harder. For instance, the value iteration algorithm requires double-exponential time.
Note that like for concurrent discounted games, if we force Eve to follow some strategy, the game reduces to a MDP and there exists optimal positional strategies.

We will not prove the following known lemma. We will however, show the weaker statement that the decision problem for the value can be done in PSPACE.

\begin{lemma}\label{lemm:reach_determined}
Concurrent reachability games are determined. Also, finding the value of a concurrent reachability game can be done in TFNP[NP]
\end{lemma}


We will argue that there might not be optimal strategies for Eve in concurrent reachability games. 
The game we will use will later be a member of a family of games that requires high patience to play well.

The snowball game (or purgatory 1) is defined as follows:
There are 3 vertices, the goal vertex $\Win$, $\bot$ (an absorbing vertex) and a vertex 1, which has a 2x2 matrix, such that $\dest(x,r,c)$ is a dirac distribution over (1)
$\Win$ for $r=c$, (2) $1$ for $r<c$ and (3) $\bot$ for $r>c$.
When we illustrate the game, we write view the goal vertex $\Win$ as being an absorbing vertex with color 1.
There is an illustration of the snowball game in \ref{fig:snowball}. 

\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]
\ma{s}[$1:$]{2}{2};

\node at (s-1-1.center) {$1^*$};
\node at (s-2-1.center) {$0^*$};
\node at (s-1-2.center) {$0$};
\node at (s-2-2.center) {$1^*$};


\end{tikzpicture}
\caption{The snowball game or purgatory 1, in which no optimal strategy exists for Eve}\label{fig:snowball}
\end{figure}


For any $\epsilon>0$, consider the stationary strategy for Eve that plays the first action with probability $1-\epsilon$. 
If Adam plays the left column always, play will reach $\Win$ with pr. $1-\epsilon$. If Adam plays the right column always, play reaches $\Win$ with pr. 1. Hence, the strategy for Eve is $\epsilon$-optimal and the value of the vertex is 1.

On the other hand, if Adam plays the right column whenever Eve plays the first action with pr. 1, and otherwise plays the first action, 
then in the last round in the vertex there must be a positive pr. that play goes to vertex 0. Hence, Eve has no optimal strategy.


\begin{lemma}\label{lemm:no_opt_reach}
Eve need not have an optimal strategy in a concurrent reachability game
\end{lemma}

The following lemma states some classical results for concurrent reachability games that we will not prove.
\begin{lemma}\label{lemm:reach_class}
\begin{itemize}
\item For any $\epsilon>0$, there are always $\epsilon$-optimal stationary strategies for Eve and optimal stationary strategies for Adam. 
\item The value iteration algorithm converges to the optimal value and is defined exactly like for concurrent discounted games, except that $\gamma=0$ and the target vertex has value 1 in the first iteration
\item The values $v$ are the least fix-point (i.e., every other fix-point $v'$ is such that for all $i$ $v_i\leq v'$) of the value iteration operator $\ValueOp$

\end{itemize}
\end{lemma}
(Note that the games are not symmetric, in that Eve tries to reach a node and Adam tries to stay away from it, and in particular, even though Eve need not have an optimal strategy in a concurrent reachability game, Adam always has one)






The decision problem for the existential first order theory over the reals is the following decision problem:
Given a function $F:\R^n\rightarrow \{\text{true},\text{false}\}$, is there an vector $v$ such that $F(v)$ is true?
The function $F$ must be an well-formed (i.e. connected with logical ``and'', ``or'' and ``not'') quantifier-free formula over polynomial inequalities.
E.g. $x^2y+z\geq 5\wedge \neg (xz\leq 3)$ would be such a function.

\begin{lemma}
The decision problem for the existential theory over the reals is in PSPACE
\end{lemma} 

We will now, given a number $c$, encode the problem whether the value in a concurrent reachability game is  $<c$, starting from some vertex $x$.
The idea is that we can describe a fix-point of the value iteration operator, i.e.
we can describe that $v_i=\Value[M^i(v)]_i$. Since we know that the values are the least fix-point, we can then just add the condition that $v_x<c$. 
We can describe the value of a matrix game by guessing a strategy for each player, $\sigma$ for Eve and $\tau$ for Adam, and then checking that these strategies are optimal by showing that the outcome obtained by following Eve's strategy is equal to what is obtained by following Adam's.
I.e. we check that for Eve's strategy, the least outcome obtained when Adam plays any given column is $v_i$ and similar for Adam.
We describe that as $v_i=\min_{a\in [m]} (\sigma M^i_{*,a}(v))$ and $v_i=\max_{a\in [m]} (\tau M^i_{a,*}(v))$, where $M^i_{*,j}(v)$ is the $j$-th column of $M^i(v)$ and $M^i_{j,*}(v)$ is the $j$-th row for all $j$. 
We can express that $x=\min(a_1,a_2,\dots,a_n)$ for any number $n$ and any polynomials $a_1,a_2,a_n$, in the first order theory of the reals by stating that \[
x\leq a_1\wedge x\leq a_2\wedge \dots \wedge x\leq a_n \wedge (x=a_1\vee x=a_2\vee \dots \vee x=a_n) \enspace .\]
Similarly, one can also describe that $x=\max(a_1,a_2,\dots,a_n)$ for any number $n$ and any polynomials $a_1,a_2,a_n$.

We can similarly make a statement that $v_x\leq c$. Using that PSPACE is equal to co-PSPACE, we also get that we can find if $v_x\geq c$ and $v_x>c$ and therefore also $v_x=c$ and $v_x\neq c$, all in PSPACE.

\begin{lemma}
Decision problems for the values in a concurrent reachability game is in PSPACE
\end{lemma}

The set of vertices that have value 0 can be found in polynomial time. This is because, the set of vertices that have value 0 in the time limited game of length $n$ has also value 0 in all other time limited games. That this is so is easy to see by considering that Eve plays an $\epsilon$-optimal strategy for $v>\epsilon$, where $v$ is the value of the vertex with the lowest value. The game then devolves to a MDP for Adam, and the statement is true for such.

\begin{lemma}\label{lemm:find_0_reach}
The set of vertices of value 0 in a concurrent reachability game can be found in polynomial time
\end{lemma}

Next, we will also argue that we can find the set of vertices $S_1$ that have value 1 in polynomial time as well.
For notational convenience, for stationary strategies $\sigma,\tau$ we will, for a set $x$ and a set of vertices $S$ use \[
F^{\sigma,\tau}(x,S):=\sum_{r\in A_1}\sum_{c\in A_2}\sum_{x'\in S}\sigma(x)(r)\tau(x)(c)\delta(x,r,c)(x')\enspace ,\]
 i.e. the probability when the players follows $\sigma,\tau$ to go from $x$ directly to a vertex in $S$.

For a set $S$, containing $\Win$ and a non-empty subset $S'\subseteq(S\setminus \{\Win\})$ and a vertex $x\in S'$, let the {\em subset property} be the following:
For each $\epsilon>0$, there is a strategy $\sigma$ for Eve, such that for any strategy $\tau$ for Adam, $F^{\sigma,\tau}(x,S\setminus S')\cdot \epsilon >F^{\sigma,\tau}(x,V\setminus S)$.

For a set $S$, containing $\Win$, let the {\em value-1-property} be that the subset property is satisfied for each $S'\subseteq(S\setminus \{\Win\})$ (with some $x\in S'$).
For a set $S$ satisfying the value-1-property, we will define some subsets.
Let $S^0$ be the set consisting of $\Win$.
Let $S^i$, for each $i\geq 1$ be the set of vertices such that for each $x\in S^i$,
the vertex $x$ satisfies the subset property for $S'$ and $S=\bigcup_{j=0}^{i-1}S^j$.
Let $\ell$ be the largest number such that $S^\ell$ is non-empty (note that $S^i$, for $i>\ell$ must then be empty).


We will be using the following lemma.



\begin{lemma}
The set of vertices $S_1$ of value 1 satisfies the value-1-property
\end{lemma}
\begin{proof}
The proof is by contradiction. Thus, there is an $S$ (not containing $\Win$) such that $S_1$ and $S$ does not satisfy the subset property for any $x\in S$. I.e. for some constant $\epsilon>0$,
$F^{\sigma,\tau}(x,S_1\setminus S)\epsilon \leq F^{\sigma,\tau}(x,V\setminus S_1)$ for any strategy $\sigma$ for Eve and some strategy $\tau_{\sigma}$ for Adam and for every $x\in S$.
But then, all vertices in $S$ have value $\leq (1-\epsilon)+\epsilon V_{\max}$ where $v_{\max}<1$ is the largest value of a vertex in $V\setminus S_1$.
This is because to get to $\Win$ from $S$, it must leave $S$ and in that step, the probability to go to a vertex in  $V\setminus S_1$ (from which one cannot obtain more than $v_{\max}$) is at least the constant $\epsilon$.  
\end{proof}

We will argue that this is a precise characterization of $S_1$ next.

\begin{lemma}
Consider a set $S'$ satisfying the value-1-property.
Then each vertex of $S'$ has value 1.\label{lem:sufficent_for_value1}
\end{lemma}
\begin{proof}
We will for all $i$ and any $\epsilon>0$ construct a strategy $\sigma_{i,\epsilon}$ for Eve that starting from a vertex in $\bigcup_{j=i}^n S^j$ will eventually get to $S^{i-1}$ with probability at least $1-\epsilon$ (especially, the strategy $\sigma_{1,\epsilon}$ is $\epsilon$-optimal). We will do it using backwards induction in $i$ and thus start from $i=\ell$.

Note that the base case for $i=\ell$ follows directly from the condition.
We now for any $\epsilon>0$ will find a strategy $\sigma_{i,\epsilon}$, given that we have a strategy $\sigma_{i+1,\epsilon'}$ for any $\epsilon'>0$.
The idea is that the subset property gives a strategy $\sigma$ such that for all $\tau$, $F^{\sigma,\tau}(x,S'\setminus S)\epsilon/2 >F^{\sigma,\tau}(x,V\setminus S')$, for $S=\bigcup_{j=i}^{n} S^j$.
Note that in expectation, following this strategy, we go to some other vertex in $S$ with at least some fixed probability $p$ (that could be quite close to 1).
Hence, in expectation, we need to be in such a vertex $1/(1-p)$ times before entering either $S'\setminus S$ or $V\setminus S'$.
We therefore follow the strategy $\sigma_{i+1,\epsilon'}$ in $\bigcup_{j=i+1}^n S^j$, where $\epsilon'=\frac{\epsilon}{2(1-p)}$.
The inductive construction then follows by applying union bound over the $1/(1-p)$ times we are in $S^i$.
\end{proof}
Note that the lemmas together shows that $S_1$ is the largest set satisfying value-1-property.


Our algorithm, \crgLim, for finding $S_1$ is then as follows:
Assign to each vertex $x$ a rank $\rk_x$, a value in $0,1,\dots,n,\bot$, starting with $\rk_x=0$.
Let $\overline{S}^i$ be the set of vertices of rank $i$.
Let $\overline{S}_1=V\setminus S^{\bot}$.
We increment (where a rank is less than another if it is a smaller number. Also, all other ranks are below $\bot$) the rank of a non-goal vertex $x\in \overline{S}^i$, whenever it does not satisfy the subset property for $S=\overline{S}_1$ and $S'=\bigcup_{j=i}^n \overline{S}^j$.
Note that no vertex can satisfy the subset property for rank 0, since $S'$ is all vertices.
Whenever a stable configuration is reached, output $\overline{S}_1$.


\begin{lemma}
The output of the \crgLim\ algorithm is correct
\end{lemma}
\begin{proof}
The idea is that we want $S^i=\overline{S}^i$ at termination.
Note that the subset property is harder to satisfy for a vertex $x$ if we remove vertices from $S$ or from $(S\setminus S')$.
Thus, if at some time we have that $x$ does not satisfy the subset property for $S=\overline{S}_1$ and $S'=\bigcup_{j=i}^n \overline{S}^j$, then it does not do so for $S$ being any subset of $\overline{S}_1$ or $(S\setminus S')$ being any subset of $\bigcup_{j=0}^{i-1} \overline{S}^j$.
However, initially $S_1\supseteq\overline{S}_1$ (being all vertices) and $\bigcup_{j=i}^n S^j\supseteq \bigcup_{j=0}^{i-1} \overline{S}^j$ for all $i$. But we must have that $S_1\supseteq\overline{S}_1$ and $\bigcup_{j=i}^n S^j\supseteq \bigcup_{j=0}^{i-1} \overline{S}^j$ for all $i$, at all latter points as well, since in the last iteration it was satisfied, for all $i$ and all $x\in S^i$, we have that the subset property is satisfied for $S=\overline{S}_1$ and $S'=\bigcup_{j=i}^n \overline{S}^j$, because we have that $S\supseteq S_1$ and $(S\setminus S')=\bigcup_{j=1}^{i-1} \overline{S}^j\supseteq \bigcup_{j=1}^{i-1} S^j$.
On the other hand, eventually no vertex gets it rank incremented (since there are a finite number of ranks and vertices) and the algorithm terminates with a set $\overline{S}_1\supseteq S_1$ satisfying the value-1-property. Since $S_1$ is the largest such set, we have that $\overline{S}_1=S_1$.
\end{proof}


We will now consider the running time of the algorithm.
We will consider that we can decide whether a pair of sets $S$ and $S'$ satisfies the subset property for a vertex $x$ can be solved in $O(k)$ time.
Let $S_x$ be the set of vertices that can be visited in a play immediately after $x$. Observe that $|S_x|$ is a lower bound on the number of arrows from matrix $x$ in our illustration of the game.
Consider a vertex $x$ and a rank $i$, and we want to find an upper bound on the computation we do on $x$ while it has rank $i$.
Clearly, we only need to consider incrementing the rank of $x$ whenever a vertex in $S_x$ has changed its value and only if it is changed to either $i$ (from $i-1$) or to $\bot$ (from $n$), because otherwise, the sets $S$ and $S'$ have not changed. 
Thus, we only do at most $2|S_x|+1$ checks whether $x$ satisfies the subset property.
There are $n+1$ ranks, so in total for $x$, we use at most $(n+1)(2|S_x|+1)$ checks.
Hence, in total over all $x$, we do $O(n\sum_{x} |S_x|)$ checks.

\begin{lemma}
The run time of the \crgLim\ algorithm is $O(nk\sum_{x} |S_x|)$ times 
\end{lemma}

We will then finally consider how to check whether $x$ satisfies the subset property for a pair of sets $S$ and $S'$.
We will do so by constructing a strategy $\sigma=\sigma(\epsilon)$ for Eve satisfying the property for any fixed $\epsilon>0$.
We will construct the strategy  $\sigma$ from some sequence of pairs of sets (of rows and columns) $(R_1,C_1),(R_2,C_2),\dots,(R_{\ell},C_{\ell})$. We will let $C_i^*=\bigcup_{j=1}^i C_j$ and similar for $R^*_i$.
For convenience, we also define $C_0^*$ as the empty set of columns.
We will define $R_i$ from $C_{i-1}^*$ as each row $r\not\in R_{i-1}^*$ such that, for all $c\not\in C^*_{i-1}$, we have that $F^{r,c}(x,V\setminus S)=0$.
We will define $C_i\not\in C_{i=1}^*$ from $R_i$ as each column $c$ such that there is a $r\in R_i$ such that 
$F^{r,c}(x,S\setminus S')>0$.
The set $R^{*}_{\ell}$ is the first set such that $R^*_{\ell+1}$ is empty (clearly, by construction all sets $R_i,C_i$ for $i>\ell$ would also be empty).

\begin{lemma}
There is a strategy $\sigma(\epsilon)$ for all $\epsilon>0$ iff $C_{\ell}^*$ is the set of all columns
\end{lemma}
\begin{proof}
We will first argue that if $C_{\ell}^{*}$ is not all columns $C$, then the strategy $\tau$ that plays uniformly over $C'=(C\setminus C_{\ell}^*)$ shows that no strategy $\sigma(\epsilon)$ exists for small enough $\epsilon>0$. This is because any row $r$ such that $F^{r,c}(x,S\setminus S')>0$ for some $c\in C'$ is also such that $F^{r,c'}(x,V\setminus S)>0$ for some column $c'\in C'$. This is because otherwise $r$ would be in $R^i$ for some $i$ and then $c$ would be in $C_{\ell}^*$. Hence, the probability $F^{r,c'}(x,V\setminus S)$ cannot be more than a constant factor smaller than $F^{r,c}(x,S\setminus S')$.


Otherwise, if $C_{\ell}^*=C$, then let $\sigma(\epsilon)$ be the strategy that picks an $i$ from the distribution $\dist$ and then plays an action in $R^i$ uniformly at random. The distribution $\dist$ is such that for all $j\in \{1,\dots,\ell-1\}$ we have $\Pr^{\dist}[i=j]\epsilon\delta_{\min}/m=\Pr^{\dist}[i>j]$. 

To argue that $\sigma=\sigma(\epsilon)$ satisfies the subset property for $x,S,S'$
consider each column $c$. We have that $c\in C^j$ for some $j$. 
Let $p$ be the pr. with which a row in $R^j$ is played by $\sigma$ and thus, $F^{\sigma,c}(x,S\setminus S')\geq p\delta_{\min}>0$.
Any row $r$ such that $F^{r,c}(x,V\setminus S)>0$ must be outside $R^*_j$ by construction (and if such a row exists $j<\ell$). 
We play such rows with pr. $\leq \Pr^{\dist}[i>j]$ and thus $\Pr^{\dist}[i>j]\geq F^{\sigma,c}(x,V\setminus S)$.
We have that $pm>\Pr^{\dist}[i=j]$ (strict because $j<\ell$) and thus, \[
F^{\sigma,c}(x,S\setminus S')\epsilon\geq p\delta_{\min}\epsilon>\delta_{\min}\epsilon/m\Pr^{\dist}\nolimits[i=j]\geq 
\Pr^{\dist}\nolimits[i>j]\geq F^{\sigma,c}(x,V\setminus S) \enspace .\]
This completes the proof of the lemma.\end{proof}


Our algorithm for checking if a vertex $x$ satisfies the subset property for sets $S,S'$ is as follows:
We will construct the sequence of sets $(R_1,C_1),(R_2,C_2),\dots,(R_{\ell},C_{\ell})$.
To do so we will use a datastructure.
The datastructure has the following properties:
Initially, for each column $c$, we will make a list $L_c$ of the rows $r$ such that $F^{r,c}(x,V\setminus S)>0$.
We will also have a counter for each row $r$ that initially contains how many such columns there are.
Finally for each row $r$, there is a list $L_r$ of columns such that $F^{r,c}(x,S\setminus S')>0$.

The algorithm then uses the datastructure as follows:
Let $i\leftarrow 1$.
Add all the rows with the counter at 0 to $R^i$. 
If $R^i$ is the empty set, return whether $C_{i-1}^*$ is all columns.
For each row $r$ in $R^i$ go through $c\in L_r$ and subtract 1 from the counter of each row in $L_c$. If a counter reach 0, add it to $R^{i+1}$.
Increment $i$.
Go to line 3.

The total time is $O(\sum_{r,c}|\supp(\dest(x,r,c))|)$ for the algorithm.

\begin{lemma}
We can check whether a vertex $x$ satisfies the subset property for sets $S$ and $S'$ in time \[O(\sum_{r,c}|\supp(\dest(x,r,c))|)\]
\end{lemma}

We therefore get that \begin{lemma}\label{lem:val1}\label{lemm:find_1_reach}
We can find the set of vertices of value 1 in time $O(n\sum_{x} |S_x|\sum_{r,c}|\supp(\dest(x,r,c))|)$
\end{lemma}

Next, we will give a lower bound for patience, i.e. that in some games, the patience for every $\epsilon$-optimal stationary strategy must be high. 
For a number $k$, let purgatory $k$ be the following game:
There are $2+k$ vertices, $\Win$ (which is vertex 0), one vertex $\bot$ which is absorbing and each other vertex $i\in \{1,\dots, k\}$ has a 2x2 matrix, such that $\dest(x,r,c)$ is a dirac distribution over (1)
$i-1$ for $r=c$, (2) $k$ for $r<c$ and (3) $\bot$ for $r>c$.
There is an illustration of Purgatory $4$ in Figure \ref{fig:purgatory}. 

\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]

\ma{s2}[$3:$]{2}{2};
\ma[shift={($(s2)+(0,3cm)$)}]{s1}[$4:$]{2}{2};
\ma[shift={($(s2)+(3cm,0)$)}]{s3}[$2:$]{2}{2};
\ma[shift={($(s3)+(3cm,0)$)}]{s4}[$1:$]{2}{2};

\draw (s2-1-2.center) to (s1);
\draw (s3-1-2.center) to[bend right] (s1);
\draw (s4-1-2.center) to[bend right] (s1);

\foreach \x/\y in {2/3,3/4} {
\draw (s\x-2-2.center) to (s\y);
\draw (s\x-1-1.center) to[out=70] (s\y);
}
\foreach \x in {1,2,3,4} \node at (s\x-2-1.center) {$0^*$};
\draw (s1-2-2.center) to (s2-1-2);
\draw (s1-1-1.center) to[out=200] (s2-1-1);
%\foreach \x in {1,2,3} \node at ($(s\x-2-2.center)-(0.2cm,0)$) {$0$};
%\foreach \x in {2,3,4} \node at ($(s\x-1-2.center)-(0.2cm,0)$) {$0$};
%\foreach \x in {2,3} \node at ($(s\x-1-1.center)-(0.2cm,0)$) {$0$};

%\node at ($(s1-1-1.center)+(0.2cm,0)$) {$0$};
\niceloop{s1-1-2};

\node at (s4-1-1.center) {$1^*$};
\node at (s4-2-2.center) {$1^*$};

\end{tikzpicture}
\caption{Purgatory $4$. For clarity, the colors are omitted, except that $0^*$ corresponds to an edge to an absorbing vertex different from $\Win$ and $1^*$ corresponds to an edge to $\Win$}\label{fig:purgatory}
\end{figure}


It is easy to see that all vertices but $\bot$ is in $S_1$, using the value 1 property.

\begin{lemma}\label{lem:purgatory}
For any $0<\epsilon<1/2$ and any $k\geq 1$, there is a unique strategy for Eve with least patience which is $\epsilon$-optimal in purgatory $k$. That strategy has patience $\epsilon^{-2^{k-1}}$
\end{lemma}
\begin{proof}
We will find the best strategy with patience $1/p$ for any number $0<p<1/2$.
It is clear that the best strategy with patience $1/p$ is to play the strategy that maximizes the pr. of eventually reaching $i$ from vertex $j$ for all $j>i$ while having patience $1/p$.
Let $x_k=1-p$ and let $x_i=1-\sqrt{1-x_{i+1}}$. We will argue that $x_i$ is the probability of eventually reaching vertex $i-1$ from vertex $k$ for all $i\in \{1,\dots,k\}$ and that there is a unique best strategy with patience $1/p$.

The unique best strategy in vertex $k$ is to play the top action with pr. $1-p$ and the bottom action with pr. $p$. This ensures that the pr. $x_k$ of reaching $k-1$ from $k$ is $1-p$ if Adam plays the left column.

Consider now vertex $i\in \{1,\dots,k-1\}$.
For the purpose of finding good strategies in $i$, we can view vertex $i$, when Eve plays her best strategy in $j>i$ and Adam plays a best response, as a smaller reachability game with 3 vertices, i.e. $i-1$ (as $\Win$), $\bot$ and $i$, where $\dest(i,r,c)$ is (1) a dirac distribution over $i-1$ for $r=c$, (2) a dirac distribution over $\bot$ for $r>c$ and (3) a distribution that goes to $\bot$ with pr. $1-x_{i-1}$ and to $i$ with the remaining pr. for $r<c$. See Figure \ref{fig:1purgatory}


Let $p_i$ be the probability with which a strategy $\sigma$ plays the top row in vertex $i$.
We can then consider the game as a MDP, since we have fixed a stationary strategy for one of the players.
It is clear that the pr. to reach $i-1$ from $i$ if Adam plays the right column is strictly increasing in $p_i$ and the pr. to reach $i-1$ from $i$ if Adam plays the left column is strictly decreasing in $p_i$. We will consider the strategy such that the pr. of reaching $i-1$ is equal no matter which column Adam plays (by the previous statement, this strategy must then be optimal).
Observe that the pr. of reaching $i-1$ if Adam always plays the left column is $p_i$ (which is then also the pr. to reach $i-1$ from $i$) and if he always plays the right column it is  $p_i x_i p_i+(1-p_i)$ (using that the pr. of reaching $i-1$ from $i$ is $p_i$).
Thus, we have that 
\[
p_i=p_i x_i p_i+(1-p_i)\Rightarrow 0=p_i^2/2 x_i-p_i+1/2 \Rightarrow p_i=\frac{1\pm \sqrt{1-x_i}}{x_i}\enspace .\] We see that $\frac{1+ \sqrt{1-x_i}}{x_i}>1$ and thus the solution is $p_i=\frac{1- \sqrt{1-x_i}}{x_i}$ or that $(x_{i-1}=)x_ip_i=1- \sqrt{1-x_i}$.

Consider now that the strategy is exactly $\epsilon$-optimal, implying that $x_1=1-\epsilon$. 
We will argue that $p=\epsilon^{2^{k-1}}$.
We will do so by arguing  using induction in $i$ that $x_i=1-\epsilon^{2^{k-1}}$, since $x_k=1-p$ (this also shows that the strategy is indeed using patience $1/p$ since the probabilities in the other vertices, which are $p_i=\frac{x_{i-1}}{x_i}$, are strictly above $1/p$).
We have already noted that  $x_1=1-\epsilon=1-\epsilon^{2^0}$.
We will next argue that $x_i=1-\epsilon^{2^{k-1}}$, for $i\geq 2$ using that $x_{i-1}=1-\epsilon^{2^{k-2}}$.
We have that \[
1-\epsilon^{2^{k-2}}=x_{i-1}=1-\sqrt{1-x_{i}}\Rightarrow \sqrt{1-x_{i}}= \epsilon^{2^{k-2}}\Rightarrow
1-\epsilon^{2^{k-1}}=x_i\enspace .
\]
This completes the proof of the lemma.
\end{proof}

Concurrent reachability games are not symmetric in the players. E.g. Adam always have an optimal strategy but Eve might not. We will next argue that Adam still requires double exponential patience to play well.

Consider the following game called purgatory duel $k$ which can be viewed as a symmetric version of purgatory $k$.
There are $3+2k$ vertices, $\Win$ (which is vertex 0), one vertex $\bot$ which is absorbing (and is also vertex $0'$), and the start vertex $s$ and each other vertex $\{1,\dots, k,1',\dots,k'\}$ has a 2x2 matrix. Each vertex $x\in \{1,\dots, k\}$ is such that $\dest(x,r,c)$ is a dirac distribution over (1) $x-1$ for $r=c$, (2) $s$ for $r<c$ and (3) $\bot$ for $r>c$.
 Each vertex $x'\in \{1',\dots, k'\}$ is such that $\dest(x',r,c)$ is a dirac distribution over (1) $x-1'$ for $r=c$, (2) $s$ for $r<c$ and (3) $\Win$ for $r>c$. The start vertex is 1x1 matrix and is such that $\dest(s,r,c)$ is a uniform distribution over $k$ and $k'$.
There is a illustration of Purgatory Duel $2$ in Figure \ref{fig:purgatoryduel}.


\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]

\ma[shift={($(4.5cm,4.5cm)$)}]{s}[$s:$]{1}{1};
\ma[shift={($(3cm,0)$)}]{s2'}[$2':$]{2}{2};
\ma[shift={($(s2')+(3cm,0)$)}]{s1'}[$1':$]{2}{2};
\ma[shift={($(0,3cm)$)}]{s2}[$2:$]{2}{2};
\ma[shift={($(s2)+(0,3cm)$)}]{s1}[$1':$]{2}{2};


\foreach \x in {1,2,1',2'} \draw (s\x-1-2.center) to (s);
\foreach \x/\y in {1/0,2/0,1'/1,2'/1} \node at (s\x-2-1.center) {$\y ^*$};
\foreach \x/\y in {1/1,1'/0} \foreach \z in {1,2} \node at (s\x-\z-\z.center) {$\y ^*$};
\draw (s2-1-1.center) to (s1);
\draw (s2-2-2.center) to[out=30,in=-30] (s1);
\draw (s2'-2-2.center) to (s1');
\draw (s2'-1-1.center) to[out=60,in=120] (s1');
\draw (s.center) to[out=225,in=100] node[pos=0.6,right] {$1/2$} (s2');
\draw (s.center) to[out=225,in=-10] node[pos=0.6,above] {$1/2$} (s2) ;


\end{tikzpicture}
\caption{Purgatory duel $2$. For clarity, the colors are omitted, except that $0^*$ corresponds to an edge to an absorbing vertex different from $\Win$ and $1^*$ corresponds to an edge to $\Win$}\label{fig:purgatoryduel}
\end{figure}

We will say that a strategy $\sigma$ for Eve mirrors a strategy $\tau$ for Adam, if $\sigma(i)=\tau(i')$ and $\sigma(i')=\tau(i)$ for all $i$.
Similarly, $\tau$ mirrors $\sigma$.

\begin{lemma}
The value of vertex $s$ is $1/2$. Also, for any $\epsilon>0$, any $(1/2-\epsilon)$-optimal strategy $\tau$ for Adam does not follow a dirac distribution in $i'$ for any $i'\in\{1',\dots,k'\}$. Finally, every $\epsilon$-optimal strategy is a mirror of an $\epsilon$-optimal strategy
\end{lemma}
\begin{proof}
First, the value of vertex $s$ is at most $1/2$. This is because Adam can mirror any strategy $\sigma$ for Eve. This ensures that any play reaching $\Win$ is mirrored by an equally likely play reaching $\bot$. Thus, then the players follows these strategies, the pr. to reach $\Win$ is equal to the pr. to reach $\bot$ (there might also be some positive pr. to not reach neither, but Adam also wins those plays). 

Fix $\epsilon>0$ and consider a strategy $\tau$ for Eve that plays a dirac distribution in $i'\in\{1',\dots,k'\}$. Then $\tau$ is not $(1/2-\epsilon)$-optimal. We can see that as follows: Let $\sigma$ be the strategy for Eve that plays $r=1$ when in $j'\in \{(i+1)',\dots,k'\}$ and the action which is not equal to $\tau(i')$ when in $i'$. This ensures that the play will always reach either $\Win$ or $s$ from $k'$. But then  Eve can play an $(\epsilon/2)$-optimal strategy for purgatory $k$ in $\{1,\dots,k\}$, ensuring that $\Win$ is reached with pr. at least $1-\epsilon/2$. But then $\tau$ is not $(1/2-\epsilon)$-optimal.

Consider that Adam is following a strategy $\tau$ that is not playing a dirac distribution in $i'\in\{1',\dots,k'\}$ and Eve is playing an arbitrary strategy $\sigma$. Then, eventually play reaches $\Win$ or $\bot$ with pr. 1,  because in every $k+1$ steps, either $s$ is visited or either $\Win$ or $\bot$ is reached, and after $s$ has been visited, $k'$ is next half the time and from $k'$ $\bot$ is reached with positive pr.

Consider an $\epsilon$-optimal strategy $\tau$ for Adam, for $\epsilon<1/2$. 
Then, let Eve's mirror strategy to $\tau$ be $\sigma_{\tau}$. Now, either $\Win$ or $\bot$ is reached and because the strategies mirrors each other, the pr. to reach $\Win$ is equal to that of reaching $\bot$. Thus, we see that the value is at most $1/2$, implying that it is exactly $1/2$.

It also follows that the strategies that are $\epsilon$-optimal mirrors each other.
\end{proof}



We will now argue that Eve's (and thus Adam's) $\frac{1}{4}$-optimal strategies requires high patience.

To do so we will use the following lemma, showing that you can sometimes modify a concurrent game (or any of its special cases) and get a game with less value. While the proof is explicitly for concurrent reachability games, the proof is basically identical for concurrent discounted and mean-payoff games.
In a game $G$, for a vertex $v$ and a duration $T$, let $v^T_G$ be the value of the time-limited game with duration $T$.

\begin{lemma}\label{lem:change_succ}
Consider a concurrent reachability game $G$ and a pair of vertices $u,v$, such that for all $T$, we have that $u^T_G\geq v^T_G$. Consider a vertex $w$ such that for a pair of actions, $(r,c)$ we have that $v\in \supp(\dest(w,r,c))$. Consider an alternate game $G'$ equal to $G$, except that some of the probability mass is moved from $v$ to $u$ when playing $(r,c)$ in $w$, i.e. $0<\dest(G,w,r,c)(v)-\dest(G',w,r,c)(v)=\dest(G',w,r,c)(u)-\dest(G,w,r,c)(u)$.
Then for all vertices $z$ we have that $z^T_G\leq z^T_{G'}$
\end{lemma}
\begin{proof}
The proof is by induction in $T$.
The proof is trivial for $T=0$, because $z^T_G=0= z^T_{G'}$ for all non-goal vertices (and the goal vertex $\Win$ has value 1).
For $T\geq 1$, we have that $z^{T-1}_G\leq z^{T-1}_{G'}$.
But, matrix games are monotone in their entries, so it follows directly that for $z\neq w$ we have that $z^{T}_G\leq z^{T}_{G'}$.
Consider the matrix for $w^T_G$ compared to $w^T_{G'}$. All entries but the one for $(r,c)$ are smaller directly by induction. We also have that $v^T_{G}\leq u^T_{G}\leq u^T_{G'}$, the first inequality by definition and the second by induction. We thus see that all entries in $w^T_{G}$ are smaller than in $w^T_{G'}$.
The lemma follows.
\end{proof}

We are now ready to find the patience in concurrent reachability games.

\begin{lemma}
Any $(1/4)$-optimal strategy, for either player, in purgatory duel $k$ has patience at least $(3/4)^{-2^{k-1}}$ for each $k$
\end{lemma}
\begin{proof}
We will show that the lemma is true for Eve's strategies and that it is true for Adam's follows from Lemma \ref{lem:change_succ}.
Consider an $(1/4)$-optimal strategy $\sigma$ for Eve. Fixing this strategy for Eve, we get a MDP for Adam. Clearly, in this MDP $G'$, we have that $0=\bot^T_{G'}\leq s^T_{G'}$ for all $T$. We can thus apply Lemma \ref{lem:change_succ} to changing $\dest(1',1,1)$ from $\bot$ to $s$. In the resulting game $G''$, 
we still have that $0=\bot^T_{G''}\leq s^T_{G''}$ and thus, we can change $\dest(1',2,2)$ from $\bot$ to $s$.
Let the next game be $G^*$.
Thus, for any $i'\in \{1',\dots,k'\}$, the plays from $i'$ to $\bot$ in $G^*$ all goes through $s$. Note that Adam can ensure that the play reaches $s$ from $i'$ and thus, when he plays optimally, do so.
Thus, whenever $s$ is entered and Adam plays optimally, $k$ is enter eventually with pr. 1.
For the purpose of the value, we can thus disregard $s$ and vertices in $\{1',\dots,k'\}$ and just view each edge going to $s$ as going to $k$ instead.
But the resulting game is purgatory $k$ (in which Eve has fixed his strategy) and Eve is playing a strategy that gives value at least $1/4$, which requires  at least $(3/4)^{-2^{k-1}}$ patience, by Lemma \ref{lem:purgatory}.
\end{proof}





\section{Concurrent mean-payoff games}
In this section we consider concurrent mean-payoff games. 
We will show that in general, any  $\epsilon$-optimal strategy in some concurrent mean-payoff games are quite complex. 
We will first, however, show that finding the value of a concurrent mean-payoff game can be done in polynomial space.

\begin{lemma}\label{lemm:class_meanpayoff}
Concurrent mean-payoff games are determined and the value is the limit of the value of the corresponding time-limited game as well as the limit of the corresponding discounted game, for the discount factor going to 0 from above.
There is an polynomial time algorithm, ala Lemma~\ref{lem:val1}, for finding the set of vertices where a finite memory strategy suffice to ensure $1-\epsilon$ (recall that all rewards are in $\{0,1\}$).
For any fixed number $n$, there is a polynomial time algorithm for approximating the value in a concurrent mean-payoff game with $n$ vertices (i.e. the running time is polynomial in the number of actions)
\end{lemma}
We will not show this lemma, but simply note that the $\epsilon$-optimal strategies known for general concurrent mean-payoff games  can be viewed as playing the corresponding discounted game with a variable discount factor that depends on how ``nice'' the rewards has been up to now. Basically, in each round you play the optimal strategy in the corresponding discounted game with a discount factor $\gamma$. Whenever 
 your rewards are close to or better than the value, you decrease $\gamma$ towards 0 and in each round your rewards are much worse than the value you let $\gamma$ increase, except not bigger than the initial $\gamma$ in the first round. Much of this section will argue that many natural candidates for simpler types of strategies does not work.


We will show that approximating the value, however, can, as mentioned, be done in polynomial space. The proof relies on Proposition~22 from~\cite{HKLMT:2011}, stating the following:
\begin{proposition}
Let $\epsilon=2^{-j}$, where $j$ is some positive integer, and the probabilities be rational numbers where the nominator and denominator have bitsize at most $\tau$. Also, let $\lambda=\epsilon^{\tau m^{O(n^2)}}$. Consider some state $s$ and let the value of that state in the $\lambda$ discounted game be $v_{\lambda}$ and the value in mean-payoff game be $v$, then $|v-v_{\lambda}|<\epsilon$.
\end{proposition}

We will use that to again reduce to the existential theory over the reals. 
For a fixed discount factor $\gamma$, we can easily express the value of the corresponding discounted game, like we expressed the value of a concurrent reachability game.
We have that the value $v$ is then $v=\lim_{\gamma\rightarrow 0^+} f(\gamma)$, where $f$ is the found expression.
I.e. for any $\epsilon$, there is a $\gamma'$ such that for all $\gamma<\gamma$, we have that $|f(\gamma)-v|\leq \epsilon$.
Also, that $v>c$ means that there is $\epsilon$, such that $v-\epsilon>c$.

The problem is thus to come up with a polynomial sized formula to express that $\lambda$ is $\epsilon^{\tau m^{O(n^2)}}=2^{-j \tau m^{O(n^2)}}$.

That can be done as follows, using $\ell=O(n^2)\cdot \log(m)+\log(j\tau)$ many variables, $v_0,v_1,\dots v_{\ell-1}$:
\[
v_0=1/2
\]
and for all $0<i< \ell$, we have that
\[
v_i=v_{i-1}\cdot v_{i-1}.
\]
Using induction, we see that $v_i=2^{-2^{i}}$, i.e., $v_1=1/2=2^{-2^0}$ and \[
v_i=v_{i-1}\cdot v_{i-1}=2^{-2^{i-1}}\cdot 2^{-2^{i-1}}=2^{-2^{i-1}-2^{i-1}}=2^{-2^{i}}\]
In particular, \[
v_{\ell-1}=2^{-2^{\ell}}=2^{-2^{O(n^2)\cdot \log(m)+\log(j\tau)}}=2^{-j\tau m^{O(n^2)}}
\] is the value we wanted for $\lambda$.
Thus, for a given number $v$, we can test if the value of a concurrent  $\lambda$-discounted game is above $v+\epsilon$, which, using the proposition above, implies that $v$ is below the value of the corresponding concurrent mean-payoff game. On the other hand, the proposition also implies that if the value of the concurrent  $\lambda$-discounted game is below $v-\epsilon$, then the value of the concurrent mean-payoff game is below $v$. Being able to answer these questions lets you easily approximate the value of a concurrent mean-payoff using binary search. 

We get the following lemma.
\begin{lemma}
Approximating the value of a concurrent mean-payoff game can be in done in polynomial space
\end{lemma}



We will now consider a specific, well-studied example of a concurrent mean-payoff game, since it shows that many natural kinds of strategies do not suffice in general.
The game is called the big match and is defined as follows:
There are 3 vertices, $\{0,s,1\}$, where the vertices in $\{0,1\}$ are absorbing, and with value equal to their name.
The last vertex $s$ has a 2x2-matrix and for all $i,j$ for $i\neq j$, we have that 
$c(s,1,1)=1$, and for $i\neq 1\neq j$ we have that $c(s,1,1)=0$.
Also,  $\dest(s,1,i)=s$ for each $i$, $\dest(s,2,1)=0$ and $\dest(s,2,2)=1$. There is an illustration in Figure \ref{fig:bm}.
The value of the Big Match is $1/2$.

\begin{figure}

\center
\begin{tikzpicture}[node distance=3cm,-{stealth},shorten >=2pt]
\ma{s}[$s:$]{2}{2};

\node at (s-1-1.center) {$1$};
\node at (s-2-1.center) {$0^*$};
\node at (s-1-2.center) {$0$};
\node at (s-2-2.center) {$1^*$};


\end{tikzpicture}
\caption{The Big Match}\label{fig:bm}
\end{figure}

Consider a finite-memory strategy $\sigma$ for Eve. We will argue that $\sigma$ cannot guarantee $\epsilon$ (any strategy can guarantee $-1$, since the colors are between $0$ and $1$) for any $0<\epsilon$. Let $\tau$ be the stationary strategy for Adam that plays $1$ with pr. $\epsilon/2$.
Then playing $\sigma$ against $\tau$, we get an Markov chain, where the vertex space is pairs of memory states and game vertices. 
In Markov chains, eventually, with pr. 1, a set of vertices $S$ is reached such that the set of vertices visited infinitely often is $S$. Such a set is called ergodic.
The set $S$ can clearly only contain 1 game vertex, since whenever $s$ is left, it is never entered again.
Hence, if $S$ contains $s$, the pr. that play will ever reach $\{0,1\}$ is 0.
In the MC we get from the players playing $\sigma$ and $\tau$, let $T_{\epsilon/2}$ be such that with pr. $\epsilon/2$ some ergodic set has been reached. 
Let $\tau'$ be the strategy that plays $\tau$ for $T_{\epsilon/2}$ and afterwards plays $2$. 

When $\sigma$ is played against $\tau'$, either we reach $\{0,1\}$ and Adam plays 1 only finitely many times, while in $s$ (the latter because there are only finitely many numbers below $T_{\epsilon/2}$). Thus, for Eve to win a play, the play needs to reach vertex 1. There are two ways to do so, either Eve stops before $T_{\epsilon/2}$ or after. In the former case, the pr. to reach $1$ is only $\epsilon/2$ (because Adam needs to play $2$ at the time, which is only done with pr. $\epsilon/2$). The latter only happens with pr. $\epsilon/2$ by definition of $T_{\epsilon/2}$ (because, Adam could play $2$ for an arbitrary number of steps while following $\tau$ but $s$ would not be left anyway).

We get the following lemma.

\begin{lemma}\label{lemm:no_finite_meanpayoff}
No finite memory strategy can guarantee more than $0$ in the Big Match.
\end{lemma}

The principle of sunken cost states that, when acting rationally, one should disregard cost already paid. We will next argue that this does not apply (naively) to the Big Match.
A strategy following the principle of sunken cost would not depend on past cost paid and thus, in each step $T$, there is a pr. $p_T$ of stopping for Eve.
Such strategies are called Markov strategies in the Big Match.
Fix some Markov strategy $\sigma$ for Eve. We will argue, like before, that $\sigma$ cannot guarantee more than $\epsilon$ for any $\epsilon>0$.
Note that Eve does not depend on the choices of Adam and thus, either she stops with pr. 1 or she does not.
In the former case, Adam just plays $1$ forever. When Eve stops, the vertex reached is thus $-1$.
Alternately, if Eve does not stop with pr. 1, there must be a time $T$, such that she only stops with pr. $\epsilon$ after $T$ (this is actually also the case even if she stops with pr. 1). 
Adam's strategy is then to play $1$ for $T$ steps and $2$ thereafter. Observe that the pr. to reach $1$ is thus at most $\epsilon$, in that it must be that Eve stops after $T$. If she does not stop (or stops in $0$), there will be only finitely many 1s.

We see the following:
\begin{lemma}\label{lemm:no_markov_meanpayoff}
No Markov strategy can guarantee more than $0$ in the Big Match
\end{lemma}

\section{Bibilographic references}%rename to right name. xxx
We will now give the references for this chapter, split into a few paragraphs, each corresponding to a section in the chapter.

John von Neumanns work on matrix games~\cite{vonNeumann&Morgenstern:1944} (also called normal form games), showing that they have a value and there exists optimal stationary strategies, is typically considered the founding work in game theory. Besides that paper, Dantzig~\cite{Dantzig:1965} showed the equivalence to linear programming, and thus that they can be solved in polynomial time using e.g. Khachiyan's~\cite{Kha:1979} work on the ellipsoid method.
There are also some results on how complex the strategies for matrix games are: For any $\epsilon>0$, there exists an $\epsilon$-optimal strategy that plays uniformly over a multi-set of actions of size $\lceil (\ln n)/\epsilon^2\rceil$ as shown by Lipton and Young~\cite{Lipton&Young:1994} (this is a stronger requirement than patience).
Also, as shown by Feder, Nazerzadeh and Saberi~\cite{FNS:2007} there exists games such that any $\epsilon$-optimal strategy has support at least $\Omega(\frac{\log n}{\epsilon^2})$ (note that if, for some $x$, the support is $\Omega(x)$ then patience is also $\Omega(x)$).
Finally,  as shown by Hansen, Ibsen-Jensen, Podolskii and Tsigaridas~\cite{HIPT:2013}, there is an optimal strategy in any matrix game with patience less than $(n+2)^{\frac{n+2}{2}}/2^{n+1}$ and for each $k$ there exists games with $n=m=2^k$ such that any optimal strategy has patience at least $n^{n/2}/2^{n(1+o(1))}$ (there are also results for $m$ and $n$ not equal to $2^k$ for some $k$, but not quite as tight to the upper bound).

Shapley~\cite{Sha:1953} first considered concurrent games and focused on the class of concurrent discounted games. For these, he showed that they have a value and that there are optimal stationary strategies, using in essence the proof we used for the first 3 items of Lemma~\ref{cor:long}.
The proof of the fourth item, i.e. that the value can be approximated in PPAD, comes from the work of Etessami  and Yannakakis~\cite{EY:2007}. The proof of the fifth item, an upper bound on the patience of $\epsilon$-optimal strategies appears in~\cite{ibsenjensen:2012}.

Everett~\cite{Everett:1957} was the first to consider concurrent reachability games (formally, he considered a slight generalization).
In that paper, he showed that the games have a value and $\epsilon$-optimal stationary strategies (i.e. the first part of Lemma~\ref{lemm:reach_determined} and Lemma~\ref{lemm:reach_class}). He also used the snowball game to show that Eve does not always have an optimal strategy (i.e. Lemma~\ref{lemm:no_opt_reach}).
Finally, he introduced the notion of patience for strategies.
It was shown by Himmelberg, Parthasarathy, Raghavan and Vleck~\cite{Parthasarathy:1971,HPRV:1976} that Adam has an optimal strategy.
Frederiksen and Miltersen~\cite{FM:2013} showed that for each vertex $x$ and action $a$ (except for one action for each vertex), there is a number $c_{x,a}$ and an integer $d_{x,a}$, such that 
for any $\epsilon>0$, the strategy that plays each action $a'$ in vertex $x'$ with pr. $c_{x',a'} \epsilon^{d_{x',a'}}$ is $\epsilon$-optimal (the last action in each vertex is played with the remaining pr.).
They used that to show that approximating the value (since the value can be irrational, it seems reasonable to approximate) can be done in TFNP[NP], slightly inside PSPACE.
Finding the set of vertices of value~0, i.e. Lemma~\ref{lemm:find_0_reach}, is folklore.
Finding the set of vertices of value~1, on the other hand, i.e. Lemma~\ref{lemm:find_1_reach}, is by deAlfaro, Henzinger and Kupferman~\cite{dAHK:1998} (their proof is different).
Hansen, Kouck{\'y} and Miltersen~\cite{Hansen&Koucky&Miltersen:2009} showed that purgatory $k$ requires patience $\epsilon^{2^{k-1}}$ for any $1>\epsilon>0$ for Eve. 
Later, Chatterjee, Hansen and Ibsen-Jensen~\cite{HIC:2017} showed that purgatory duel $k$ requires patience $(3/4)^{2^{k-1}}$ for any $0\leq \epsilon<1/4$ for either player.

Gillette~\cite{Gil:1957} was the first to consider concurrent mean-payoff games and introduced the Big Match game we use as an example. He showed that the Big Match does not have stationary strategies ensuring more than $0$. 
Later, Blackwell and Ferguson~\cite{BF:1968} showed that the Big Match have a value and that value is $1/2$ by showing that some strategies that depends on the full history is $\epsilon$-optimal. They also showed that no optimal Markov strategy (i.e. Lemma~\ref{lemm:no_markov_meanpayoff}) can ensure more than $0$ in that game.
Next, Kohlberg~\cite{Kohlberg:1974} extended this to show that all repeated games with absorbing states have a value.
Finally, Mertens and Neyman~\cite{MN:1981} showed that all concurrent mean-payoff games have a value (i.e the first part of Lemma~\ref{lemm:class_meanpayoff}).
The strategies employed in all these papers kept track on the sum of over the rounds of the values of the vertex in that round minus the color in that round (the strategy by Mertens and Neyman set the memory to 0 if it should have been negative though).
Finding the set of vertices where for every $\epsilon>0$, a finite memory strategy can ensure value $1-\epsilon$ was done by Chatterjee and Ibsen-Jensen~\cite{CI:2015} (the middle part of Lemma~\ref{lemm:class_meanpayoff}).
Futheremore, finding the values in a game with a fixed number of vertices in polynomial time was done by Hansen, Kouck{\'y}, Lauritzen, Miltersen and Tsigaridas~\cite{HKLMT:2011}, informally speaking by doing binary search for the values.
That finite-memory strategies cannot ensure more than $0$ in the Big Match, i.e. Lemma~\ref{lemm:no_finite_meanpayoff} seems to be folklore.
Hansen, Ibsen-Jensen and Kouck{\'y}~\cite{HIK:2016} considered extending Markov strategies with a finite amount of space and showed that if the memory is a deterministic function of the history, then no such strategy can ensure more than $-1$ in the Big Match. They also showed, for any fixed $\epsilon>0$, that in round $T$ one only needs $O(\log \log T)$ bits of memory to play $\epsilon$-optimal in any absorbing game.
Finally, Hansen, Ibsen-Jensen and Neyman~\cite{HIN:2018} showed that Markov strategies extended with a single bit of space suffice to play the Big Match $\epsilon$-optimally, for any $\epsilon>0$ (naturally, the strategy used randomisation to update the memory state).







