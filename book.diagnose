***********
* Summary *
***********

153 undefined knowledge(s).
29 autoreference(s) are introduced twice.
7 autoreference(s) are used but not introduced.

58 autoreference(s) are properly used.
26 autoreference(s) are defined but not used.


************************
* Undefined knowledges *
************************

\knowledge {players}{}
\knowledge {graph}{}
\knowledge {edge}{}
\knowledge {incoming vertex}{}
\knowledge {outgoing vertex}{}
\knowledge {outgoing edge}{}
\knowledge {incoming edge}{}
\knowledge {path}{}
\knowledge {starts from}{}
\knowledge {ends in}{}
\knowledge {successor}{}
\knowledge {predecessor}{}
\knowledge {reachable}{}
\knowledge {outdegree}{}
\knowledge {indegree}{}
\knowledge {simple path}{}
\knowledge {cycle}{}
\knowledge {simple cycle}{}
\knowledge {self loop}{}
\knowledge {conditions}{}
\knowledge {qualitative condition}{}
\knowledge {quantitative condition}{}
\knowledge {colouring function}{}
\knowledge {qualitative game}{}
\knowledge {quantitative game}{}
\knowledge {winning}{}
\knowledge {optimal}{}
\knowledge {determined}{}
\knowledge {solving a game}{}
\knowledge {computing the winning regions}{}
\knowledge {constructing a winning strategy}{}
\knowledge {ensures}{}
\knowledge {$\varepsilon $-optimal}{}
\knowledge {value problem}{}
\knowledge {computing the value}{}
\knowledge {closed under adding prefixes}{}
\knowledge {closed under removing prefixes}{}
\knowledge {prefix independent}{}
\knowledge {monotonic under adding prefixes}{}
\knowledge {monotonic under removing prefixes}{}
\knowledge {word RAM}{}
\knowledge {machine words}{}
\knowledge {unit cost model}{}
\knowledge {successors}{}
\knowledge {predecessors}{}
\knowledge {strongly polynomial time}{}
\knowledge {linear program}{}
\knowledge {\textit {B{\"u}chi} objective}{}
\knowledge {\textit {CoB{\"uchi}} objective}{}
\knowledge {weight}{}
\knowledge {mean payoff}{}
\knowledge {discounted payoff}{}
\knowledge {Automata}{}
\knowledge {transition based acceptance conditions}{}
\knowledge {deterministic}{}
\knowledge {$\omega $-regular languages}{}
\knowledge {positional strategy}{}
\knowledge {graph with condition $W$}{}
\knowledge {positionally determined}{}
\knowledge {positionally determined for both players}{}
\knowledge {uniformly positionally determined}{}
\knowledge {determined with finite memory strategies}{}
\knowledge {$\Omega $ reduces to $\Omega '$}{}
\knowledge {$W$ reduces to $W'$}{}
\knowledge {subgame}{}
\knowledge {trap}{}
\knowledge {operator}{}
\knowledge {fixed point}{}
\knowledge {complete}{}
\knowledge {contracting}{}
\knowledge {lattice}{}
\knowledge {complete lattice}{}
\knowledge {inflationary}{}
\knowledge {preserves suprema}{}
\knowledge {deflationary}{}
\knowledge {preserves infima}{}
\knowledge {pre-fixed point}{}
\knowledge {post-fixed point}{}
\knowledge {progress measure}{}
\knowledge {attractor strategy}{}
\knowledge {counter-attractor strategy}{}
\knowledge {traps}{}
\knowledge {trap for Eve}{}
\knowledge {trap for Adam}{}
\knowledge {Muller}{}
\knowledge {Streett}{}
\knowledge {Rabin}{}
\knowledge {submixing property}{}
\knowledge {submixing}{}
\knowledge {Latest Appearance Record}{}
\knowledge {LAR}{}
\knowledge {satisfies}{}
\knowledge {respects}{}
\knowledge {dominion}{}
\knowledge {succinct encoding}{}
\knowledge {cost}{}
\knowledge {Markov decision process (MDP)}{}
\knowledge {deterministic strategies,}{}
\knowledge {\emph {$ \mec $-safe}}{}
\knowledge {stochastic arena}{}
\knowledge {random vertices}{}
\knowledge {stochastic game}{}
\knowledge {value}{}
\knowledge {check}{}
\knowledge {raise}{}
\knowledge {checks}{}
\knowledge {fold}{}
\knowledge {call}{}
\knowledge {bluffing}{}
\knowledge {player $1$ plays only actions that are safe with respect to her belief}{}
\knowledge {clock}{}
\knowledge {clock constraints}{}
\knowledge {timed arena}{}
\knowledge {guard}{}
\knowledge {zone}{}
\knowledge {difference-bound matrix}{}
\knowledge {DBM}{}
\knowledge {federations}{}
\knowledge {safe time-predecessors}{}
\knowledge {finite memory}{}
\knowledge {\emph {energy} games}{}
\knowledge {\emph {bounding} games}{}
\knowledge {colourings}{}
\knowledge {qualitative objectives}{}
\knowledge {colouring}{}
\knowledge {multidimensional}{}
\knowledge {multi-dimensional mean-payoff games}{}
\knowledge {uniform}{}
\knowledge {positional}{}
\knowledge {well-structured systems}{}
\knowledge {energy objective}{}
\knowledge {multi-energy objective}{}
\knowledge {Multi-energy games}{}
\knowledge {existential Pareto bound}{}
\knowledge {energy}{}
\knowledge {total-payoff}{}
\knowledge {shortest path}{}
\knowledge {two-player zero-sum games}{}
\knowledge {MDPs}{}
\knowledge {randomized strategies}{}
\knowledge {dimension}{}
\knowledge {threshold problem}{}
\knowledge {prefix independence}{}
\knowledge {cycle games}{}
\knowledge {attractor}{}
\knowledge {pseudo-polynomial}{}
\knowledge {\textit {shortest path}}{}
\knowledge {end-components}{}
\knowledge {BWC}{}
\knowledge {\textit {end-components}}{}
\knowledge {almost-surely}{}
\knowledge {surely}{}
\knowledge {\textit {window objectives}}{}


****************************
* autoref-introduced-twice *
****************************

simple.tex:56: {sink}{default}{document}
objectives.tex:124: {parity}{default}{base}
fixed_points.tex:42: {monotonic objective}{default}{document}
parity.tex:1: {parity}{default}{base}
index.tex:58: {probability distribution}{default}{document}
index.tex:58: {support}{default}{document}
index.tex:58: {Dirac}{default}{document}
index.tex:66: {probability space}{default}{document}
index.tex:76: {sigma-algebra}{default}{document}
index.tex:79: {measurable set}{default}{document}
index.tex:82: {probability measure}{default}{document}
index.tex:105: {random variable}{default}{document}
index.tex:108: {expectation}{default}{document}
index.tex:118: {MDP}{default}{document}
index.tex:120: {probabilistic transition function}{default}{document}
index.tex:142: {randomised strategy}{default}{document}
index.tex:158: {basic cylinder}{default}{document}
index.tex:166: {event}{default}{document}
index.tex:267: {indicator}{default}{document}
index.tex:281: {almost-surely winning}{default}{document}
index.tex:281: {positively winning}{default}{document}
index.tex:281: {surely winning}{default}{document}
notations.tex:3: {probability distribution}{default}{document}
notations.tex:3: {support}{default}{document}
notations.tex:3: {Dirac}{default}{document}
notations.tex:13: {probability space}{default}{document}
notations.tex:21: {sigma-algebra}{default}{document}
notations.tex:24: {measurable set}{default}{document}
notations.tex:26: {probability measure}{default}{document}
notations.tex:45: {random variable}{default}{document}
notations.tex:48: {expectation}{default}{document}
notations.tex:55: {MDP}{default}{document}
notations.tex:57: {MDP}{default}{document}
notations.tex:57: {probabilistic transition function}{default}{document}
notations.tex:85: {randomised strategy}{default}{document}
notations.tex:108: {basic cylinder}{default}{document}
notations.tex:116: {event}{default}{document}
notations.tex:119: {expectation}{default}{document}
notations.tex:218: {indicator}{default}{document}
notations.tex:261: {almost-surely winning}{default}{document}
notations.tex:261: {positively winning}{default}{document}
notations.tex:261: {surely winning}{default}{document}
sec-counters.tex:70: {sink}{default}{document}
sec-avag-mono.tex:46: {monotonic objective}{default}{document}
index.tex:420: {beyond worst-case synthesis}{default}{document}
index.tex:441: {beyond worst-case synthesis}{default}{document}


******************************
* Autoref used without intro *
******************************

\nointro {default}{base}{mean-payoff}
\nointro {default}{base}{finite-memory}
\nointro {default}{base}{memory structure}
\nointro {default}{document}{expected value}
\nointro {default}{document}{expected value}
\nointro {default}{document}{one-counter game}
\nointro {default}{document}{percentile queries}


***********************************
* Autoref introduced but not used *
***********************************

knowledge.tex:1: {game}{default}{base}
knowledge.tex:3: {timed game}{default}{base}
notations.tex:92: {deterministic strategy}{default}{document}
notations.tex:93: {MR}{default}{document}
notations.tex:94: {MD}{default}{document}
notations.tex:227: {play-based payoff}{default}{document}
notations.tex:232: {step-based payoff}{default}{document}
reachability.tex:71: {union bound}{default}{document}
reachability.tex:202: {closed}{default}{document}
reachability.tex:203: {sub-MDP}{default}{document}
discounted.tex:93: {$\vec {x}$-safe}{default}{document}
mean_payoff_strongly_connected.tex:64: {Ergodic theorem}{default}{document}
12_knowledge.tex:44: {total}{default}{document}
12_knowledge.tex:86: {hit-or-run game}{default}{document}
12_knowledge.tex:88: {robot game}{default}{document}
12_knowledge.tex:92: {upward closure}{default}{document}
12_knowledge.tex:95: {downward closure}{default}{document}
12_knowledge.tex:97: {downwards closed}{default}{document}
12_knowledge.tex:99: {principal ideal}{default}{document}
13_knowledge.tex:4: {multiobjective}{default}{document}
13_knowledge.tex:7: {rich behavioural models}{default}{document}
13_knowledge.tex:13: {multidimension quantitative games}{default}{document}
13_knowledge.tex:16: {Pareto-optimal strategy}{default}{document}
13_knowledge.tex:19: {Pareto frontier}{default}{document}
13_knowledge.tex:21: {half-memoryless determinacy}{default}{document}
13_knowledge.tex:23: {worst-case guarantees}{default}{document}


