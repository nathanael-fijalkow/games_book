
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Notations &#8212; Games on graphs</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Positive and almost-sure reachability and safety in MDPs" href="reachability.html" />
    <link rel="prev" title="Markov Decision Processes" href="index.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/cover.jpg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Games on graphs</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../1_Introduction/index.html">
   Introduction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/intro.html">
     What is this book about?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/simple.html">
     A first model of games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/objectives.html">
     Objectives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/computation.html">
     Computational models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/automata.html">
     Automata
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/memory.html">
     Memory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/reductions.html">
     Reductions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/subgames.html">
     Traps and subgames
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/fixed_points.html">
     Generic fixed point algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/value_iteration.html">
     Value iteration algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/strategy_improvement.html">
     Strategy improvement algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_Introduction/references.html">
     Bibliographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Classic
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../2_Regular/index.html">
   Regular Games
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../2_Regular/attractors.html">
     Reachability games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_Regular/buchi.html">
     BÃ¼chi games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_Regular/parity.html">
     Parity games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_Regular/muller.html">
     Rabin, Streett, and Muller games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_Regular/zielonka.html">
     Zielonka tree
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_Regular/references.html">
     Bibliographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../3_Parity/index.html">
   Parity Games
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../3_Parity/strategy_improvement.html">
     An exponential time strategy improvement algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_Parity/zielonka.html">
     A quasipolynomial time attractor decomposition algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_Parity/separation.html">
     A quasipolynomial time separating automata algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_Parity/value_iteration.html">
     A quasipolynomial time value iteration algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_Parity/relationships.html">
     Comparing the three families of algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3_Parity/references.html">
     Bibliographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../4_Payoffs/index.html">
   Games with Payoffs
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../4_Payoffs/notations.html">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_Payoffs/qualitative.html">
     Refining qualitative objectives with quantities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_Payoffs/mean_payoff.html">
     Mean payoff games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_Payoffs/discounted_payoff.html">
     Discounted payoff games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_Payoffs/shortest_path.html">
     Shortest path games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_Payoffs/total_payoff.html">
     Total payoff games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../4_Payoffs/references.html">
     Bibliographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Stochastic
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="index.html">
   Markov Decision Processes
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="reachability.html">
     Positive and almost-sure reachability and safety in MDPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="discounted.html">
     Discounted payoff in MDPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mean_payoff_properties.html">
     Mean-payoff in MDPs: General properties and linear programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mean_payoff_strongly_connected.html">
     Mean-payoff optimality in strongly connected MDPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="end_components.html">
     End components
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="reductions.html">
     Reductions to optimal reachability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimal_reachability.html">
     Optimal reachability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="references.html">
     Bibliographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../6_Stochastic/index.html">
   Stochastic Games
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../6_Stochastic/notations.html">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_Stochastic/determinacy.html">
     Positional determinacy of stochastic reachability games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_Stochastic/relations.html">
     Relations between all games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_Stochastic/algos.html">
     Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_Stochastic/references.html">
     Bibliographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Information
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../7_Concurrent/index.html">
   Concurrent Games
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../7_Concurrent/notations.html">
     Notations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_Concurrent/matrix_games.html">
     Matrix games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_Concurrent/discounted.html">
     Concurrent discounted games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_Concurrent/reachability.html">
     Concurrent reachability games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_Concurrent/mean_payoff.html">
     Concurrent mean-payoff games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_Concurrent/discounted.html">
     Concurrent discounted games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7_Concurrent/references.html">
     Bibilographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../8_Imperfect/index.html">
   Games with Signals
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../8_Imperfect/finite_duration.html">
     Finite duration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_Imperfect/finite_duration.html#min-left-1-x-3-1-y-3-1-x-9-1-y-right-right-frac-1-4-max-x-y-in-0-1-2-min-left-4-6y-6-2x-6y-br-right">
     \min\left(
{(1-x) +  3 (1-y)},
{3(1-x) - 9(1-y)}
\right)\right)\
=&amp;
\frac{1}{4}\max_{(x,y)\in[0,1]^2}
\min\left(
4 - 6y,
-6 -2x + 6y
     <br/>
     \right)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_Imperfect/infinite_duration.html">
     Infinite duration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../8_Imperfect/references.html">
     Bibliographic references
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="notations">
<span id="sec-notations"></span><h1>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">Â¶</a></h1>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\expv}{\mathbb{E}} \newcommand{\discProbDist}{f} \newcommand{\sampleSpace}{S} \newcommand{\sigmaAlg}{\mathcal{F}} \newcommand{\probm}{\mathbb{P}} \newcommand{\rvar}{X} \\\newcommand{\actions}{A} \newcommand{\colouring}{c} \newcommand{\probTranFunc}{\Delta} \newcommand{\edges}{E} \newcommand{\colours}{C} \newcommand{\mdp}{\mathcal{M}} \newcommand{\vinit}{v_0} \newcommand{\cylProb}{p} \newcommand{\emptyPlay}{\epsilon} \newcommand{\objective}{\Omega} \newcommand{\genColour}{\textsc{c}} \newcommand{\quantObj}{f} \newcommand{\quantObjExt}{\bar{\quantObj}} \newcommand{\indicator}[1]{\mathbf{1}_{#1}} \newcommand{\eps}{\varepsilon} \newcommand{\maxc}{\max_{\colouring}} 
\newcommand{\winPos}{W_{&gt;0}}
\newcommand{\winAS}{W_{=1}}
\newcommand{\cylinder}{\mathit{Cyl}}
\newcommand{\PrePos}{\text{Pre}_{&gt;0}}
\newcommand{\PreAS}{\text{Pre}_{=1}}
\newcommand{\PreOPPos}{\mathcal{P}_{&gt;0}}
\newcommand{\OPAS}{\mathcal{P}_{=1}}
\newcommand{\safeOP}{\mathit{Safe_{=1}}}
\newcommand{\closed}{\mathit{Cl}}\\\newcommand{\reachOP}{\mathcal{V}}
\newcommand{\discOP}{\mathcal{D}}
\newcommand{\valsigma}{\vec{x}^{\sigma}}
\newcommand{\lp}{\mathcal{L}}
\newcommand{\lpdisc}{\lp_{\mathit{disc}}}
\newcommand{\lpreach}{\lp_{\mathit{reach}}}
\newcommand{\lpmp}{\lp_{\mathit{mp}}}
\newcommand{\lpsol}[1]{\bar{\vec{#1}}}
\newcommand{\lpsolg}[1]{\bar{#1}}
\newcommand{\lpmpdual}{\lpmp^{\mathit{dual}}}
\newcommand{\actevent}[3]{\actions^{#1}_{#2,#3}} 
\newcommand{\MeanPayoffSup}{\MeanPayoff^{\;+}}
\newcommand{\MeanPayoffInf}{\MeanPayoff^{\;-}}
\newcommand{\mcprob}{P}
\newcommand{\invdist}{\vec{z}}
\newcommand{\hittime}{T}
\newcommand{\playPay}{\textsf{p-Payoff}}
\newcommand{\stepPay}{\textsf{s-Payoff}}
\newcommand{\Pay}{\textsf{Payoff}}
\newcommand{\mec}{M}
\newcommand{\OPS}{\mathcal{S}_{=1}}
\newcommand{\smallmp}{\mathit{mp}}
\newcommand{\vgood}{v_{\mathit{good}}}
\newcommand{\vbad}{v_{\mathit{bad}}}
\newcommand{\finact}{fin}
\newcommand{\mecs}{\mathit{MEC}}
\newcommand{\slice}[2]{#1_{#2-}}
\newcommand{\ReachOp}{\mathcal{R}}
\newcommand{\dPayoffStep}[1]{\DiscountedPayoff^{\;(#1)}}
\newcommand{\solvset}{S}
\newcommand{\Eve}{\textrm{Eve}}
\newcommand{\Adam}{\textrm{Adam}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Zinfty}{\Z \cup \set{\pm \infty}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rinfty}{\R \cup \set{\pm \infty}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Qinfty}{\Q \cup \set{\pm \infty}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\newcommand{\Op}{\mathbb{O}}
\newcommand{\Prob}{\mathbb{P}} \newcommand{\dist}{\mathcal{D}} \newcommand{\Dist}{\dist} \newcommand{\supp}{\textrm{supp}} 
\newcommand{\game}{\mathcal{G}} \renewcommand{\Game}{\game} \newcommand{\arena}{\mathcal{A}} \newcommand{\Arena}{\arena} 
\newcommand{\col}{\textsf{col}} \newcommand{\Col}{\col} 
\newcommand{\mEve}{\mathrm{Eve}}
\newcommand{\mAdam}{\mathrm{Adam}}
\newcommand{\mRandom}{\mathrm{Random}}
\newcommand{\vertices}{V} \newcommand{\VE}{V_\mEve} \newcommand{\VA}{V_\mAdam} \newcommand{\VR}{V_\mRandom} 
\newcommand{\ing}{\textrm{In}}
\newcommand{\Ing}{\ing}
\newcommand{\out}{\textrm{Out}}
\newcommand{\Out}{\out}
\newcommand{\dest}{\Delta} 
\newcommand{\WE}{W_\mEve} \newcommand{\WA}{W_\mAdam} 
\newcommand{\Paths}{\textrm{Paths}} \newcommand{\play}{\pi} \newcommand{\first}{\textrm{first}} \newcommand{\last}{\textrm{last}} 
\newcommand{\mem}{\mathcal{M}} \newcommand{\Mem}{\mem} 
\newcommand{\Pre}{\textrm{Pre}} \newcommand{\PreE}{\textrm{Pre}_\mEve} \newcommand{\PreA}{\textrm{Pre}_\mAdam} \newcommand{\Attr}{\textrm{Attr}} \newcommand{\AttrE}{\textrm{Attr}_\mEve} \newcommand{\AttrA}{\textrm{Attr}_\mAdam} \newcommand{\rank}{\textrm{rank}}
\newcommand{\Win}{\textrm{Win}} 
\newcommand{\Lose}{\textrm{Lose}} 
\newcommand{\Value}{\textrm{val}} 
\newcommand{\ValueE}{\textrm{val}_\mEve} 
\newcommand{\ValueA}{\textrm{val}_\mAdam}
\newcommand{\val}{\Value} 
\newcommand{\Automaton}{\mathbf{A}} 
\newcommand{\Safe}{\mathtt{Safe}}
\newcommand{\Reach}{\mathtt{Reach}} 
\newcommand{\Buchi}{\mathtt{Buchi}} 
\newcommand{\CoBuchi}{\mathtt{CoBuchi}} 
\newcommand{\Parity}{\mathtt{Parity}} 
\newcommand{\Muller}{\mathtt{Muller}} 
\newcommand{\Rabin}{\mathtt{Rabin}} 
\newcommand{\Streett}{\mathtt{Streett}} 
\newcommand{\MeanPayoff}{\mathtt{MeanPayoff}} 
\newcommand{\DiscountedPayoff}{\mathtt{DiscountedPayoff}}
\newcommand{\Energy}{\mathtt{Energy}}
\newcommand{\TotalPayoff}{\mathtt{TotalPayoff}}
\newcommand{\ShortestPath}{\mathtt{ShortestPath}}
\newcommand{\Sup}{\mathtt{Sup}}
\newcommand{\Inf}{\mathtt{Inf}}
\newcommand{\LimSup}{\mathtt{LimSup}}
\newcommand{\LimInf}{\mathtt{LimInf}}
\newcommand{\NL}{\textrm{NL}}
\newcommand{\PTIME}{\textrm{PTIME}}
\newcommand{\NP}{\textrm{NP}}
\newcommand{\UP}{\textrm{UP}}
\newcommand{\coNP}{\textrm{coNP}}
\newcommand{\coUP}{\textrm{coUP}}
\newcommand{\PSPACE}{\textrm{PSPACE}}\end{aligned}\end{align} \]</div>
<p>We write vectors in boldface: <span class="math notranslate nohighlight">\( \vec{x}, \vec{y}, \)</span> etc. For a vector <span class="math notranslate nohighlight">\( \vec{x} \)</span> indexed by a set <span class="math notranslate nohighlight">\( I \)</span> (i.e. <span class="math notranslate nohighlight">\( \vec{x}\in \mathbb{R}^I \)</span>) we denote by <span class="math notranslate nohighlight">\( \vec{x}_i \)</span> the value of the component whose index is  <span class="math notranslate nohighlight">\(i\in I  \)</span>.</p>
<p>A (discrete) probability distribution over a finite or countably infinite set <span class="math notranslate nohighlight">\(A\)</span> is a function <span class="math notranslate nohighlight">\(\discProbDist A \colon \rightarrow [0,1]\)</span> such that <span class="math notranslate nohighlight">\(\sum_{a\in A}\discProbDist(a)=1\)</span>. The support of such a distribution <span class="math notranslate nohighlight">\(\discProbDist\)</span> is the set of all <span class="math notranslate nohighlight">\(a\in A\)</span> with <span class="math notranslate nohighlight">\(\discProbDist(a)&gt;0\)</span>. A distribution <span class="math notranslate nohighlight">\(f\)</span> is called Dirac if its support has size 1.
We denote by <span class="math notranslate nohighlight">\(\dist(A)\)</span> the set of all probability distributions over <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>We also deal with probabilities over uncountable sets of events. This is accomplished via the standard notion of a <strong>probability space.</strong></p>
<div class="proof definition admonition" id="5-def:probspace">
<p class="admonition-title"><span class="caption-number">Definition 132 </span> (Probability space)</p>
<div class="definition-content section" id="proof-content">
<p>A probability space is a triple
<span class="math notranslate nohighlight">\((\sampleSpace,\sigmaAlg,\probm)\)</span> where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sampleSpace\)</span> is a non-empty set of <strong>events</strong> (so called
<strong>sample space</strong>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigmaAlg\)</span> is a sigma-algebra over <span class="math notranslate nohighlight">\(\sampleSpace\)</span>,
i.e. a collection of subsets of <span class="math notranslate nohighlight">\(\sampleSpace\)</span> that contains the empty set
<span class="math notranslate nohighlight">\(\emptyset\)</span> and that is closed under complementation and countable unions. The members of <span class="math notranslate nohighlight">\(\sigmaAlg\)</span> are called <span class="math notranslate nohighlight">\(\sigmaAlg\)</span>-measurable
sets.</p></li>
<li><p><span class="math notranslate nohighlight">\(\probm\)</span> is a probability measure on <span class="math notranslate nohighlight">\(\sigmaAlg\)</span>, i.e. a function
<span class="math notranslate nohighlight">\(\probm\colon \sigmaAlg\rightarrow[0,1]\)</span> such that:</p></li>
<li><p><span class="math notranslate nohighlight">\(\probm(\emptyset)=0\)</span>;</p></li>
<li><p>for all <span class="math notranslate nohighlight">\(A\in \sigmaAlg\)</span> it holds <span class="math notranslate nohighlight">\(\probm(\sampleSpace \setminus
A)=1-\probm(A)\)</span>; and</p></li>
<li><p>for all countable sequences of pairwise disjoint sets <span class="math notranslate nohighlight">\(A_1,A_2,\dots \in \sigmaAlg\)</span> (i.e., <span class="math notranslate nohighlight">\(A_i \cap A_j = \emptyset\)</span> for all <span class="math notranslate nohighlight">\(i\neq j\)</span>)
we have <span class="math notranslate nohighlight">\(\sum_{i=1}^{\infty}\probm(A_i)=\probm(\bigcup_{i=1}^{\infty} A_i)\)</span>.</p></li>
</ul>
</div>
</div><p>A random variable in the probability space <span class="math notranslate nohighlight">\((\sampleSpace,\sigmaAlg,\probm)\)</span> is an <span class="math notranslate nohighlight">\(\sigmaAlg\)</span>-measurable function <span class="math notranslate nohighlight">\(\rvar\colon \Omega \rightarrow \R \cup
\{-\infty,\infty\}\)</span>, i.e.,
a function such that for every <span class="math notranslate nohighlight">\(a\in \R \cup \{ -\infty,\infty\}\)</span> the set
<span class="math notranslate nohighlight">\(\{\omega\in \Omega\mid \rvar(\omega)\leq a\}\)</span> belongs to <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>. We denote by <span class="math notranslate nohighlight">\(\expv[\rvar]\)</span> the expected value of a random variable <span class="math notranslate nohighlight">\(\rvar\)</span>~(see Chapter 5 in <span id="id1">[<span>Bil:1995</span>]</span> for a formal definition).</p>
<p>We first give a syntactic notion of an MDP which is an analogue of the notion of an arena for games.</p>
<div class="proof definition admonition" id="5-def:MDP">
<p class="admonition-title"><span class="caption-number">Definition 133 </span> (MDP)</p>
<div class="definition-content section" id="proof-content">
<p>A Markov decision process is a tuple <span class="math notranslate nohighlight">\((\vertices,\edges,\probTranFunc,\colouring)\)</span>. The meaning of <span class="math notranslate nohighlight">\(\vertices\)</span>, <span class="math notranslate nohighlight">\(\edges\)</span>, and <span class="math notranslate nohighlight">\(\colouring\)</span> is the same as for games, i.e. <span class="math notranslate nohighlight">\(\vertices\)</span> is a finite set of vertices, <span class="math notranslate nohighlight">\(\edges\subseteq \vertices\times\vertices\)</span> is a set of edges and <span class="math notranslate nohighlight">\(\colouring\colon \edges \rightarrow \colours\)</span> a mapping of edges to a set of colours. However, the meaning of <span class="math notranslate nohighlight">\(\probTranFunc\)</span> is now different: <span class="math notranslate nohighlight">\(\probTranFunc\)</span> is a partial probabilistic transition function of type <span class="math notranslate nohighlight">\(\probTranFunc\colon \vertices \times \actions \rightarrow \dist(\edges)\)</span>, such that the support of <span class="math notranslate nohighlight">\(\probTranFunc(v,a)\)</span> only contains edges outgoing from <span class="math notranslate nohighlight">\(v\)</span>.
We usually write <span class="math notranslate nohighlight">\(\probTranFunc(v'\mid v,a)\)</span> as a shorthand for <span class="math notranslate nohighlight">\(\probTranFunc(v,a)((v,v'))\)</span>, i.e. the probability of transitioning from <span class="math notranslate nohighlight">\(v\)</span> to <span class="math notranslate nohighlight">\(v'\)</span> under action <span class="math notranslate nohighlight">\(a\)</span>.</p>
</div>
</div><p>We also stipulate that for each edge <span class="math notranslate nohighlight">\((v_1,v_2)\)</span> there exists an action <span class="math notranslate nohighlight">\(a\in \actions\)</span> such that <span class="math notranslate nohighlight">\(\probTranFunc(v_2\mid v_1,a)&gt;0\)</span>. Edges not satisfying this can be always removed without changing the semantics of the MDP, which is defined below. We denote by <span class="math notranslate nohighlight">\( p_{\min} \)</span> the smallest non-zero edge probability in a given MDP, i.e. <span class="math notranslate nohighlight">\( p_{\min} = \min\{x&gt;0 \mid \exists u,v \in \vertices, a \in \actions \text{ s.t. } x = \probTranFunc(v\mid u,a)\}. \)</span></p>
<p>We denote by <span class="math notranslate nohighlight">\(\edges_\genColour\)</span> the set of edges coloured by <span class="math notranslate nohighlight">\(\genColour\)</span>. Also, for MDPs where <span class="math notranslate nohighlight">\(\colours\)</span> is some set of numbers, we use <span class="math notranslate nohighlight">\(\maxc\)</span> to denote the number <span class="math notranslate nohighlight">\(\max_{e\in 
\edges}|\colouring(e)|\)</span>.
In the setting of MDPs it is technically convenient to encode regular objectives (Reachability, BÃ¼chi,\dots) by colours on <strong>vertices</strong> as opposed to edges. Hence, when discussing these objectives, we assume that the colouring function <span class="math notranslate nohighlight">\(\colouring\)</span> has the type <span class="math notranslate nohighlight">\(\vertices \rightarrow \colours\)</span>.</p>
<blockquote>
<div><p><strong>Plays and strategies in MDPs</strong></p>
</div></blockquote>
<p>The way in which a play is generated in an MDP is similar to games, but now encompasses a certain degree of randomness. There is a single player, say Eve, who controls all the vertices. Eveâs interaction with the world described by an MDP is probabilistic. One reason is the stochasticity of the transition function, the other is the fact that in MDP settings, it is usually permitted for Eve to use randomised strategies. Formally, a randomised strategy is a function <span class="math notranslate nohighlight">\(\sigma : E^* \to \dist(A)\)</span>, which to each finite play assigns a probability distribution over actions.
We typically shorten <span class="math notranslate nohighlight">\(\sigma(\play)(a)\)</span> to <span class="math notranslate nohighlight">\(\sigma(a\mid \play)\)</span>.</p>
<p>In this section, we will refer to randomised strategies simply as strategies. The strategies known from the game setting will be called  deterministic strategies. Formally, a deterministic strategy can be viewed as a special type of a randomised strategy which always selects a Dirac distribution over the edges. We shorten memoryless randomised/deterministic to MR and MD, respectively.</p>
<p>Now a play in an MDP is produced as follows: in each step, when the finite play produced so far (i.e. the history of the game tokenâs movement) is <span class="math notranslate nohighlight">\(\play\)</span>, Eve chooses an action <span class="math notranslate nohighlight">\(a\)</span> randomly according to the distribution <span class="math notranslate nohighlight">\(\sigma(\play)\)</span>. Then, an edge outgoing from <span class="math notranslate nohighlight">\(\last(\play)\)</span> is chosen randomly according to <span class="math notranslate nohighlight">\(\probTranFunc(\last(\play),a)\)</span> and the token is pushed along the selected edge. As shown below, this intuitive process can be formalized by constructing a special probability space whose sample space consists of infinite plays in the MDP.</p>
<blockquote>
<div><p><strong>Formal semantics of MDPs</strong></p>
</div></blockquote>
<p>Formally, to each MDP <span class="math notranslate nohighlight">\(\mdp\)</span>, each (Eveâs) strategy <span class="math notranslate nohighlight">\(\sigma\)</span> in <span class="math notranslate nohighlight">\(\mdp\)</span>, and
each initial vertex <span class="math notranslate nohighlight">\(\vinit\)</span> we assign a probability space
<span class="math notranslate nohighlight">\((\sampleSpace_{\mdp},\sigmaAlg_{\mdp},\probm^{\sigma}_{\mdp,\vinit})\)</span>. To
explain the individual components, we need the notion of a cylinder set. A
basic cylinder determined by a finite play <span class="math notranslate nohighlight">\(\play\)</span> is the set of
all infinite plays in <span class="math notranslate nohighlight">\(\mdp\)</span> having <span class="math notranslate nohighlight">\(\play\)</span> as a prefix. Now the above
probability space consists of the following components:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sampleSpace_{\mdp}\)</span> is the set of all infinite plays in <span class="math notranslate nohighlight">\(\mdp\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span> is the <strong>Borel</strong> sigma-algebra over
<span class="math notranslate nohighlight">\(\Omega_{\mdp}\)</span>; this is the smallest sigma-algebra containing all the
basic cylinder sets determined by finite plays in <span class="math notranslate nohighlight">\(\mdp\)</span>. The sets in
<span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span> are called events. Note that the smallest sigma-algebra of the desired property is guaranteed to exist, since an intersection of an arbitrary number of sigma-algebras is again a sigma algebra.</p></li>
<li><p><span class="math notranslate nohighlight">\(\probm^{\sigma}_{\mdp,\vinit}\)</span> is the unique probability measure
arising from the <strong>cylinder construction</strong> detailed below. We use
<span class="math notranslate nohighlight">\(\expv^{\sigma}_{\mdp,\vinit}\)</span> to denote the expectation operator
associated to the measure <span class="math notranslate nohighlight">\(\probm^{\sigma}_{\mdp,\vinit}\)</span>.</p></li>
</ul>
<p>Since the sample space <span class="math notranslate nohighlight">\(\sampleSpace_{\mdp}\)</span> is uncountable, we construct the
probability measure by first specifying a probability of certain simple sets of
runs and then using an appropriate <strong>measure-extension</strong> theorem to extend
the probability measure, in a unique way, to all sets in <span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span>.
The standard cylinder construction<br />
proceeds as follows: for each finite play <span class="math notranslate nohighlight">\(\play\)</span> we define the probability
<span class="math notranslate nohighlight">\(\cylProb(\play)\)</span> such that</p>
<ul class="simple">
<li><p>for an empty play <span class="math notranslate nohighlight">\(\emptyPlay\)</span> we put <span class="math notranslate nohighlight">\(\cylProb(\emptyPlay)=1\)</span>;</p></li>
<li><p>for a non-empty play <span class="math notranslate nohighlight">\(\play=\play_0\cdots \play_{k}\)</span> initiated in
<span class="math notranslate nohighlight">\(\vinit\)</span> we put</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\cylProb(\play) = \cylProb(\play_{&lt; k})\cdot \Big(\sum_{a \in \actions} 
\sigma(a\mid \play_{&lt; k})\cdot \probTranFunc(\last(\play)\mid 
\last(\play_{&lt; k}),a) 
\Big), \]</div>
<p>where we use the convention that <span class="math notranslate nohighlight">\(\last(\play_{&lt; 0})=\vinit\)</span>;</p>
<ul class="simple">
<li><p>for all other <span class="math notranslate nohighlight">\(\play\)</span> we have <span class="math notranslate nohighlight">\(\cylProb(\play)=0\)</span>.</p></li>
</ul>
<p>Now using an appropriate measure-extension theorem
(such as Hahn-Kol-mo-go-rov theorem~\cite[Corollary 2.5.4 and Proposition  2.5.7]{Rosenthal:2006}, or Carathâeodory theorem~\cite[Theorem 1.3.10]{Ash&amp;Doleans-Dade:2000}) one can show that there is a
unique probability
measure <span class="math notranslate nohighlight">\(\probm^{\sigma}_{\mdp,\vinit} \)</span> on <span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span> such that for
every cylinder set <span class="math notranslate nohighlight">\(\cylinder(\play)\)</span> determined by some finite play <span class="math notranslate nohighlight">\(\play\)</span> we have <span class="math notranslate nohighlight">\(\probm_{\vinit}^\sigma(\cylinder(\play))=\cylProb(\play)\)</span>. (Abusing the notation, we write <span class="math notranslate nohighlight">\(\probm^{\sigma}_{\mdp,\vinit}(\play)\)</span> for the probability of this cylinder set). There
are some intermediate steps to be performed before an extension theorem
can be applied, and we omit these due to space constraints. Full details on the
cylinder construction can be found, e.g. in <span id="id2">[<a class="reference internal" href="references.html#id99"><span>ADoleansD00</span></a>,<a class="reference internal" href="references.html#id121"><span>Novotny15</span></a>]</span>.</p>
<p>While the construction of the probability measure
<span class="math notranslate nohighlight">\(\probm^{\sigma}_{\mdp,\vinit}\)</span> might seem a bit esoteric, in the context of
MDP verification we do not usually need to be concerned with all the delicacies
behind the associated probability space. The sets of plays that we work with
typically arise from the basic cylinder sets by means of countable unions,
intersections, and simple combinations thereof; such sets by definition belong
to the
sigma-algebra <span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span>, and their probabilities can be inferred using
basic probabilistic reasoning. Nevertheless, one should keep in mind that all the
probabilistic argumentation rests on solid formal grounds.</p>
<p>In the standard MDP literature <span id="id3">[<a class="reference internal" href="references.html#id122"><span>Put05</span></a>]</span>, the plays are often defined as alternating sequence of vertices and actions. Here we stick to the edge-based definition inherited from deterministic games. Still, we would sometimes like to speak about quantities such as probability that action <span class="math notranslate nohighlight">\(a\)</span> is taken in step <span class="math notranslate nohighlight">\(i\)</span>. To this end, we introduce, for each strategy <span class="math notranslate nohighlight">\(\sigma\)</span>, each action <span class="math notranslate nohighlight">\(a\)</span>,  and each <span class="math notranslate nohighlight">\(i\geq 0\)</span>,  a random variable <span class="math notranslate nohighlight">\(\actevent{\sigma}{a}{i}\)</span> such that <span class="math notranslate nohighlight">\(\actevent{\sigma}{a}{i}(\play)=\sigma(\play_{&lt; i})(a)\)</span>. It is easy to check that  <span class="math notranslate nohighlight">\(\expv^\sigma_v[\actevent{\sigma}{a}{i}]\)</span> is the probability that action <span class="math notranslate nohighlight">\(a\)</span> is played in step <span class="math notranslate nohighlight">\(i\)</span> when using strategy <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<blockquote>
<div><p><strong>Objectives in MDPs</strong></p>
</div></blockquote>
<p>Similarly to plays, the notions of both qualitative and quantitative objectives
are inherited from the non-stochastic world of games. However, since plays in
MDPs are generated stochastically, even for a fixed strategy <span class="math notranslate nohighlight">\(\sigma\)</span> there is
typically no single infinite play that would constitute the outcome of
<span class="math notranslate nohighlight">\(\sigma\)</span>. A concrete <span class="math notranslate nohighlight">\(\sigma\)</span> might yield different outcomes, depending on the
results of random events during the interaction with the MDP. Hence, we need a
more general way of evaluating strategies in MDPs.</p>
<p>In the game setting, a qualitative objective was given as a set <span class="math notranslate nohighlight">\(\objective
\subseteq \colours^{\omega}\)</span>. In the MDP setting, we require that such
<span class="math notranslate nohighlight">\(\objective\)</span> is measurable in the sense that the set <span class="math notranslate nohighlight">\(\colouring^{-1}(\objective) = \{\play \in \sampleSpace_{\mdp} \mdp \colouring(\play) \in \objective \}\)</span> belongs to <span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span>. We can then talk about a
probability that the produced play falls satisfies <span class="math notranslate nohighlight">\(\objective\)</span>. For instance, for a
colour <span class="math notranslate nohighlight">\(\genColour\)</span> the objective <span class="math notranslate nohighlight">\( \Reach(\genColour) \)</span> is indeed measurable, since <span class="math notranslate nohighlight">\( \colouring^{-1}(\objective) \)</span> can be written as a countable union of all basic cylinders that are determined by finite plays ending in a vertex coloured by <span class="math notranslate nohighlight">\( \genColour \)</span>. Indeed, all the qualitative objectives studied in previous chapters can be shown measurable in a similar way, and we encourage the reader to prove this as an exercise.
Hence, the expression
<span class="math notranslate nohighlight">\(\probm^{\sigma}_{\mdp,\vinit}(\Reach(\genColour))\)</span>
denotes the probability that a vertex of colour <span class="math notranslate nohighlight">\(\genColour\)</span> is reached when
using
strategy <span class="math notranslate nohighlight">\(\sigma\)</span> from vertex <span class="math notranslate nohighlight">\(\vinit\)</span>.
In line with previous conventions, we
stipulate that Eve aims to maximize this probability.</p>
<p>The situation is more complex for quantitative objectives. As shown in the previous chapter,
when working with quantitative objectives, the set of colours <span class="math notranslate nohighlight">\(\colours\)</span> is typically the set of real numbers (or a subset thereof), and the quantitative objective is given by an aggregating function <span class="math notranslate nohighlight">\(\quantObj\colon \colours^\omega \rightarrow \R\)</span>, which can be extended into a function <span class="math notranslate nohighlight">\(\quantObjExt\colon \edges^\omega \rightarrow \R \)</span> by putting <span class="math notranslate nohighlight">\( \quantObjExt(\play) = \quantObj( \colouring(\play_0)\colouring(\play_1)\cdots) \)</span>.
In the MDP setting, we
require that <span class="math notranslate nohighlight">\(\quantObjExt\)</span> is <span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span>-measurable, which
means that for each <span class="math notranslate nohighlight">\(x\in \R\)</span> the set <span class="math notranslate nohighlight">\(\{\pi\in \edges^\omega\mid 
\quantObj(\colouring(\play_0)\colouring(\play_1)\cdots) \leq x\}\)</span> belongs to
<span class="math notranslate nohighlight">\(\sigmaAlg_{\mdp}\)</span> (again this holds for all the objectives studied
in the previous chapters). Then there are two ways in which we can define the expected payoff achieved by strategy <span class="math notranslate nohighlight">\( \sigma \)</span> from a vertex <span class="math notranslate nohighlight">\( v \)</span>.
First, we can treat
<span class="math notranslate nohighlight">\(\quantObjExt\)</span> as a random variable
in the probability space
<span class="math notranslate nohighlight">\((\sampleSpace_{\mdp},\sigmaAlg_{\mdp},\probm^{\sigma}_{\mdp,v})\)</span>. Then the play-based payoff of <span class="math notranslate nohighlight">\(\sigma\)</span> from <span class="math notranslate nohighlight">\( v \)</span>, which we denote by <span class="math notranslate nohighlight">\( \playPay_\quantObj(v,\sigma) \)</span>, is the expected value of this random variable, i.e. <span class="math notranslate nohighlight">\( \playPay_\quantObj(v,\sigma) = \expv_{v}^\sigma [\quantObjExt] \)</span>. That is, we compute the expected payoff over all plays. This approach subsumes also qualitative objectives: For such an objective <span class="math notranslate nohighlight">\(\objective\)</span> we can consider an indicator random
variable <span class="math notranslate nohighlight">\(\indicator{\objective}\)</span>, such that <span class="math notranslate nohighlight">\(\indicator{\objective}(\play)=1\)</span>
of
<span class="math notranslate nohighlight">\(\play\in\Omega\)</span> and <span class="math notranslate nohighlight">\(\indicator{\objective}(\play)=0\)</span> otherwise. Then
<span class="math notranslate nohighlight">\(\probm^{\sigma}_{\mdp,v}(\objective) = 
\expv^{\sigma}_{\mdp,v}[\indicator{\objective}] = \playPay_{\indicator{\objective}}(v,\sigma)\)</span>.</p>
<p>The second approach to quantitative objectives in MDPs, common e.g. in the operations research literature, is step-based: for each time step <span class="math notranslate nohighlight">\(i\)</span> we compute the expected one-step reward (i.e. colour) encountered in that step and then  aggregate  these one-step expectations. Formally, the step-based payoff of <span class="math notranslate nohighlight">\( \sigma \)</span> from <span class="math notranslate nohighlight">\( v \)</span> is <span class="math notranslate nohighlight">\( \stepPay_f(v,\sigma) = \quantObj(\expv_{v}^\sigma[\colouring(\play_0)] \expv_{v}^\sigma[\colouring(\play_1)]\cdots] ) \)</span>, where for each <span class="math notranslate nohighlight">\( i \)</span> we treat the expression <span class="math notranslate nohighlight">\( \colouring(\play_i) \)</span> as a random variable returning the colour (i.e. a number) which labels the <span class="math notranslate nohighlight">\( i \)</span>-th edge of the randomly produced play (recall here that we index edges from <span class="math notranslate nohighlight">\( 0 \)</span>).</p>
<p>Depending on the concrete quantitative objective and on the shape of <span class="math notranslate nohighlight">\(\sigma\)</span>, the path- and step-based payoffs from a given vertex might or might not be equal. Nevertheless, in this chapter we study only objectives for which these two semantics yield the <strong>same optimization criteria:</strong> no matter which of the two semantics we use, the optimal values will be the same and strategy that is optimal w.r.t. one of the semantics is also optimal for the other one. Hence, we will fix the play-based approach as the default one, writing just <span class="math notranslate nohighlight">\( \Pay_f(v,\sigma)\)</span> instead of <span class="math notranslate nohighlight">\( \playPay_f(v,\sigma) \)</span>. We will prove the equivalence with step-based payoff where necessary. Also, we will drop the subscript <span class="math notranslate nohighlight">\( f \)</span> when the payoff function is known from the context.</p>
<blockquote>
<div><p><strong>Optimal strategies and decision problems</strong></p>
</div></blockquote>
<p>Let us fix an MDP <span class="math notranslate nohighlight">\(\mdp\)</span> and an objective given by a random variable
<span class="math notranslate nohighlight">\(\quantObj\)</span>. The value of a vertex <span class="math notranslate nohighlight">\(v\in\vertices\)</span> is the number
<span class="math notranslate nohighlight">\(\Value(v)=\sup_{\sigma} \Pay_f(v,\sigma)\)</span>. We let <span class="math notranslate nohighlight">\(\Value(\mdp)\)</span> denote the <span class="math notranslate nohighlight">\(|\vertices|\)</span>-dimensional vector whose component
indexed by <span class="math notranslate nohighlight">\(v\)</span> equals <span class="math notranslate nohighlight">\(\Value(v)\)</span>.</p>
<p>We say that a strategy <span class="math notranslate nohighlight">\(\sigma\)</span> is <span class="math notranslate nohighlight">\(\eps\)</span>-optimal in <span class="math notranslate nohighlight">\(v\)</span>, for some <span class="math notranslate nohighlight">\(\eps\geq 0\)</span>, if <span class="math notranslate nohighlight">\(\Pay_f(v,\sigma) \geq \Value(v) - \eps\)</span>. A <span class="math notranslate nohighlight">\(0\)</span>-optimal strategy is simply called optimal.</p>
<p>For qualitative objectives, there are additional modes of objective satisfaction. Given such an objective <span class="math notranslate nohighlight">\(\objective\)</span>, we say that a strategy <span class="math notranslate nohighlight">\(\sigma\)</span> is almost-surely winning from <span class="math notranslate nohighlight">\(v\)</span> if <span class="math notranslate nohighlight">\(\expv^{\sigma}_{\mdp,v}[\indicator{\objective}]=1\)</span>, i.e. if the run produced by <span class="math notranslate nohighlight">\(\sigma\)</span> falls into <span class="math notranslate nohighlight">\(\objective\)</span> with probability <span class="math notranslate nohighlight">\(1\)</span>. We also say that <span class="math notranslate nohighlight">\(\sigma\)</span> is positively winning from <span class="math notranslate nohighlight">\( v \)</span> if <span class="math notranslate nohighlight">\(\expv^{\sigma}_{\mdp,v}[\indicator{\objective}]&gt;0\)</span>. For strategies that are winning in the non-stochastic game sense, i.e. that <strong>cannot</strong> produce a run not belonging to <span class="math notranslate nohighlight">\(\objective\)</span>, are usually called surely winning to distinguish them from the above concepts. We denote by <span class="math notranslate nohighlight">\(\winPos(\mdp,\objective)\)</span> and <span class="math notranslate nohighlight">\(\winAS(\mdp,\objective)\)</span> the sets of all vertices of <span class="math notranslate nohighlight">\(\mdp\)</span> from which there exists a positively or almost-surely winning strategy for the objective <span class="math notranslate nohighlight">\(\objective\)</span>, respectively.</p>
<p>The problems pertaining to the existence of almost-surely or positively winning strategy are often called <strong>qualitative problems</strong> in the MDP literature, while the notion <strong>quantitative problems</strong> covers the general notion of optimizing the expectation of some random variable. We do not use such a nomenclature here so as to avoid confusion with qualitative vs. quantitative objectives as defined in Chapter <a class="reference internal" href="../1_Introduction/index.html#chap-introduction"><span class="std std-ref">Introduction</span></a>. Instead, we will refer directly to, e.g. almost-sure reachability while using the term optimal reachability to refer to the expectation-maximization problem.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5_MDP"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">Markov Decision Processes</a>
    <a class='right-next' id="next-link" href="reachability.html" title="next page">Positive and almost-sure reachability and safety in MDPs</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By a set of authors coordinated by Nathana&euml;l Fijalkow<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>