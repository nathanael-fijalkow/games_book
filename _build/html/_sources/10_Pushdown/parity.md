(10-sec:parity)=
# Parity pushdown games

```{math}
\def\AC#1{\textcolor{green!50!black}{\checkmark}\marginpar{\color{green!50!black}AC: #1}} 
\def\acchanged#1{{#1}}
\def\OS#1{\textcolor{red}{\checkmark}\marginpar{\color{red}OS: #1}} 

\renewcommand{\qed}{$\square$}
\newcommand{\pop}{\mathrm{pop}}
\newcommand{\push}[1]{\mathrm{push}(#1)}
\newcommand{\PDS}{\mathcal{P}}
\newcommand{\pdscol}[1]{\col(#1)}
\newcommand{\vect}[1]{\overrightarrow{#1}}
\newcommand{\ttrue}{t\! t}
\newcommand{\ffalse}{f\!\! f}
\newcommand {\Stepsg}[1]{\ensuremath{\mathit{Steps}_{#1}}}
\newcommand{\sh}{\mathrm{sh}}
\newcommand{\fgraph}{\widetilde{G}}
\newcommand{\farena}{\widetilde{\arena}}
\newcommand{\fgame}{\widetilde{\game}}
\newcommand{\fplay}{\widetilde{\play}}
\newcommand{\fsigma}{\widetilde{\sigma}}
\newcommand {\Rounds}[1]{\ensuremath{\mathit{Rounds}_{#1}}}
\newcommand{\kind}[2]{kind(#1,#2)}
\newcommand{\factcol}[2]{\mathrm{mcol}(#1,#2)}
\newcommand{\fac}[1]{kind^{#1}}
\newcommand{\pcol}[1]{\mathrm{mcol}^{#1}}
\newcommand{\hgraph}{\widehat{G}}
\newcommand{\harena}{\widehat{\arena}}
\newcommand{\hgame}{\widehat{\game}}
\newcommand{\hplay}{\widehat{\play}}
\newcommand{\hsigma}{\widehat{\sigma}}
\newcommand{\Eve}{\textrm{Eve}}
\newcommand{\Adam}{\textrm{Adam}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Zinfty}{\Z \cup \set{\pm \infty}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rinfty}{\R \cup \set{\pm \infty}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Qinfty}{\Q \cup \set{\pm \infty}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\newcommand{\Op}{\mathbb{O}}
\newcommand{\Prob}{\mathbb{P}} \newcommand{\dist}{\mathcal{D}} \newcommand{\Dist}{\dist} \newcommand{\supp}{\textrm{supp}} 
\newcommand{\game}{\mathcal{G}} \renewcommand{\Game}{\game} \newcommand{\arena}{\mathcal{A}} \newcommand{\Arena}{\arena} 
\newcommand{\col}{\textsf{col}} \newcommand{\Col}{\col} 
\newcommand{\mEve}{\mathrm{Eve}}
\newcommand{\mAdam}{\mathrm{Adam}}
\newcommand{\mRandom}{\mathrm{Random}}
\newcommand{\vertices}{V} \newcommand{\VE}{V_\mEve} \newcommand{\VA}{V_\mAdam} \newcommand{\VR}{V_\mRandom} 
\newcommand{\ing}{\textrm{In}}
\newcommand{\Ing}{\ing}
\newcommand{\out}{\textrm{Out}}
\newcommand{\Out}{\out}
\newcommand{\dest}{\Delta} 
\newcommand{\WE}{W_\mEve} \newcommand{\WA}{W_\mAdam} 
\newcommand{\Paths}{\textrm{Paths}} \newcommand{\play}{\pi} \newcommand{\first}{\textrm{first}} \newcommand{\last}{\textrm{last}} 
\newcommand{\mem}{\mathcal{M}} \newcommand{\Mem}{\mem} 
\newcommand{\Pre}{\textrm{Pre}} \newcommand{\PreE}{\textrm{Pre}_\mEve} \newcommand{\PreA}{\textrm{Pre}_\mAdam} \newcommand{\Attr}{\textrm{Attr}} \newcommand{\AttrE}{\textrm{Attr}_\mEve} \newcommand{\AttrA}{\textrm{Attr}_\mAdam} \newcommand{\rank}{\textrm{rank}}
\newcommand{\Win}{\textrm{Win}} 
\newcommand{\Lose}{\textrm{Lose}} 
\newcommand{\Value}{\textrm{val}} 
\newcommand{\ValueE}{\textrm{val}_\mEve} 
\newcommand{\ValueA}{\textrm{val}_\mAdam}
\newcommand{\val}{\Value} 
\newcommand{\Automaton}{\mathbf{A}} 
\newcommand{\Safe}{\mathtt{Safe}}
\newcommand{\Reach}{\mathtt{Reach}} 
\newcommand{\Buchi}{\mathtt{Buchi}} 
\newcommand{\CoBuchi}{\mathtt{CoBuchi}} 
\newcommand{\Parity}{\mathtt{Parity}} 
\newcommand{\Muller}{\mathtt{Muller}} 
\newcommand{\Rabin}{\mathtt{Rabin}} 
\newcommand{\Streett}{\mathtt{Streett}} 
\newcommand{\MeanPayoff}{\mathtt{MeanPayoff}} 
\newcommand{\DiscountedPayoff}{\mathtt{DiscountedPayoff}}
\newcommand{\Energy}{\mathtt{Energy}}
\newcommand{\TotalPayoff}{\mathtt{TotalPayoff}}
\newcommand{\ShortestPath}{\mathtt{ShortestPath}}
\newcommand{\Sup}{\mathtt{Sup}}
\newcommand{\Inf}{\mathtt{Inf}}
\newcommand{\LimSup}{\mathtt{LimSup}}
\newcommand{\LimInf}{\mathtt{LimInf}}
\newcommand{\NL}{\textrm{NL}}
\newcommand{\PTIME}{\textrm{PTIME}}
\newcommand{\NP}{\textrm{NP}}
\newcommand{\UP}{\textrm{UP}}
\newcommand{\coNP}{\textrm{coNP}}
\newcommand{\coUP}{\textrm{coUP}}
\newcommand{\PSPACE}{\textrm{PSPACE}}
\newcommand{\EXPSPACE}{\textrm{EXPSPACE}}
\newcommand{\EXP}{\textrm{EXP}}
\newcommand{\kEXP}{\textrm{kEXP}}
```
We now focus on the central case of parity objectives. 
In Subsection {ref}`10-subsec:computing-profiles`, we show how to compute the set of profils using a reduction to finite parity game.

For the rest of this section, we fix a parity pushdown game $\game$ played on an arena $\arena = (G,\VE,\VA)$ generated by a pushdown system $\PDS = (Q,Q_{\mEve}, Q_{\mAdam}, \Gamma,\Delta)$. We also let $V=\VE\cup\VA$ and we let the colours used in the game be $\{0,\dots,d\}$.


(10-subsec:computing-profiles)=
## Computing the Profiles

In this section we show how to build a parity game played on a **finite** arena that permits to compute the profiles in $\game$. 

We first start with some terminology and a basic result. For an infinite play $\play=v_0v_1\cdots$, let
$\Stepsg{\play}$ be the set of indices of positions where no
configuration of strictly smaller stack height is visited later in the
play. More formally, 
$$
\Stepsg{\play}=\{i\in\mathbb{N}\mid \forall
j\geq i, \sh(v_j)\geq \sh(v_i)\}.
$$
 Note that $\Stepsg{\play}$ is always
infinite and hence induces a factorisation of the play $\play$ into
finite pieces.

In the factorisation induced by $\Stepsg{\play}$, a factor $v_i\cdots v_j$ is called a
**bump** if $\sh(v_j)=\sh(v_i)$, called a **Stair** otherwise (that is, if $\sh(v_j)=\sh(v_i)+1$).

For any play $\play$ with $\Stepsg{\play}=\{n_0<n_1<\cdots\}$, we can define the sequence $(\pdscol{\play}_i)_{i\geq
0}\in\{0,\dots,d\}^{\mathbb{N}}$ by setting $\pdscol{\play}_i=\max\{\pdscol{v_k}\mid n_i\leq k\leq n_{i+1}\}$.
This sequence fully characterises the parity objective.

````{prf:proposition} NEEDS TITLE 10-prop:trans_cond
:label: 10-prop:trans_cond

Let $\play$ be a play. Then $\play$ satisfies the parity condition  if and only if $\limsup((\pdscol{\play}_i)_{i\geq 0})$ is even.

````


### Simulation Game

In the sequel, we build a new parity game $\fgame$ over a **finite** arena $\farena$.
This new game
**simulates** the original pushdown game, in the sense that the
sequence of visited colours during a correct simulation of some play $\play$ in $\game$ is
exactly the sequence $(\pdscol{\play}_i)_{i\geq 0}$. Moreover, a play in which
a player does not correctly simulate the pushdown game is losing
for that player. We shall see that the winning region in $\fgame$ allows us to compute the set of profiles $\{\mathcal{R}(q,\gamma) \mid q\in Q \text{ and } \gamma\in\Gamma\}$. Hence, by  {prf:ref}`10-thm:regularity-wr`, it will imply that one can solve a pushdown game as well as compute its winning region.



```{figure} ./../FigAndAlgos/10-fig:reduced-arena.png
:name: 10-fig:reduced-arena
:align: center
Local structure of the arena $\farena$
```


Before providing a precise description of the arena
$\farena$, let us consider the following informal description of
this simulation game. We aim at simulating a play in the pushdown game from some initial vertex $(p_{in},\bot)$. In $\farena$ we
keep track of only the control state and the top stack symbol of
the currently simulated configuration.

The interesting case is when it is in a control state $p$ with top
stack symbol $\alpha$, and the player owning $p$ wants to push a
symbol $\beta$ onto the stack and change the control state to $q$. For
every strategy of \Eve, there is a certain set of possible
(finite) continuations of the play that will end with popping
$\beta$ from the stack. We require \Eve to declare a vector
$\vect{S}=(S_0,\dots,S_d)$ of $(d+1)$ subsets of
$Q$, where $S_i$ is the set of all
states the game can be in after popping $\beta$ along those
plays where in addition the largest visited colour while $\beta$ was on
the stack is $i$.

\Adam has then two choices. He can continue the game by pushing
$\beta$ onto the stack and updating the state (we call this a
**pursue** move). Otherwise, he can pick a set $S_i$
and a state $r\in S_i$, and
continue the simulation from that state $r$ (we call this a
**jump** move). If he does a pursue move, then he remembers the
vector $\vect{S}$ claimed by \Eve; if later on, a pop transition is simulated, the play
goes in a sink vertex and \Eve wins if and only if the resulting state is in
$S_c$ where $c$ is the largest colour seen in
the current stack level (this information is encoded in the vertex, reset after each pursue move and updated after each jump
move). If \Adam
does a jump move to a state $r$ in $S_{i}$, the currently
stored value for $c$ is updated to $\max(c,i,\pdscol{r})$,
which is the largest colour seen since the current stack level was
reached.

Therefore the main vertices of this new arena are of the form $(p,\alpha,\vect{R},c)$, which are controlled by the player who controls $p$.
Intermediate vertices are used to handle the previously
described intermediate steps. The local structure is given in
{numref}`10-fig:reduced-arena`. Two special sink vertices $\ttrue$ and $\ffalse$ are
used to simulate pop moves. This arena is equipped with a
colouring function on the edges: an edge from a vertex
$(p,\alpha,\vect{R},c,q,\beta,\vect{S})$ to a vertex $(r,\alpha,\vect{R},\max(c,i,\pdscol{r})$ has colour $i$ where $i$ is the colour of the simulated bump, an edge from a vertex
$(p,\alpha,\vect{R},c,q,\beta,\vect{S})$ to a vertex $(q,\beta,\vect{S}\pdscol{q})$ simulating a jump move has colour $\pdscol{q}$, the loop on $\ttrue$ has colour $0$ while the loop on $\ffalse$ has colour $1$; all other edges get the irrelevant colour $0$. 


We now formally describe arena
$\farena$ (we refer to {numref}`10-fig:reduced-arena`) and provide some extra insight.

*  The main vertices of $\farena$ are those of the form
$(p,\alpha,\vect{R},c)$, where $p\in Q$, $\alpha\in
\Gamma$, $\vect{R}=(R_0,\dots,R_d)\in
(2^Q)^{d+1}$ and $c\in\{0,\dots,d\}$. A vertex $(p,\alpha,\vect{R},c)$ is reached when
simulating a finite play $\play$ in $\game$ such that:
\begin{itemize}

*  The last vertex in $\play$ is $(p,\bot s\alpha)$ for some $s\in \Gamma^*$.

*  \Eve claims that she has a strategy to continue $\play$ in such
  a way that if $\alpha$ is eventually popped, the control state
  reached after popping belongs to $R_m$, where $m$ is
  the largest colour visited since the stack height was at least $|s\alpha|$.

*  The colour $c$ is the largest one since the current stack level was reached from a lower stack level.

A vertex $(p,\alpha,\vect{R},c)$ is controlled by \Eve if
and only if $p\in Q_\mEve$.

\item The vertices $\ttrue$ and $\ffalse$ are here to ensure
  that the vectors $\vect{R}$ encoded in the main vertices are correct. They are both controlled by \Eve and are sink vertices with a self loop with colour $0$ for $\ttrue$ and $1$ for $\ffalse$.

There is a transition from some vertex
$(p,\alpha,\vect{R},c)$ to $\ttrue$, if and
only if there exists a transition rule $(r,pop)\in\Delta(p,\alpha)$,
such that $r\in R_{c}$ (this means that $\vect{R}$ is correct with respect
to this transition rule).
Dually, there is a transition from a vertex
$(p,\alpha,\vect{R},c)$ to $\ffalse$
if and only if there exists a transition rule
$(r,pop)\in\Delta(p,\alpha)$ such that $r\notin R_{c}$ (this means that
$\vect{R}$ is not correct with respect to this transition rule).

\item To simulate a transition rule
  $(q,push(\beta))\in\Delta(p,\alpha)$, the player that controls
  $(p,\alpha,\vect{R},c)$ moves to
  $(p,\alpha,\vect{R},c,q,\beta)$. This vertex is
  controlled by \Eve who has to give a vector
$\vect{S}=(S_0,\dots,S_d)\in
(2^Q)^{d+1}$ that describes the control states that can be
  reached if $\beta$ is eventually popped. To describe this vector,
  she goes to the corresponding vertex $(p,\alpha,\vect{R},c,q,\beta,\vect{S})$.

Any vertex $(p,\alpha,\vect{R},c,q,\beta,\vect{S})$ is
controlled by \Adam who chooses either to simulate a bump or a
stair. In the first case, he additionally has to pick the maximal colour of the
bump. To simulate a bump with maximal
colour $i$, he goes, through an edge coloured by $i$, to a vertex
$(r,\alpha,\vect{R},\max(c,i,\pdscol{r}))$, for some $r\in
S_i$.

To simulate a stair, \Adam goes, through an edge coloured by $\pdscol{q}$, to the vertex
$(q,\beta,\vect{S},\pdscol{q})$.

The last component of the vertex (that stores the
largest colour seen since the currently simulated stack level was
reached) has to be updated in all those cases. After simulating a bump
of maximal colour $i$, the maximal colour is
$\max(c,i,\pdscol{r})$. After simulating a stair, this colour has to
be initialized (since a new stack level is simulated). Its value, is
therefore $\pdscol{q}$, which is the unique colour since the (new) stack
level was reached.

\end{itemize}

The edges for which we did not precise the colour are assigned colour $0$.


The following theorem relates this new game $\fgame$ and the profiles in the pushdown game $\game$.

````{prf:theorem} NEEDS TITLE 10-thm:games
:label: 10-thm:games

The following holds.

1. [(i)] A configuration $(p_{in},\bot)$ is winning for \Eve in $\game$
if and only if
$(p_{in},\bot,(\emptyset,\dots,\emptyset),\pdscol{p_{in}})$
is winning for \Eve in
$\fgame$.

2. [(ii)] For every $q\in Q$, $\gamma\in\Gamma$ and $R\subseteq Q$, $R\in\mathcal{R}(q,\gamma)$ if and only if
$(q,\gamma,(R,\dots,R),\pdscol{q})$
is winning for \Eve in
$\fgame$.

````

The rest of the section is devoted to the proof of  {prf:ref}`10-thm:games`. We only prove point $(i)$ as the proof of point $(ii)$ is a subpart of the proof of $(i)$.

### Factorisation of a play in $\game$.

Recall that, for an infinite play $\play=v_0v_1\cdots$ in $\game$, 
$\Stepsg{\play}$ denotes the set of indices of positions where no
configuration of strictly smaller stack height is visited later in the
play. Note that $\Stepsg{\play}$ is always
infinite and hence induces a factorisation of the play $\play$ into
finite pieces. 

Indeed, for any play $\play$ with $\Stepsg{\play}=\{n_0<n_1<\cdots\}$, one can define the sequence $(\play_i)_{i\geq
0}$ by setting ${\play}_i=v_{n_i}\cdots v_{n_{i+1}}$. Note that each of the $\Lambda_i$ is either a bump or a stair.
We designate $(\play_i)_{i\geq 0}$ as the **rounds factorisation** of $\play$ and we let $\pdscol{\play_i}$ denotes the largest colour in $\play_i$.



### Factorisation of a play in $\fgame$.

Recall that in $\farena$ only some edges have a relevant colour while all others get colour $0$. Hence, to represent a play, we
only keep the relevant colours of edges. More precisely, we only need to encode the
colours in $\{0,\dots,d\}$ that appears when simulating a bump: a play will be
represented as a sequence of vertices together with colours in
$\{0,\dots,d\}$ that correspond to (relevant) colours appearing on edges.

For any play in $\fgame$, a **round** is a factor between two
visits through vertices of the form
$(p,\alpha,\vect{R},c)$. We have the following possible forms for a round:


*  The round is of the form

$$
(p,\alpha,\vect{R},c)(p,\alpha,\vect{R},c,q,\beta)(p,\alpha,\vect{R},c,q,\beta,\vect{S})i (r,\alpha,\vect{R},\max(c,i,\pdscol{s}))
$$
 and
corresponds therefore to the simulation of a rule pushing $\beta$
followed by a sequence of moves that ends by popping $\beta$. Moreover $i$ is the largest colour encountered while $\beta$ was on the stack.

*  The round is of the form

$$
(p,\alpha,\vect{R},c)(p,\alpha,\vect{R},c,q,\beta)(p,\alpha,\vect{R},c,q,\beta,\vect{S})\pdscol{q}(q,\beta,\vect{S},\pdscol{q})
$$
 and
corresponds therefore to the simulation of a rule pushing a symbol $\beta$
leading to a new stack level below which the play will never go. We designate it as a **stair**.



For any play $\fplay=v_0v_1v_2\cdots$ in $\fgame$, we consider the
subset of indices corresponding to vertices of the form
$(p,\alpha,\vect{R},c)$. More precisely:
\begin{equation*}\Rounds{\fplay}=\{n\mid v_n=(p,\alpha,\vect{R},c),\ p\in Q,\
\alpha\in\Gamma,\\ \vect{R}\in(2^Q)^{d+1},\
0\leq c\leq d\}\end{equation*}

Therefore, the set $\Rounds{\fplay}$ induces a natural factorisation
of $\fplay$ into rounds.

````{prf:definition} NEEDS LABEL Rounds factorisation

For a (possibly finite) play $\fplay=v_0v_1v_2\cdots$, we call
**rounds factorisation** of $\fplay$, the (possibly finite) sequence
$(\fplay_i)_{i\geq 0}$ of rounds defined as follows. Let
$\Rounds{\fplay}=\{n_0<n_1<n_2<\cdots\}$, then for all $0\leq i<|\Rounds{\fplay}|$,
define $\fplay_i=v_{n_i}\cdots v_{n_{i+1}}$.

Therefore, for every $i\geq 0$, the first vertex in $\fplay_{i+1}$ equals
the last one in $\fplay_i$. Moreover,
$\fplay=\fplay_1\odot\fplay_2\odot\fplay_3\odot\cdots$, where
$\fplay_i\odot\fplay_{i+1}$ denotes the concatenation of $\fplay_i$
with $\fplay_{i+1}$ without its first vertex.Finally, the **colour** of a round is the unique colour in
$\{0,\dots,d\}$ appearing in the round.

````

In order to prove both implications of  {prf:ref}`10-thm:games` , we
build from a winning strategy for \Eve in one game a winning strategy
for her in the other game. The main argument to prove that the new
strategy is winning is to prove a correspondence between the
factorisations of plays in both games.


### Proof of the Direct Implication of  {prf:ref}`10-thm:games`



Assume that the configuration $(p_{in},\bot)$ is winning for \Eve in $\game$,
and let $\sigma$ be a corresponding winning strategy for her.

Using $\sigma$, we define a strategy $\fsigma$ for \Eve in
$\fgame$ from $(p_{in},\bot,(\emptyset,\dots,\emptyset),\pdscol{p_{in}})$.
This strategy stores a finite play in $\game$, that is an element in
$V^*$. This memory will
be denoted $\play$. At the beginning $\play$ is initialized to
the vertex $(p_{in},\bot)$. We first describe $\fsigma$, and then we
explain how $\play$ is updated. Both the strategy $\fsigma$ and the
update of $\play$, are described for a round.

\vspace{0.1cm}
**Choice of the move. ** Assume that the play is in some
vertex $(p,\alpha,\vect{R},c)$ for $p\in Q_\mEve$. The
move given by $\fsigma$ depends on $\sigma(\play)$:

*  If $\sigma(\play)=(r,pop)$, then \Eve goes to
$\ttrue$ (Proposition \ref{prop:par_dir_dep_paritexp} will
  prove that this move is always possible).

*  If $\sigma(\play)=(q,push(\beta))$, then \Eve goes to
$(p,\alpha,\vect{R},c,q,\beta)$.

In this last case, or in the case where $p\in Q_\mAdam$ and \Adam goes
to $(p,\alpha,\vect{R},c,q,\beta)$, we also have to explain
how \Eve behaves from
$(p,\alpha,\vect{R},c,q,\beta)$. She has to provide a
vector $\vect{S}\in (2^Q)^{d+1}$ that describes which states
can be reached if $\beta$ is eventually popped, depending on the largest colour visited in the
meantime. In order to define $\vect{S}$, \Eve considers the set of
all possible continuations of $\play\cdot(q,s\alpha\beta)$ (where
$(p,s\alpha)$ denotes the last vertex of $\play$) where she
respects her strategy $\sigma$. For each such play, she checks whether some
configuration of the form $(r,s\alpha)$ is visited after $\play\cdot
(q,s\alpha\beta)$, that is if the stack level of $\beta$ is eventually left. If it
is the case, she considers the first configuration $(r,s\alpha)$
appearing after $\play\cdot (q,\sigma\alpha\beta)$ and the largest
colour $i$ since
$\beta$ was on the stack.
For every $i\in\{0,\dots d\}$, $S_i$, is exactly the set of states
$r\in Q$ such that the preceding case happens. 
More formally, \begin{equation*}
\begin{split}
S_i=&\{r\mid \exists\ \pi\cdot(q,s\alpha\beta)v_0\cdots v_k(r,s\alpha)\cdots\text{ play in } \game  \text{ where \Eve respects } \sigma \text{ and s.t. }\\ & \sh(v_j)>|\sigma\alpha|,\ \forall j=0,\dots,k  \text{, and }\max(\{\pdscol{v_j}\mid j=0,\dots,k\}\cup\{\pdscol{q}\})=i\}
\end{split}
\end{equation*}Finally, we set $\vect{S}=(S_0,\dots,S_d)$ and \Eve moves to
$(p,\alpha,\vect{R},c,q,\beta,\vect{S})$.

\vspace{0.1cm}
**Update of $\play$. ** The memory $\play$ is updated after
each visit to a vertex of the form $(p,\alpha,\vect{R},c)$.
We have two cases depending on the kind of the last round:

*  The round is a bump, and therefore a bump
  of colour $i$ (where $i$ is the colour of the round) starting with some
  transition $(q,push(\beta))$ and ending in a state $r\in S_i$ was simulated. Let $(p,s\alpha)$ be the last vertex in
  $\play$. Then the memory becomes $\play$ extended by
  $(q,s\alpha\beta)$ followed by a sequence of moves, where \Eve
  respects $\sigma$, that ends by popping $\beta$ and reach
  $(r,s\alpha)$ while having $i$ as largest colour. By definition of $S_i$ such a sequence of moves always exists.

*  The round is a stair and therefore we have simulated a transition 
  $(q,push(\beta))$. If $(p,s\alpha)$ denotes the last vertex in $\play$, then the updated memory is $\play\cdot (q,s\alpha\beta)$.

Therefore, with any finite play $\fplay$ in $\fgame$ in which \Eve
respects her strategy $\fsigma$, is associated a finite play $\play$ in
$\game$. An immediate induction shows that \Eve respects $\sigma$ in
$\play$. The same arguments works for an infinite play $\fplay$, and the
corresponding play $\play$ is therefore infinite, starts from
$(p_{in},\bot)$ and \Eve respects $\sigma$ in that play. Therefore it is
a winning play.

The following proposition is a direct consequence of how $\fsigma$ was defined.

````{prf:proposition} NEEDS TITLE prop:par_dir_dep_paritexp
:label: prop:par_dir_dep_paritexp

Let $\fplay$ be a finite play in $\fgame$ that starts from
$(p_{in},\bot,(\emptyset,\dots,\emptyset),\pdscol{p_{in}})$,
ends in a vertex of the form $(p,\alpha,\vect{R},c)$,
and where \Eve respects $\fsigma$. Let $\play$ be the play associated with $\fplay$
built by the strategy $\fsigma$. Then the following holds:

1.  $\play$ ends in a vertex of the form $(p,s\alpha)$ for some $s\in\Gamma^*$.

2.  $c$ is the largest colour visited in $\play$ since $\alpha$ was pushed.

3.  Assume that $\play$ is extended, that \Eve keeps respecting
  $\sigma$ and that the next move after $(p,\sigma\alpha)$ is to some
  vertex $(r,\sigma)$. Then $r\in R_c$.

````


Proposition \ref{prop:par_dir_dep_paritexp} implies that the
strategy $\fsigma$ is well defined when it provides a move to
$\ttrue$. Moreover, one can deduce that, if \Eve respects $\fsigma$, $\ffalse$ is never reached.

For plays that do not visits $\ttrue$ nor $\ffalse$, using the definitions of $\farena$ and $\fsigma$, we easily deduce the
following proposition.


````{prf:proposition} NEEDS TITLE prop:toto
:label: prop:toto

Let $\fplay$ be an infinite play in $\fgame$ that starts from
$(p_{in},\bot,(\emptyset,\dots,\emptyset),$ $\pdscol{p_{in}})$,
and where \Eve respects $\fsigma$. Let $\play$ be the associated
play built by the strategy $\fsigma$, and let $(\play_i)_{i\geq 0}$ be its rounds factorisation. Let $(\fplay_i)_{i\geq 0}$ be
the rounds factorisation of $\fplay$. Then, for every $i\geq 1$ the
following hold:

1.  $\fplay_i$ is a bump if and only if $\play_i$ is a bump

2.  $\fplay_i$ has colour $\pdscol{\play_i}$.

````

\Cref{prop:toto} implies that for any infinite play
$\fplay$ in $\fgame$ starting from
$(p_{in},\bot,(\emptyset,\dots,\emptyset),$ $\pdscol{p_{in}})$
where \Eve respects $\fsigma$, the sequence of visited colours in $\fplay$ is
$(\pdscol{\play}_i)_{i\geq 0}$ for the corresponding play $\play$
in $\game$.
Hence, using  {prf:ref}`10-prop:trans_cond` we conclude that
$\fplay$ is winning if
and only if $\play$ is winning. As $\play$
is winning for \Eve, it follows that $\fplay$ is also winning for
her.


(10-subsec:strategy-pushdown)=
### Proof of the Converse Implication of  {prf:ref}`10-thm:games`

Note that in order to prove the converse implication of  {prf:ref}`10-thm:games` one could follow the direct implication and consider the point of view of \Adam. Nevertheless the proof we give here starts from a winning strategy for \Eve in $\fgame$ and deduces a strategy for her in $\game$: this induces a more involved proof but has the advantage to lead to an effective construction of a winning strategy for \Eve in $\game$ if one has an effective strategy for her in $\fgame$

Assume now that \Eve has a winning strategy $\fsigma$ in $\fgame$
from $(p_{\mathit in},\bot,(\emptyset,\dots,\emptyset),\pdscol{p_{in}})$.
Using $\fsigma$, we build a strategy $\sigma$ for \Eve in
$\game$ for plays starting from $(p_{\mathit in},\bot)$.

The strategy $\sigma$ uses, as memory, a stack $\Pi$, to store the complete
description of a play in $\fgame$. Recall here that a play in
$\fgame$ is represented as a sequence of vertices together with
colours in $\{0,\dots d\}$. Up to coding we can assume that we distinguish for free between stairs and bumps for transitions from vertices of the form $(p,\alpha,\vect{R},c,q,\beta,\vect{S})$.

The stack alphabet of $\Pi$ is the set of vertices of 
$\farena$ together with the colours $\{0,\dots,d\}$. In the following, $top(\Pi)$ will denote
the top stack symbol of $\Pi$ while $StCont(\Pi)$ will be
the word obtained by reading $\Pi$ from bottom to top (without
considering the bottom-of-stack symbol of $\Pi$). In any play
where \Eve respects $\sigma$, $StCont(\Pi)$ will be a play
in $\fgame$ that starts from $(p_{\mathit
in},\bot,(\emptyset,\dots,\emptyset),\pdscol{p_{in}})$ and where
\Eve respects her winning strategy $\fsigma$. Moreover, for any
play $\play$ where \Eve respects $\sigma$, we will always have that
$top(\Pi)=(p,\alpha,\vect{R},c)$ if and only if
the current configuration in $\play$ is of the form
$(p,s\alpha)$. Finally, if \Eve keeps respecting $\sigma$, and
if $\alpha$  is eventually popped the configuration reached
will be of the form $(r,s)$ for some $r\in R_i$, where
$i$ is the largest visited colour since $\alpha$  was on the stack.
Initially, $\Pi$ only contains $(p_{in},\bot,(\emptyset,\dots,\emptyset),\pdscol{p_{in}})$.

In order to describe $\sigma$, we assume that we are in some
configuration $(p,s\alpha)$ and that
$top(\Pi)=(p,\alpha,\vect{R},c)$. We first
describe how \Eve plays if $p\in Q_\mEve$, and then we explain how
the stack is updated.

*  **Choice of the move.** Assume that $p\in Q_\mEve$ and
that \Eve has to play from some vertex $(p,s\alpha)$. For
this, she considers the value of $\fsigma$ on $StCont(\Pi)$.

If it is a move to $\ttrue$, \Eve plays a transition
$(r,pop)$ for some state $r\in R_c$. \Cref{10-ini:lemma:games:ReturningSets_paritexp} will prove that such
an $r$ always exists.

If the move given by $\fsigma$ is to go to some vertex
$(p,\alpha,\vect{R},c,q,\beta)$, then \Eve applies
the transition $(q,push(\beta))$.

*  **Update of $\Pi$.** Assume that the last move,
played by \Eve or \Adam, was to go from $(p,s\alpha)$ to some
configuration $(r,s)$. The update of $\Pi$ is illustrated
by {numref}`10-fig:mise_a_jour_pile_strategie` and
explained in what follows. \Eve pops in $\Pi$ until she finds
some configuration of the form
$(p',\alpha',\vect{R'},c',p'',\alpha,\vect{R})$
that is part of a stair. This
configuration is therefore in the stair that simulates the pushing
of $\alpha$ onto the stack. \Eve updates $\Pi$ by pushing
$c$ in $\Pi$ followed by
$(r,\alpha',\vect{R'},\max(c',c,\pdscol{r}))$.

Assume that the last move, played by \Eve or \Adam, was to go from
$(p,s\alpha)$ to some configuration $(q,s\alpha\beta)$, and let
$(p,\alpha,\vect{R},c,q,\beta,\vect{S})=\fsigma(StCont(\Pi)\cdot(p,\alpha,\vect{R},c,q,\beta))$.
Intuitively, $\vect{S}$ describes which states \Eve can force a play to
reach if $\beta$ is eventually popped. \Eve updates $\Pi$ by
successively pushing
$(p,\alpha,\vect{R},c,q,\beta)$,
$(p,\alpha,\vect{R},c,q,\beta,\vect{S})$, and
$(q,\beta,\vect{S},\pdscol{q})$.

%\scalebox{1}%\drawline(0,0)(100,0) \drawline(0,0)(0,60) \gasset{AHnb=0}%\drawline(10,10)(15,20)\drawline(15,20)(20,30)%\drawqbezier(40,40,47.5,55,55,40)%\drawline[AHnb=0,dash={0.5 0.5}0](20,30)(80,30)%\drawcircle[Nfill=y](30,30,2)\drawcircle[Nfill=y](40,40,2)\drawcircle[Nfill=y](80,40,2)%$(p'',s\alpha'')$}\put(83,40){\small%%\drawline(0,-4)(0,-55)%\put(-7,-26){\rotatebox{270}{$\Pi$}}%\put(7,-30){$\cdots$}%\drawline(44,-4)(44,-55)\put(38,-10){\rotatebox{270}{\scriptsize$(p',\alpha',\vect{R'},c',p'',\alpha)$}}%\drawline(60,-4)(60,-55)\put(54,-9){\rotatebox{270}{\scriptsize$(p'',\alpha'',\vect{R},\pdscol{p''})$}}%\drawline[AHnb=0,dash={0.2 0.5}0](30,30)(28,0)%\drawline[AHnb=0,dash={0.2 0.5}0](30,30)(36,0)%%\drawline[AHnb=0,dash={0.2 0.5}0](52,0)(52,-4)%\drawline[AHnb=0,dash={0.2 0.5}0](60,0)(60,-4)%\drawline(80,-4)(80,-55)%%\drawline[AHnb=0,dash={0.2 0.5}0](80,0)(80,-4)%\drawline[AHnb=0,dash={0.2 0.5}0](87,0)(87,-4)%\put(68,-30){$\cdots$}%\put(83,40){\small $(p,s\alpha)$} \put(100,40){\small min. col. = $c$} \put(85,30){\small min. col. = $c'$}%%%%{\unitlength=2.2pt\begin{picture}(150,110)(0,-60)%\drawqbezier(0,10,5,20,10,10)%\drawqbezier(20,30,25,40,30,30) \drawline(30,30)(40,40)%\drawqbezier(55,40,62.5,70,70,40) \drawqbezier(70,40,75,50,80,40)%0.5}0](20,30)(90,30) \drawline[AHnb=0,dash={0.5%\drawcircle[Nfill=y](30,30,2)\drawcircle[Nfill=y](90,30,2)%\put(31,19){$\sigma=\sigma'\alpha'$}%%\drawline(0,-4)(0,-65)%\put(-7,-32){\rotatebox{270}{$\Pi$}}%\put(7,-34){$\cdots$}%\drawline(50,-4)(50,-65)\put(42,-14){\rotatebox{270}{\scriptsize$(p',\alpha',\vect{R'},c',p'',\alpha'')$}}%\drawline(70,-4)(70,-65)\put(79,-34){\rotatebox{270}{\scriptsize$c$}}%\drawline[AHnb=0,dash={0.2 0.5}0](30,30)(28,0)%\drawline[AHnb=0,dash={0.2 0.5}0](30,30)(36,0)%%\drawline(90,-4)(90,-65) \drawline(97,-4)(97,-65)%%\drawline[AHnb=0,dash={0.2 0.5}0](90,0)(90,-4)%\drawline[AHnb=0,dash={0.2 0.5}0](97,0)(97,-4)%\put(85,23){\small min. col.$=$}\put(85,18){\small $\max(c',c,\pdscol{r})$}%%\vspace{0.5cm} \caption{Updating the strategy's stack
\begin{figure}
\begin{center}
\includegraphics{./pile_update.pdf}
\caption{Updating the strategy's stack $\Pi$}\label{10-fig:mise_a_jour_pile_strategie}
\end{center}
\end{figure}

The following lemma gives the meaning of the information stored
in $\Pi$.


````{prf:lemma} NEEDS TITLE 10-ini:lemma:games:ReturningSets_paritexp
:label: 10-ini:lemma:games:ReturningSets_paritexp

Let $\play$ be a finite play in $\game$, where \Eve respects
$\sigma$, that starts from $(p_{\mathit in},\bot)$ and
that ends in a configuration $(p,s\alpha)$. We have the
following facts:



1.  $top(\Pi)=(p,\alpha,\vect{R},c)$ with
$\vect{R}\in(2^Q)^{d+1}$ and $0\leq c\leq d$.

2.  $StCont(\Pi)$ is a finite play in $\fgame$ that starts
from $(p_{\mathit in},\bot,(\emptyset,\dots,\emptyset),\pdscol{p_{in}})$,
that ends with $(p,\alpha,\vect{R},c)$ and where
\Eve respects $\fsigma$.

3.  $c$ is the largest colour visited since $\alpha$ was pushed.

4.  If $\play$ is extended by some move
that pops $\alpha$, the configuration $(r,s)$ that is reached
is such that $r\in R_c$.

````



````{admonition} Proof
:class: dropdown tip

The proof goes by induction on $\play$. We first show that the
last point is a consequence of the second and third points. To aid readability, one can refer to 
{numref}`10-fig:mise_a_jour_pile_strategie`. Assume that
the next move after $(p,s\alpha)$ is to apply a transition
$(r,pop)\in\Delta(p,\alpha)$. The second point implies that
$(p,\alpha,\vect{R},c)$ is winning for \Eve in
$\fgame$. If $p\in Q_\mEve$, by definition of $\sigma$, there is
some edge from that vertex to $\ttrue$, which means that
$r\in R_c$ and allows us to conclude. If $p\in Q_\mAdam$, note that there is no
edge from $(p,\alpha,\vect{R},c)$ (winning position
for \Eve) to the (losing) vertex $\ffalse$. Hence we
conclude in the same way.

Let us now prove the other points. For this, assume that the
result is proved for some play $\play$, and let $\play'$ be an
extension of $\play$. We have two cases, depending on how $\play'$
extends $\play$:

*  $\play'$ is obtained by applying a push transition. The result is trivial in that case.

*  $\play'$ is obtained by applying a pop transition. Let
$(p,s\alpha)$ be the last configuration in $\play$, and let
$\vect{R}$ be the last vector component in $top(\Pi)$ when
in configuration $(p,s\alpha)$. By the induction
hypothesis, it follows that $\play'=\play\cdot(r,s)$ with
$r\in R_c$. Considering how $\Pi$ is updated, and
using the fourth point, we easily deduce that the new strategy
stack $\Pi$ is as desired (one can have a look at 
{numref}`10-fig:mise_a_jour_pile_strategie` for more
intuition).

````

Actually, we easily deduce a more precise result.

````{prf:lemma} NEEDS TITLE 10-lemme:toto_paritexp
:label: 10-lemme:toto_paritexp

Let $\play$ be a finite play in $\game$ starting from
$(p_{in},\bot)$ and where \Eve respects $\sigma$. Let $(\play_i)_{i\geq 0}$ be its rounds factorisation. Let
$\play=StCont(\Pi)$, where $\Pi$ denotes the strategy's
stack in the last vertex of $\play$. Let
$(\play_i)_{i=0,\dots,k}$ be the rounds factorisation of $\play$.
Then the following holds:

*  $\play_i$ is a bump if and only if ${\play}_i$ is a bump.

*  $\play_i$ has colour $\pdscol{\play}_i$.

````

Both \Cref{10-ini:lemma:games:ReturningSets_paritexp} and
\Cref{10-lemme:toto_paritexp} are for finite plays. A version for
infinite plays would allow us to conclude. Let $\play$
be an infinite play in $\game$. We define an
infinite version of $\play$ by considering the limit of the stack
contents $(StCont(\Pi_i))_{i\geq 0}$ where $\Pi_i$ is the
strategy's stack after the first $i$ moves in $\play$.%See {cite}`Ser04` for similar constructions. 
It is easily seen that such a limit
always exists, is infinite and corresponds to a play won by \Eve in $\fgame$.
Moreover the results of \Cref{10-lemme:toto_paritexp} apply.

Let $\play$ be a play in $\game$ with initial
vertex $(p_{in},\bot)$, and where \Eve respects $\sigma$,
and let $\play$ be the associated infinite play in $\fgame$.
Therefore $\play$ is won by \Eve. Using \Cref{10-lemme:toto_paritexp} and  {prf:ref}`10-prop:trans_cond`,
we conclude, as in the direct implication that $\play$ is
winning.


(10-subsec:computing-all)=
## Solving the Game and Computing the Winning Region and Strategy

Combining  {prf:ref}`10-thm:games` and  {prf:ref}`10-thm:regularity-wr`, we obtain the following upper bounds regarding the problem of deciding the winner in a pushdown game and on constructing a finite state automaton recognising the winning region (in the sense of \Cref{10-rk:automata-winning-region}).

````{prf:theorem} NEEDS TITLE 10-thm:solving-upper-bound
:label: 10-thm:solving-upper-bound

Let $\game$ be a parity pushdown game using colours $\{0,\dots,d-1\}$ and played on an arena generated by a pushdown system with $n$ control states and with a stack alphabet of size $m$. Then the following holds

1. [(i)] One can construct in time $\mathcal{O}(m^d 2^{nd^2})$ a deterministic finite state automaton with $2^n$ states recognising the winning region of \Eve in $\game$.

2. [(ii)] One can decide in time $\mathcal{O}(m^d 2^{nd^2}+|s|)$, for any configuration $(p,\bot s)$, whether it is winning for \Eve in $\game$. 

````



````{admonition} Proof
:class: dropdown tip

Consider the parity game $\fgame$ from Subsection {ref}`10-subsec:computing-profiles`. Let $n=|Q|$ and $m=|\Gamma|$. Then $\fgame$ is played on an arena with $\mathcal{O}(n m 2^{2nd})$ vertices and it uses $d$ colours. Hence\OS{Ajouter ref a un chapitre precedent}, computing the winning region of this later game can be achieved in time $\mathcal{O}(m^d 2^{nd^2})$.\OS{Bien vérifier que je ne me trompe pas dans les calculs ici}

Using,  {prf:ref}`10-thm:games`, it follows that the set of profiles can be computed in time $\mathcal{O}(m^d 2^{nd^2})$, and by  {prf:ref}`10-thm:regularity-wr` (and its proof) we know that we can construct (in the same time complexity) a deterministic finite state automaton with $2^n$ states recognising the winning region $W_\mEve$.

The upper bound for the second item simply follows from the fact that running a deterministic automaton on a word is performed in linear time in the length of the word.

````

Regarding lower bound, the following result shows that the previous upper bound is optimal. Note that it is enough to consider a reachability objective.

````{prf:theorem} NEEDS TITLE 10-thm:solving-lower-bound
:label: 10-thm:solving-lower-bound

Let $\game$ be a reachability pushdown game. Then the following problem is hard for \EXP: decide whether $(p,\bot)$ is winning for \Eve in $\game$.

````


````{admonition} Proof
:class: dropdown tip

The lower bound is established by reducing the halting problem for alternating linear space bounded Turing machine. 

Consider an alternating linear space bounded Turing machine $\mathcal{M}$. We can safely assume that $\mathcal{M}$ has a unique tape and on an input of size $n$ it uses at most $n$ tape squares. Let $Q=Q_\exists\cup Q_\forall$ be the states of the Turing machine where $Q_\exists$ are the existential states and $Q_\forall$ are the universal ones; we let $q_a$ be the (unique) accepting state of the machine. Call $A$ the tape alphabet and let $T\subseteq Q\times A\times Q\times A\times \{\leftarrow,\rightarrow\}$ the transition table of the machine. A configuration of $\mathcal{M}$ is a word $C$ of the form $uqv\in A^*QA^*$ of length $n+1$ (the meaning being that $\mathcal{M}$ is in state $q$ and that the tape contains $uv$).

We now informally describe a two-player game simulating a computation of $\mathcal{M}$ and argue that it can be encoded as a reachability pushdown game. Think first of $\mathcal{M}$ as being non-deterministic, i.e. $Q_\forall=\emptyset$ and call $C_0$ the initial configuration. A run of $\mathcal{M}$ can be encoded as a word $r=C_0\sharp t_0\sharp C_1\sharp t_1\sharp C_2\sharp t_2\sharp C_3\sharp\cdots$ where for every $i\geq 0$, $t_i\in T$ is a transition of $\mathcal{M}$ that can be applied in configuration $C_i$ and $C_{i+1}$ is the configuration reached from $C_i$ by applying $t_i$; it is accepting if some $t_i$ is a transition to the accepting state of $\mathcal{M}$. A way to encode such $r$ with a pushdown game is that \Eve pushes symbols in the stack to describe $r$ and to use the control states to impose some structural constraints on the sequence of pushed symbols:

*  The first pushed configuration is $C_0$.
*  Every configuration pushed has the right form, i.e. it is a word in $A^*QA^*$ of length $n+1$. 
*  Every configuration $C$ is followed by some pattern $\sharp t\sharp$ and the transition $t$ in this pattern can be applied from the configuration. This is ensured by storing in the control state of the pushdown process the state of $\mathcal{M}$ and the content of the currently read cell in $C$. 

To ensure these properties a linear number of control states suffices. 

Of course, this is not enough because \Eve could cheat and push a configuration $C_{i+1}=x_0x_1\cdots x_n$ which is not the successor of $C_i=y_0y_1\cdots y_n$ by $t_i$. To avoid this, after she described a configuration (say $C_{i+1}$) and pushed the $\sharp$ symbol, \Adam can stop the simulation and claim a mistake by indicating the index $k$ of a wrong update in $C_{i+1}$. If so, the game goes to a special mode where the following is performed:

*  the $\sharp$ symbol is popped as well as the next $n-i$ symbols;
*  the current top symbol is $x_i$ and it is stored in the control state of the pushdown process
*  the players keep popping until a $\sharp$ symbol is seen and the next symbol $t_i$ is also stored in the control state
*  then the players pop $n-i-1$ symbols and then considering the next three symbols they can check whether the update was correct or not (there are several cases depending whether the reading tape was at distance at most $1$ of the position of index $i$).

Again, this can be implemented thanks to a linear number of control states in the pushdown process.

In case \Eve cheats the play loops in a non-final sink configuration. Otherwise it loops in a final sink configuration.

Now, if the Turing Machine $\mathcal{M}$ is alternating, the only difference is that the choice of the transition $t_i$ is made by \Eve if the control state in $C_i$ is existential and by \Adam if it is universal. The rest of the game is unchanged (in particular \Eve is still in charge of describing all configurations, regardless of whom picks the transition).

It is then immediate to check that \Eve has a winning strategy in this game if and only if the Turing machine accepts from its initial configuration.

````

%\label{10-subsec:pushdown-strat}

(10-subsec:regular-strat)=
## Regular Winning Strategies

In \ref{10-subsec:strategy-pushdown}, we have seen that the proof of  {prf:ref}`10-thm:games` shows that a winning strategy for \Eve (when it exists) can be implemented by pushdown automaton that reads the pushdown system transitions chosen by the players and indicates \Eve moves by a function depending only of the current control state and the top-most stack symbol of the strategy automaton. 

In this section, we present a different reduction whose aim is to be able to compute a positional winning strategy for \Eve which furthermore can be implemented by a finite state automata. As an added benefit, we will see that this strategy is uniform in the sense that it is winning from every vertex of the winning region of \Eve.

For the rest of this section, we fix a parity pushdown game $\game$ played on an arena $\arena = (G,\VE,\VA)$ generated by a pushdown system $\PDS = (Q,Q_{\mEve}, Q_{\mAdam}, \Gamma,\Delta)$. We also let $V=\VE\cup\VA$ and we let the colours used in the game be $\{0,\dots,d\}$.

A **summary** is a triple $(p,c,q)\in Q\times \{0,\dots,d\} \times Q$. A set $S$ of summaries is **complete** if $(p_1,c_1,q),(q,c_2,p_2)\in S$ implies that $(p_1,\max(c_1,c_2),p_2)\in S$; it is winning if $(p,c,p)\in S$ implies that $c$ is even. For $R\subseteq Q$, a set of $R$-summaries is a set of summaries $S\subseteq R\times\{0,\dots,d\}\times R$. Associated with some stack content $s$, a summary $(p,c,q)$ aims to encode the existence of a sequence of moves from $(p,s)$ to $(q,s)$ where the top symbol of $s$ is never removed and where $c$ is the largest colour visited in the sequence. 

Let $P\subseteq Q_\mEve$ and $\gamma\in\Gamma_\bot$. A **$(P,\gamma)$-local strategy** for \Eve is a partial function $\sigma_\gamma:P\rightarrow {Q\times\{\pop,\push{\gamma}\mid \gamma\in\Gamma\}}$ such that $\sigma_\gamma(p)\in \Delta(p,\gamma)$ for all $p\in P$. Equivalently it is a selection for every state in $P$ of a consistent transition of $\PDS$ when the top symbol is $\gamma$. For a subset $R\subseteq Q$, we say that $\sigma_\gamma$ **pops in $R$** if $\sigma_\gamma(q)=(r,pop)$ implies $r\in R$.
We say that a $(P,\gamma)$-local strategy is **safe** if $\sigma_\gamma(p)=(q,push(\alpha))$ implies that $P\in\mathcal{R}(q,\alpha)$. From now on, we only allowed safe local strategies.
Let $R\subseteq Q$ and $\gamma\in\Gamma$. We associate with $(R,\gamma)$ the subset 
$$
W(R,\gamma)=\{q\mid R\in\mathcal{R}(q,\gamma)\}
$$
 By a small abuse of notation we let $W(\emptyset,\bot) = \{q\mid (q,\bot)\in W_\mEve\}$. Remark that for every $q\in W(R,\gamma)\cap Q_\mAdam$ and $(r,pop)\in \Delta(q,\gamma)$ one has $r\in R$.

We now define a new game $\hgame$ played on a finite arena and equipped with an $\omega$-regular objective. We start by an informal description of plays in $\hgame$ and later formally describe the arena and the objective. 

A play in $\hgame$ begins by an initialisation phase:

*  The play starts in $(\bot, R)$ where $R=W(\emptyset,\bot)=\{q\mid (q,\bot)\in W_\mEve\}$.
*  From there, \Eve chooses $\sigma_\bot$ an $(R,\bot)$-local strategy and a set of $R$-summaries that is both complete and winning. Then, the play goes to $(\bot,R,\sigma_\bot,S)$.

Then, the plays goes for rounds of the following form:

*  From a vertex $(\gamma,R,\sigma_\gamma,S)$, where $\gamma\in \Gamma$, $R\subseteq Q$, $\sigma_\gamma$ is an $(R,\gamma)$-local strategy and $S$ is a set of $R$-summaries, \Eve chooses for every $\alpha\in\Gamma$ a $(W(R,\alpha),\alpha))$-local strategy $\sigma_\alpha$ that pops in $R$ and a set of $ W(R,\alpha)$-summaries $S_\alpha$ that is both complete and winning. The play then goes in $(\gamma,R,\sigma_\gamma,S,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma})$.
*  Then, \Adam chooses some $\alpha$ in $\Gamma$ and the play goes in $(\alpha,W(R,\alpha),\sigma_\alpha,S_\alpha)$.


Consider a tuple $(\gamma,R,\sigma_\gamma,S,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma})$ where $\gamma\in\Gamma$, $R\subseteq Q$, $\sigma_\gamma$ is an $(R,\gamma)$-local strategy, $S$ is a set of $R$-summaries, and, for every $\alpha\in\Gamma$, $\sigma_\alpha$ is a $(W(R,\alpha),\alpha))$-local strategy $\sigma_\alpha$ that pops in $R$ and $S_\alpha$ is a set of $W(R,\alpha)$-summaries that is both complete and winning. The tuple $(\gamma,R,\sigma_\gamma,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma})$ is **consistent** if, for every $(p,r)\in R^2$, one has $(p,\max(\pdscol{p},c,\pdscol{r}),r)\in S$ as soon as we are in one of the following  two situations (the second one being the degenerated version of the first one).

*  There exists $\alpha\in\Gamma$, $(q,c,q')\in S_\alpha$ such that

* [(i)] either $p\in Q_\mEve$ and $(q,\push{\alpha})=\sigma_\gamma(p)$, or $p\in Q_\mAdam$ and $(q,\push{\alpha})\in \Delta(p,\gamma)$, and 
* [(ii)] either $q'\in Q_\mEve$ and $(r,\pop)=\sigma_\alpha(q')$, or $q'\in Q_\mAdam$ and $(r,\pop)\in \Delta(q',\alpha)$.

Intuitively, if with state $p$ and top symbol $\gamma$ one can push $\alpha$ and go to state $q$ from which we know that we can later go back to the same stack content with state $q'$ and maximal colour $c$, and finally pop $\gamma$ and end in state $r$, then we conclude that we can go from $p$ to $r$ while seeing $\max(\pdscol{p},c,\pdscol{r})$ as the maximal colour.
*  There exists $\alpha\in\Gamma$ and $q\in W(R,\alpha)$ such that

* [(i)] either $p\in Q_\mEve$ and $(q,\push{\alpha})=\sigma_\gamma(p)$, or $p\in Q_\mAdam$ and $(q,\push{\alpha})\in \Delta(p,\gamma)$, and 
* [(ii)] either $q\in Q_\mEve$ and $(r,\pop)=\sigma_\alpha(q)$, or $q\in Q_\mAdam$ and $(r,\pop)\in \Delta(q,\alpha)$,
* [(iii)] $c=\pdscol{q}$.

Intuitively, if with state $p$ and top symbol $\gamma$ one can push $\alpha$ and go to state $q$ and directly pops $\gamma$ and end in state $r$, then we conclude that we can go from $p$ to $r$ while seeing $\max(\pdscol{p},\pdscol{q},\pdscol{r})$ as the maximal colour.

In the previous informal description, the only allowed choices for $(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma}$ are those that leads to consistent tuples.
Formally, we define the arena $\harena$ as follows:

*  There is a special initial vertex $(\bot,W(\emptyset,\bot))$ controlled by \Eve.
*  For every $\gamma\in \Gamma_\bot$, every $R\subseteq Q$, every $(R,\gamma)$-local strategy $\sigma_\gamma$ and every set of $R$-summaries that is both complete and winning there is a vertex $(\gamma,R,\sigma_\gamma,S)$ controlled by \Eve.
*  There is a vertex $(\gamma,R,\sigma_\gamma,S,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma})$ controlled by \Adam  for every consistent such tuple.
*  From every vertex $(\gamma,R,\sigma_\gamma,S)$ there is an edge to every vertex of the form $(\gamma,R,\sigma_\gamma,S,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma})$.
*  From every vertex $(\gamma,R,\sigma_\gamma,S,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma})$ there is an edge to $(\alpha,W(R,\alpha),\sigma_\alpha,S_\alpha)$ for every $\alpha\in\Gamma$.


Hence, a play in $\hgame$ from the initial vertex $(\gamma_0,R_0)=(\bot,W(\emptyset,\bot))$ is a sequence of vertices 
\begin{equation*}
\begin{split}
\hplay = & (\bot,R_0)(\gamma_0,R_0,\sigma_0,S_0)(\gamma_0,R_0,\sigma_0,S_0,(\sigma^1_\alpha,S^1_\alpha)_{\alpha\in\Gamma}))(\gamma_1,R_1,\sigma_1,S_1)\\ &\quad (\gamma_1,R_1,\sigma_1,S_1,(\sigma^2_\alpha,S^2_\alpha)_{\alpha\in\Gamma}))(\gamma_2,R_2,\sigma_2,S_2)\cdots
\end{split}
\end{equation*}
with $\sigma_i=\sigma_{\gamma_i}^i$ and $S_i=S_{\gamma_i}^i$ for every $i\geq 1$.

It is loosing for \Eve if there exists $(q_i)_{i\geq 0},(p_i)_{i\geq 0}\in Q^{\mathbb{N}}$ and $(c_i)_{i\geq 0}\in \{0,\dots,d\}^{\mathbb{N}}$ such that $\limsup(c_i)_{i\geq 0}$ is odd and for every $i\geq 0$ one has

*  $(q_i,c_i,p_i)\in S_i$; and 
*  either $p_i\in Q_\mEve$ and $(q_{i+1},\push{\gamma_{i+1}})=\sigma_i(p_i,\gamma_i)$ or $p_i\in Q_\mAdam$ and $(q_{i+1},\push{\gamma_{i+1}})\in \Delta((p_i,\gamma_i)$.

Note that it is easily seen that the previous objective is an $\omega$-regular one. 

We denote by $\hgame$ the previous game. The following result relies on the connection between $\hgame$ and the original pushdown game $\game$.

````{prf:theorem} NEEDS TITLE 10-thm:hgame
:label: 10-thm:hgame

\Eve has a finite memory winning strategy in $\hgame$ from $(\bot,W(\emptyset,\bot))$.

````


````{admonition} Proof
:class: dropdown tip

As the game $\hgame$ is played on a finite arena and equipped with an $\omega$-regular objective, it suffices to prove that \Eve has a winning strategy from $(\bot,W(\emptyset,\bot))$.

Consider a **positional** strategy $\sigma$ for \Eve in $\game$ that is winning on the whole winning region $W_\mEve$. Note that existence of positional winning strategies is ensure because $\game$ is a parity game.

Let $s\in\bot\Gamma^*$ be some stack content. We define a set of summaries $S_s^\sigma$ associated with $s$ (and $\sigma$) by letting 
\begin{equation*}
\begin{split}
S^s_\sigma=& \{(p,c,q) \mid \exists \play=v_0\cdots v_k \text{ with $k>0$, $v_0=(p,s)$, $v_k=(q,s)$, $\sh(v_i)\geq \sh(v_0)$ }\\  & \text{for all $0\leq i\leq k$, and such that \Eve respects $\sigma$ in $\play$ and $c$ is the largest}\\ & \text{colour visited in $\play$}\}
\end{split}
\end{equation*}
and, for every $(p,c,q)\in S^s_\sigma$, we select a play $\play_{(p,c,q)}^s$ that witnesses $(p,c,q)\in S_\sigma^s$. 

Using $\sigma$ we define a strategy $\hsigma$ for \Eve in $\hgame$ and we later argue that it is winning for her from $(\bot,W(\emptyset,\bot))$.
*  At the beginning of the play in $(\bot,R)$ with $R=W(\emptyset,\bot)$, \Eve moves to $(\bot,R,\sigma_\bot,S)$ where $\sigma_\bot(r) = \sigma((r,\bot))$ for every $r\in R$, and $S=S_\sigma^\bot$
*  Assume the current play is 
$$
\hplay=(\bot,R_0)(\gamma_0,R_0,\sigma_0,S_0)(\gamma_0,R_0,\sigma_0,S_0,(\sigma^1_\alpha,S^1_\alpha)_{\alpha\in\Gamma}))(\gamma_1,R_1,\sigma_1,S_1)\cdots (\gamma_k,R_k,\sigma_k,S_k)
$$
 and let $s_{\hplay} = \gamma_0\cdots\gamma_k$. Then, \Eve goes to $(\gamma_k,R_k,\sigma_k,S_k,(\sigma^{k+1}_\alpha,S^{k+1}_\alpha)_{\alpha\in\Gamma}))$ with $\sigma^{k+1}_{\alpha}(r) = \sigma((r,s_{\hplay}\alpha))$ for every $r$ such that $(r,s_{\hplay}\alpha)\in W_\mEve$ and $S^{k+1}_\alpha = S_\sigma^{\hplay\alpha}$. 

Assume now by contradiction that $\hsigma$ is not winning and consider a loosing play \begin{equation*}
\begin{split}
\hplay = & (\bot,R_0)(\gamma_0,R_0,\sigma_0,S_0)(\gamma_0,R_0,\sigma_0,S_0,(\sigma^1_\alpha,S^1_\alpha)_{\alpha\in\Gamma}))(\gamma_1,R_1,\sigma_1,S_1)\\ &\quad (\gamma_1,R_1,\sigma_1,S_1,(\sigma^2_\alpha,S^2_\alpha)_{\alpha\in\Gamma}))(\gamma_2,R_2,\sigma_2,S_2)\cdots
\end{split}
\end{equation*}

Hence, there exists $(q_i)_{i\geq 0},(p_i)_{i\geq 0}\in Q^{\mathbb{N}}$ and $(c_i)_{i\geq 0}\in \{0,\dots,d\}^{\mathbb{N}}$ such that $\limsup(c_i)_{i\geq 0}$ is odd and for every $i\geq 0$ one has

*  $(q_i,c_i,p_i)\in S_i$; and 
*  either $p_i\in Q_\mEve$ and $(q_{i+1},\push{\gamma_{i+1}})=\sigma_i(p_i,\gamma_i)$ or $p_i\in Q_\mAdam$ and $(q_{i+1},\push{\gamma_{i+1}})\in \Delta((p_i,\gamma_i)$.

Now, consider the play 
$$
\play=\play^{\gamma_0}_{(q_0,c_0,p_0)}\play^{\gamma_0\gamma_1}_{(q_1,c_1,p_1)}\play^{\gamma_0\gamma_1\gamma_2}_{(q_2,c_2,p_2)}\play^{\gamma_0\gamma_1\gamma_2\gamma_3}_{(q_3,c_3,p_3)}\cdots
$$

Then it is easily seen by definition that $\play$ is loosing (because $\hplay$ is) while \Eve respects her winning strategy $\sigma$, which leads a contradiction and concludes the proof.

````

Following  {prf:ref}`10-thm:hgame`, fix a finite memory winning strategy $\hsigma$ for \Eve in $\hgame$ from $(\bot,W(\emptyset,\bot))$. Using $\hsigma$ we define a positional strategy $\sigma$ for \Eve in $\game$. 

First, we inductively associate, with any word $s\in \bot \Gamma^*$, a finite play $\hplay_s$ in $\hgame$ where \Eve respects her strategy $\hsigma$: 

*  If $s=\bot$, we let $\hplay_s = (\bot,W(\emptyset,\bot))(\bot,W(\emptyset,\bot),\sigma_\bot,S)$ where $(\bot,W(\emptyset,\bot),\sigma_\bot,S) = \hsigma((\bot,W(\emptyset,\bot)))$.
*  If $s=s'\beta$ for some $\beta\in\Gamma$, let $\hsigma(\hplay_{s'}) = (\gamma,R,\sigma_\gamma,S,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma}))$ and define 
$$
\hplay_{s}= \hplay_{s'}(\gamma,R,\sigma_\gamma,S,(\sigma_\alpha,S_\alpha)_{\alpha\in\Gamma}))(\beta,W(R,\beta),\sigma_\beta,S_\beta).
$$


Now, for every configuration $(q,s)$ with $q\in Q_\mEve$, we let $(\gamma,R,\sigma_\gamma,S_\gamma)$ be the last vertex in $\hplay_s$ and if $q\in R$ we let $\sigma((q,s)) = \sigma_\gamma(q)$ and otherwise we pick an arbitrary transition for $\sigma((q,s))$ as $(q,s)$ will be a loosing position for \Eve (see \Cref{10-proposition:R-winning-states} below).

The following is a direct rephrasing of the proof of  {prf:ref}`10-thm:regularity-wr`.

````{prf:proposition} NEEDS TITLE 10-proposition:R-winning-states
:label: 10-proposition:R-winning-states

Let $s\in\Gamma^*\bot$ and let $(\gamma,R,\sigma_\gamma,S)$ be the last vertex in $\hplay_s$. Then $R=\{p\mid (p,s)\in W_\mEve\}$.

````

The following is  consequence of \Cref{10-proposition:R-winning-states} and of the requirement that in a vertex $(\gamma,R,\sigma_\gamma,S)$ \Eve should only propose $(W(R,\alpha),\alpha)$-local strategies that pops in $R$.

````{prf:proposition} NEEDS TITLE 10-proposition:stays-in-winning-region
:label: 10-proposition:stays-in-winning-region

Let $\play$ be an infinite play in $\game$ starting from some winning position for \Eve and where \Eve respects strategy $\sigma$. Then any vertex visited in $\play$ is a winning position for \Eve.

````


````{admonition} Proof
:class: dropdown tip

It suffices to prove that the property is true for the second vertex in $\play$ (and then conclude by induction as the strategy $\play$ is positional). If the initial vertex belongs to \Adam, then by definition all possible successors are winning for \Eve (otherwise the initial one would be winning for \Adam as well by prefix independence of the parity objective). If the initial vertex is controlled by \Eve there are two cases depending whether her move is to push or pop a symbol. If the move is to pop a symbol then, by construction, the state reached belong to $R$ where the last vertex in $\hplay_s$ is $(\gamma,R,\sigma_\gamma,S)$, if $s$ denote the stack content after popping: hence, by \Cref{10-proposition:R-winning-states} we conclude. If the move is to push a symbol then the result follows directly from the fact that we only consider safe local strategies and by \Cref{10-proposition:R-winning-states}.

````

The following is an easy consequence of the notion of consistent tuples.

````{prf:proposition} NEEDS TITLE 10-proposition:bumps
:label: 10-proposition:bumps

Let $(p,s)\in V$ and let $(\gamma,R,\sigma_\gamma,S)$ be the last vertex in $\hplay_s$. Assume that $p\in R$. Let $\play=v_0\cdots v_k$ be a finite play in $\game$ such that $k> 0$, $v_0=(p,s)$, $\sh(v_i)\geq\sh(v_0)$ for every $0< i< k$ and $v_k=(q,s)$ for some $q\in Q$. Then $(p,c,q)\in S$ where $c$ is the largest colour visited in $\play$. 

````


````{admonition} Proof
:class: dropdown tip

We do the proof only when we assume that the inequality $\sh(v_i)\geq\sh(v_0)$ is strict. The case where it is large is then a consequence of the fact that $S$ is complete with successive application of the strict case.
The proof is by induction on $k$. The base case is when $k=2$, and it corresponds to the degenerated case in the definition of consistent tuple. Now for the general case, when $k>2$, one simply considers the play $v_1\cdots v_{k-1}$, applies the induction hypothesis and conclude with the definition of consistent tuple again.

````

We are now ready to conclude and prove that $\sigma$ is a winning strategy for \Eve.

````{prf:theorem} NEEDS TITLE 10-thm:positional-strategy
:label: 10-thm:positional-strategy

The positional strategy $\sigma$ is winning for \Eve on the whole winning region in $\game$.

````


````{admonition} Proof
:class: dropdown tip

Consider an infinite play $\play=v_0v_1\cdots$ starting from some winning position for \Eve. Then by \Cref{10-proposition:stays-in-winning-region} we know that the play stays in the winning region. 

By contradiction assume that $\play$ is loosing. We distinguish between two cases depending whether there is some vertex that is infinitely visited or not in $\play$. 

*  Assume that there is a vertex $v=(q,s)$ that appears infinitely often in $\play$ and choose one of minimal stack height. Let $k_0$ be such that $v_{k_0}=v$ and such that $\sh(v_j)\geq \sh(v)$ for every $j\geq k_0$. Let $(k_i)_{i\geq 0}$ be the increasing sequence of integers $k_i\geq k_0$ such that $v_{k_i}=v$. We claim that the largest colour visited in the segment $v_{k_i}\cdots v_{k_{i+1}}$ is even: indeed, it is a direct consequence of \Cref{10-proposition:bumps} and of the fact that the set of summaries we consider are winning. We then conclude that the largest colour infinitely visited in $\play$ is even hence, leading a contradiction.
*  Assume that no vertex is infinitely often visited in $\play$. As the parity objective is prefix independent we can assume without loss of generality that there is no visited vertex with stack-height strictly smaller than $h=\sh(v_0)$. 
Factorise $\play$ as $v_{i_0}\cdots v_{i_1-1}v_{i_1}\cdots v_{i_2-1}v_{i_2}\cdots v_{i_3-1}\cdots$ where $\sh(v_{i_j})=\sh(v_{i_{j+1}-1})=h+j$ and $\sh(v_k)>h+j$ for all $k\geq j+1$ (equivalently stack height $h+j$ is left forever in $v_{j+1}$). Call $s_j$ the stack content in $v_{i_j}$ and consider the infinite play $\hplay$ defined as the limit of the increasing (for prefix ordering) sequence of finite plays $(\hplay_{s_j})_{j\geq 0}$: it is a play in $\hgame$ where \Eve respects $\hsigma$ hence, it is winning for her. 
Now let $v_{i_j}=(q_j,s_j)$, $v_{i_{j+1}-1}=(p_j,s_j)$ and let $c_j$ be the largest colour visited in $v_{i_j}\cdots v_{i_{j+1}-1}$. Then, as we assume that $\play$ is loosing one has $\limsup(c_i)_{i\geq 0}$ is odd. Moreover, if one lets \begin{equation*}
\begin{split}
\hplay = & (\bot,R_0)(\gamma_0,R_0,\sigma_0,S_0)(\gamma_0,R_0,\sigma_0,S_0,(\sigma^1_\alpha,S^1_\alpha)_{\alpha\in\Gamma}))(\gamma_1,R_1,\sigma_1,S_1)\\ &\quad (\gamma_1,R_1,\sigma_1,S_1,(\sigma^2_\alpha,S^2_\alpha)_{\alpha\in\Gamma}))(\gamma_2,R_2,\sigma_2,S_2)\cdots
\end{split}
\end{equation*}
one has that 
\begin{itemize}
*  $(q_i,c_i,p_i)\in S_i$ (by \Cref{10-proposition:bumps}); and 
*  either $p_i\in Q_\mEve$ and $(q_{i+1},\push{\gamma_{i+1}})=\sigma_i(p_i,\gamma_i)$ (by definition of $\sigma$) or $p_i\in Q_\mAdam$ and $(q_{i+1},\push{\gamma_{i+1}})\in \Delta((p_i,\gamma_i)$  (by definition of $\sigma$).

This means that $\hplay$ is loosing, leading a contradiction.
\end{itemize}
Hence, we conclude that $\play$ is winning which concludes the proof.

````


````{admonition} Remark 
Note that the previous strategy $\sigma$ can be computed by a finite state automaton.

````

